{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Apache Spark**](https://spark.apache.org) **distributed computing engine for big-data processing**. It takes care of many of the raw technical details of programs running on a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Components of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphics/third-party/spark-components.gif)\n",
    "\n",
    "_Source: databricks.com_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark softare package is structured in the following way. Since Spark is under active development with new features and APIs being added, legacy and current subpackages sometimes confuse novices. Looking at Spark 3.0, this should clear things up:\n",
    "\n",
    "- **Spark Core**: distributed parallel data processing built on the _Resilient Distributed Dataset (RDD)_\n",
    "- **Spark SQL**: processing structured data with SQL and the _DataFrame_\n",
    "- **Spark ML**: machine learning at scale - built on DataFrame API\n",
    "    - legacy: **SPark MLlib** - built on RDD API\n",
    "- **Spark Structured Streaming** - streaming data processing using the **Spark SQL** engine\n",
    "    - legacy: **Spark Streaming** - stream processing built on the RDD API \n",
    "- **GraphX**: graph data processing\n",
    "    - no Python API -> **GraphFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has Spark ever done for us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's true, compared to the rich ecosystem of Python data analysis tools, features we find in Spark can be somewhat limited (e.g. the variety of machine learning algorithms implemented in Spark ML). However, from the viewpoint of analytics, Spark has two major selling points.\n",
    "\n",
    "- **Spark enables - in theory - unlimited scalability of data processing** - just add more nodes to your cluster\n",
    "- **Spark abstracts away most of the details of distributed parallelism**, like..\n",
    "    - parallelization: we rarely deal with cluster nodes and CPUs explicitly \n",
    "    - _load balancing_: how to allocate tasks to nodes efficiently\n",
    "    - optimization of queries and computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphics/third-party/spark-optimizer.png)\n",
    "_Source: databricks.com - [The Catalyst Optimizer](https://databricks.com/glossary/catalyst-optimizer)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
