{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Machine Learning Pipeline with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to build a simple predictive model using machine learning. We are going to revisit the Titanic passenger list data set, and use it to train a classifier that tries to determine whether a passenger survived the disaster, based on the person's attributes in the passenger list. This is obviously an educational example using small data, but a similar sequence of steps can be applied to solve real-world predictive analytics tasks on large amounts of distributed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../.assets/data/titanic/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
      "2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\n",
      "3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S\n",
      "4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S\n",
      "5,0,3,\"Allen, Mr. William Henry\",male,35,0,0,373450,8.05,,S\n",
      "6,0,3,\"Moran, Mr. James\",male,,0,0,330877,8.4583,,Q\n",
      "7,0,1,\"McCarthy, Mr. Timothy J\",male,54,0,0,17463,51.8625,E46,S\n",
      "8,0,3,\"Palsson, Master. Gosta Leonard\",male,2,3,1,349909,21.075,,S\n",
      "9,1,3,\"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\",female,27,0,2,347742,11.1333,,S\n"
     ]
    }
   ],
   "source": [
    "!head {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and always keep the documentation close for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dictionary\n",
      "\n",
      "Variable\tDefinition\tKey\n",
      "survival \tSurvival \t0 = No, 1 = Yes\n",
      "pclass \t    Ticket class \t1 = 1st, 2 = 2nd, 3 = 3rd\n",
      "sex \t    Sex \t\n",
      "Age \t    Age in years \t\n",
      "sibsp \t    # of siblings / spouses aboard the Titanic \t\n",
      "parch \t    # of parents / children aboard the Titanic \t\n",
      "ticket \t    Ticket number \t\n",
      "fare \t    Passenger fare \t\n",
      "cabin \t    Cabin number \t\n",
      "embarked \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton\n",
      "\n",
      "\n",
      "Variable Notes\n",
      "\n",
      "pclass: A proxy for socio-economic status (SES)\n",
      "1st = Upper\n",
      "2nd = Middle\n",
      "3rd = Lower\n",
      "\n",
      "age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
      "\n",
      "sibsp: The dataset defines family relations in this way...\n",
      "Sibling = brother, sister, stepbrother, stepsister\n",
      "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
      "\n",
      "parch: The dataset defines family relations in this way...\n",
      "Parent = mother, father\n",
      "Child = daughter, son, stepdaughter, stepson\n",
      "Some children travelled only with a nanny, therefore parch=0 for them."
     ]
    }
   ],
   "source": [
    "!cat ../.assets/data/titanic/titanic-documentation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After creating a `SparkSession`, we read the contents of the .csv file into a DataFrame. For that we also need to define its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/15 14:19:08 WARN Utils: Your hostname, clsm1ba.local resolves to a loopback address: 127.0.0.1; using 192.168.0.8 instead (on interface en0)\n",
      "21/09/15 14:19:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/15 14:19:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"TitanicClassifier\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "             StructField('PassengerId', StringType()),\n",
    "             StructField('Survived', IntegerType()),\n",
    "             StructField('Pclass', IntegerType()),\n",
    "             StructField('Name', StringType()),\n",
    "             StructField('Sex', StringType()),\n",
    "             StructField('Age', IntegerType()),\n",
    "             StructField('SibSp', IntegerType()),\n",
    "             StructField('Parch', IntegerType()),\n",
    "             StructField('Ticket', StringType()),\n",
    "             StructField('Fare', DoubleType()),\n",
    "             StructField('Cabin', StringType()),\n",
    "             StructField('Embarked', StringType())\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(data_path, header=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take a look at the documentation of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Dictionary\n",
      "\n",
      "Variable\tDefinition\tKey\n",
      "survival \tSurvival \t0 = No, 1 = Yes\n",
      "pclass \t    Ticket class \t1 = 1st, 2 = 2nd, 3 = 3rd\n",
      "sex \t    Sex \t\n",
      "Age \t    Age in years \t\n",
      "sibsp \t    # of siblings / spouses aboard the Titanic \t\n",
      "parch \t    # of parents / children aboard the Titanic \t\n",
      "ticket \t    Ticket number \t\n",
      "fare \t    Passenger fare \t\n",
      "cabin \t    Cabin number \t\n",
      "embarked \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton\n",
      "\n",
      "\n",
      "Variable Notes\n",
      "\n",
      "pclass: A proxy for socio-economic status (SES)\n",
      "1st = Upper\n",
      "2nd = Middle\n",
      "3rd = Lower\n",
      "\n",
      "age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
      "\n",
      "sibsp: The dataset defines family relations in this way...\n",
      "Sibling = brother, sister, stepbrother, stepsister\n",
      "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
      "\n",
      "parch: The dataset defines family relations in this way...\n",
      "Parent = mother, father\n",
      "Child = daughter, son, stepdaughter, stepson\n",
      "Some children travelled only with a nanny, therefore parch=0 for them."
     ]
    }
   ],
   "source": [
    "!cat ../.assets/data/titanic/titanic-documentation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning pipeline is a sequence of processing steps or stages that leads from the raw data to the desired result, e.g. a trained model or a prediction. The [`pyspark.ml` module](https://spark.apache.org/docs/latest/ml-pipeline.html) provides an API to map this concept to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Estimator, Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Transformer` implements a method `transform()` which converts one DataFrame into another, generally by appending one or more columns. That could mean extracting features from a dataset, or performing prediction based on the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Estimator` is a learning algorithm, any algorithm that fits or trains on data. An Estimator implements a method `fit()`, which accepts a DataFrame and produces a `Model`, which is also a `Transformer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Pipeline` is a sequence of `PipelineStage`s, which can be `Transformer`s and `Estimator`s. A `Pipeline` also behaves like a `Estimator`, finally outputting a `PipelineModel`. What happens when you call the `fit` method of a transformer is the following: For `Estimator` stages, their `fit()` method is called to produce a `Transformer` (which becomes part of the `PipelineModel`), and that Transformer’s `transform()` method is called on the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocesssing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a classifier that predicts the target variable `Survived` - whether the passenger survived the Titanic disaster - depending on the input columns `Age`, `Fare`, `Sex` and `Embarked`. `Age` and `Fare`  contain numeric values, `Sex` and `Embarked` contain categorical values in the form of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns used in this example\n",
    "data = data[[\"PassengerId\", \"Survived\", \"Age\", \"Fare\", \"Sex\", \"Embarked\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there are a few missing values some of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(col, \" : \", data.filter(f\"{col} is NULL\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are several strategies to deal with missing values in machine learning, including replacement with dummy values, but for simplicity, we simply want to ignore a row with missing values. There are multiple ways of dropping these rows from the DataFrame. We would like to do this as a stage in a `Pipeline`, which gives us the chance to learn about how to write our own custom `Transformer`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Custom Transformers for Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop rows with missing values as part of a pipeline, we write a custom transformer that performs this step. We need to subclass the `Transformer` class, and also implement a few expected attributes. (For this simple example, we don't need them to function, they just need to be there, so we set them to constant dummy values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaDropper(Transformer):\n",
    "    \"\"\"\n",
    "    Drops rows with at least one not-a-number element\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataAfterDrop = data.dropna(subset=self.cols) \n",
    "        return dataAfterDrop\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test our transformer by using it as a stage in a pipeline, which we first `fit` to the data and then use it to`transform` the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_pipeline = Pipeline(stages=[NaDropper(cols=data.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = prepro_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_clean.columns:\n",
    "    print(col, \" : \", data_clean.filter(f\"{col} is NULL\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Using SQLTransformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As so often, there is more than one way to perform a task such as dropping missing values. The `SQLTransformer` executes arbitrary SQL statements on the DataFrame. Try applying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "# Your turn: Use SQLTransformer to drop rows with missing values \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorial Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorial attributes in the form of strings, such as `Embarked`, need to be encoded numerically before being readable by the machine learning algorithm. Among different strategies available for this task, one of the simplest is assigning a numeric index to each categorial value. This is what the `StringIndexer` estimator does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_stages = []\n",
    "enc_stages.append(StringIndexer(inputCol=\"Embarked\", outputCol=\"Embarked_encoded\"))\n",
    "enc_stages.append(StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_encoded\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = Pipeline(stages=enc_stages).fit(data_clean).transform(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go on to the training phase in which a machine learning algorithm ingests the training data to build a predictive model - here, a classifier that predicts yes or no for survival.\n",
    "\n",
    "Many types of classification algorithms exist, each with their own strengths and weaknesses whose discussion goes beyond the scope of this examples. A simple choice is building a single **decision tree**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier expects the target column to be named `label`, so we are going to rename `Survived` accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = data_encoded.withColumnRenamed(\"Survived\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate classifier performance in a reliable way, we need to split our available data into a training and a test set. The latter is put aside and will be used for evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitRatio = 0.8\n",
    "data_training, data_test = data_encoded.randomSplit([splitRatio, 1-splitRatio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assembling Features & Training**\n",
    "\n",
    "In `pyspark.ml`, the learning algorithm expects all features to train on to be placed in a single column of **feature vectors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemble_features = VectorAssembler(inputCols=[\"Age\", \"Fare\", \"Sex_encoded\", \"Embarked_encoded\"], \n",
    "                                    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last stage of the training is the ML algorithm itself. After this, we can trigger the training by calling `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stages = [assemble_features, classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(stages=training_stages).fit(data_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields a fitted model. In order to perform classification, we call the `transform` method of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation has added three new columns to the DataFrame:\n",
    "\n",
    "- prediction: the predicted label\n",
    "- rawPrediction: the direct output of the classification algorithm - interpretaion may vary among algorithms\n",
    "- probability: the probability of each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[[\"PassengerId\", \"prediction\", \"probability\", \"rawPrediction\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mllib.MulticlassMetrics` implements a number of standard metrics to evaluate the performance of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MulticlassMetrics expects label to be of type double\n",
    "predictions = (predictions.withColumn(\"label\", predictions[\"label\"].cast(\"double\")))\n",
    "\n",
    "mm = MulticlassMetrics(predictions.select([\"label\", \"prediction\"]).rdd)\n",
    "labels = sorted(predictions.select(\"prediction\").rdd.distinct().map(lambda r: r[0]).collect())\n",
    "\n",
    "metrics = pandas.DataFrame([(label, mm.precision(label=label), mm.recall(label=label), mm.fMeasure(label=label)) for label in labels],\n",
    "                        columns=[\"label\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Assembling the Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the model training workflow and implement it as a single `Pipeline` that starts from the raw data and outputs a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = spark.read.csv(data_path, header=True, schema=schema)\n",
    "stages = []\n",
    "# Your turn - implement the model training as a single Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "6"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
