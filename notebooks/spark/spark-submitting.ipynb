{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting a Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides working with a Spark cluster interactively, for example via the PySpark console or a Jupyter notebook, a typical way of running Spark programs is to submit them as a [batch job](https://en.wikipedia.org/wiki/Batch_processing): We send a Python script to the cluster via the `spark-submit` command. This chapter explains how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that PySpark uses the right Python version for the following examples by setting the environment variable `PYSPARK_PYTHON` to Python 3. A way to do this with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spark-submit` is the command line tool to send jobs to a Spark cluster. It supports Spark jobs written in Java, Scala, or Python. Additionally it offers various configuration options for tuning the performance of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Minimal Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a minimal example of a Python script containing a Spark job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/spark_job_minimal.py\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "SPARK_APP_NAME='sparkjob_template'\n",
    "conf = SparkConf().setAppName(SPARK_APP_NAME) \n",
    "spark_context = SparkContext(conf=conf)\n",
    "\n",
    "#----------------------\n",
    "# TODO: replace with your Spark code\n",
    "rdd = spark_context.range(100)\n",
    "#----------------------\n",
    "\n",
    "spark_context.stop() # don't forget to cleanly shut down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The %%file command is an IPython \"cell magic\" that automatically writes the code from the cell to a file. So, we can directly submit this to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit scripts/spark_job_minimal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a slightly more elaborate and convenient example that serves nicely as a template for writing Spark jobs.`contextlib` enables us to use the `with` statement to create and close a spark context in a very concise and clean way, and separate that from our actual Spark program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/spark_job_template.py\n",
    "\n",
    "SPARK_APP_NAME='sparkjob_template'\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(appName):\n",
    "    conf = SparkConf().setAppName(appName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        print(\"starting \", appName)\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "        print(\"stopping \", appName)\n",
    "\n",
    "\n",
    "with use_spark_context(appName=SPARK_APP_NAME) as sc:\n",
    "    #----------------------\n",
    "    # TODO: replace with your Spark code\n",
    "    rdd = sc.range(100)\n",
    "    #----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit scripts/spark_job_template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Pi Approximation as Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Remember the $\\pi$ approximation program? Wrap this into a Spark job script and submit it to the cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/pi_approximation_job.py\n",
    "\n",
    "# TODO: write a job for the pi approximation program and run it via `spark-submit`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
