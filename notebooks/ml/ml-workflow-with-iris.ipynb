{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow Overview\n",
    "Letâ€™s have a short overview on the basic steps of machine learning. We will see that there is no magic behind it - only a set of methods and best practices. In addition, the notebook will show that switching between different machine learning models is easy to do when sticking to some conventions. The overall workflow is:\n",
    "* [Data Import and Preparation](#Data-Import-and-Preparation)\n",
    "* [Data Exploration](#Data-Exploration)\n",
    "* [Feature Selection and Engineering](#Feature-Selection-and-Engineering)\n",
    "* [Model Definition](#Model-Definition)\n",
    "* [Training](#Training)\n",
    "* [Validation and Performance](#Validation-and-Performance)\n",
    "\n",
    "The overall workflow has be taken as an iterative process. The [**scikit-learn**](http://scikit-learn.org/stable/) package or short `sklearn` provides the relevant models and tools.\n",
    "### Import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Preparation\n",
    "\n",
    "The data preparation steps can take most of the time of the full workflow. Especially in real world data, often information is missing, sanity checks have to be performed, datasets have to be joined from different sources and much more.\n",
    "\n",
    "For our example, we use a famous dataset that comes with the `sklearn` package. It is the [**Iris flower dataset**](https://en.wikipedia.org/wiki/Iris_flower_data_set) and describes three different types of Iris flowers (Iris setosa, Iris versicolor, Iris virginica). For more information of the dataset see also [here](http://archive.ics.uci.edu/ml/datasets/Iris). \n",
    "\n",
    "There are other open datasets to get used with machine learning. Have a look in the [documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) of `sklearn` for other examples like handwritten digits for classification or house-prices for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "meta = iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information of the dataset\n",
    "for key in iris: \n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of DataFrame\n",
    "df = pd.DataFrame(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column name\n",
    "df.columns = iris['feature_names']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a good sandbox for a classification task. Our **target classes**, the iris species, are already included in the data set together with some variables (or **features**) describing the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding target column\n",
    "df['target'] = iris.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Remark: Real world data_\n",
    "\n",
    "On other datasets getting the information like column names can be the first task in data preparation and take some time. In addition, one of the main parts is to aggregate data from different sources to one data structure (here: `pandas.DataFrame`) on which the machine learning model will be applied. In general, the ML-algorithm need numerical data as input so that strings have to be encoded (see Feature Engineering). But also units have to be checked, time series have to be set accordingly to the correct file format (see `pandas.to_datetime`). One other major part is to perform sanity checks on the data, to check for missing values and maybe compensate outliers. As mention before, all in all this take more than half of the actual time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "As the data import and preparation is already finished we can go over to a first glance on the data itself. Our task is to get a first _feeling_ for the data. Getting used to all features and try to figure out if they have enough power for classification of the target. If we do not find any differences and all attributes behave the same way for each target then machine learning will not give you any promising result as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we have 150 samples, four features and the target information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are length and width of sepal and pental, respectively. The unit is cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different species. Each species has fifty samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target')['target'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now looking for differences between the species by calculating some summary statistics for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['target']==0].describe().loc[['mean','std']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['target']==1].describe().loc[['mean','std']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['target']==2].describe().loc[['mean','std']].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already detect that the first species (target = 0) has a significantly lower mean petal length and width. Because of this difference, petal length should be a valuable feature for a classifier. However, let's check by plotting the distribution of the feature for the different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sepal length (cm)'\n",
    "# 1: 'sepal width (cm)'\n",
    "# 2: 'petal length (cm)'\n",
    "# 3: 'petal width (cm)'\n",
    "\n",
    "variable = iris.feature_names[3]\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    df[df['target']==i][variable].plot(\n",
    "        kind='hist',\n",
    "        bins=np.linspace(df[variable].min(),df[variable].max(),15),\n",
    "        figsize=(8,5),   \n",
    "        alpha=0.5,       # transparency\n",
    "        label=f'{i} {iris.target_names[i]}',\n",
    "        legend=True\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'sepal length (cm)'\n",
    "# 1: 'sepal width (cm)'\n",
    "# 2: 'petal length (cm)'\n",
    "# 3: 'petal width (cm)'\n",
    "\n",
    "variable_A = iris.feature_names[0]\n",
    "variable_B = iris.feature_names[1]\n",
    "\n",
    "df.plot(\n",
    "    kind='scatter',\n",
    "    x=variable_A,\n",
    "    y=variable_B,\n",
    "    c=df['target'],\n",
    "    figsize=(8,5), # (width, height)\n",
    "    cmap='Set1',   # colormap\n",
    "    s=50,          # dot size\n",
    "    colorbar=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Engineering\n",
    "\n",
    "So far, we got a broad overview of our data and could detect some promising/meaningful features for a classification task. For the actual training of a ML-model we need to select **features** (**Feature Selection**) as input to classify our **target**. In our example we use all four features but we could also select only a part of them. In real world data it often makes sense to take only a selection as computing power can be limiting or as non meaningful features do not improve the overall performance of the classificator.\n",
    "\n",
    ">  **Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.** &mdash; Andrew Ng:\n",
    "\n",
    "Besides selection, creating of additional features (**feature engineering**) can be another crucial step. In this case we are fine with the four features we have but in real world data we always have to perform feature engineering to develop the full potential of ML. Some examples are \n",
    "- encoding of features (e.g. categories to numerical features), \n",
    "- apply transformations to features (e.g. logscale), \n",
    "- generate new features (e.g. simple stats)\n",
    "- rounding, binning, sampling, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection:\n",
    "training_features = [\n",
    "    'sepal length (cm)', \n",
    "    'sepal width (cm)', \n",
    "    'petal length (cm)',\n",
    "    'petal width (cm)'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Let's we start with the actual ML. Keep in mind we already spent at least half of our time with the steps before! In `sklearn` we can import several different models for ML like simple [decision trees](http://scikit-learn.org/stable/modules/tree.html) to more advanced models as a [random forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) or a [multi layer perceptron (MLP)](http://scikit-learn.org/stable/modules/neural_networks_supervised.html). If we stick to some conventions it is quite easy to use the same workflow to switch between models and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of ML models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model with key parameters\n",
    "model = RandomForestClassifier(n_estimators=25, max_depth=2)\n",
    "#model = MLPClassifier(hidden_layer_sizes=(20,), activation='relu')\n",
    "#model = GradientBoostingClassifier(n_estimators=20, max_depth=2)\n",
    "#model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the Model\n",
    "To get the most out of our model we have to perform tuning of the models **hyper-parameters**. Hyper-parameters are parameters of the learning algorithm, rather than parameters of the trained model. Here we will use only standard parameters which are often fine for a first try. However, we should always set some key parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all features as **input (`X`)**. Just to be safe we once again check for missing values and drop these samples. In addition we define our **target (`y`)** we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[training_features]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation for Validation\n",
    "\n",
    "For subsequent performance checks and validation, we split our dataset into two parts - a **training and a test set** ([`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "The rationale behind this is, simply put, that a model should not be evaluated on the same data that it was trained with: A model that memorizes all labels it has seen would achieve a perfect score - but it would clearly be useless when faced with new data to classify. We say that it would fail to **generalize**.\n",
    "\n",
    "Consequently, we use only the `train`-part for the training and put aside the `test`-part. In the example we split the dataset into two half (`test_size=0.5`). Keep in mind that train- and test-dataset should have the same information in it. For example it means that in both datasets all three species should contributing in equal share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (0,1,2):\n",
    "    test = y_test[y_test==i].count()\n",
    "    train = y_train[y_train==i].count()\n",
    "    print(f'Species {i}: test {test} and train {train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning: Actual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these preparations, the actual training is as simple as calling the `fit` method of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Performance\n",
    "\n",
    "The actual machine learning training is done. Let's have a look on our results and compare how good our model performs on our training data and the test datasets. If we see the same performance on both sets we can take this as a strong indicator for a valid model. If the model performs much better on our training dataset, there is something wrong (-> **overtraining**)! With `model.predict_proba(dataset)` we get the prediction for each sample to belong to each iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test = model.predict_proba(X_test)\n",
    "y_proba_train = model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get three probabilities per sample\n",
    "y_proba_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum for each sample is one\n",
    "y_proba_test.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results for one species i\n",
    "i=1\n",
    "y_proba_test_i = y_proba_test[:,i]\n",
    "y_proba_train_i = y_proba_train[:,i]\n",
    "\n",
    "# Probability for one species i\n",
    "y_proba_test_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many do really belong to species i?\n",
    "y_proba_test_i[(y_test == i)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respective probabilities\n",
    "y_proba_test_i[(y_test == i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look if wrong samples get a high\n",
    "# probability for the species? (->False Positives)\n",
    "y_proba_test_i[(y_test != i)].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many probabilities are greater than 50 percent?\n",
    "(y_proba_test_i > 0.50).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do really belong to specie i? \n",
    "y_proba_test_i[(y_test == i).values].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of all species:\n",
    "for i in range(3):\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    species_i = y_proba_test_i[(y_test == i)].mean()\n",
    "    not_species_i = y_proba_test_i[(y_test != i)].mean()\n",
    "    print(f'Species {i}: Mean probability for true {species_i:.2f} and false {not_species_i:.2f} predictions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the 'decision' off the classifier\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y_pred_test==0).sum())\n",
    "print((y_pred_test==1).sum())\n",
    "print((y_pred_test==2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis test\n",
    "\n",
    "So far, we did everything by hand. There is a easy way to check the results by visualization. Each chart gives the probability of all samples to belong to one species. In addition, each color gives the true membership. A good classifier will show a good splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2]:\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.hist(y_proba_test_i[(y_test == 0).values], bins=np.linspace(0,1,20), alpha=0.5, density=False, label=iris.target_names[0])\n",
    "    plt.hist(y_proba_test_i[(y_test == 1).values], bins=np.linspace(0,1,20), alpha=0.5, density=False, label=iris.target_names[1])\n",
    "    plt.hist(y_proba_test_i[(y_test == 2).values], bins=np.linspace(0,1,20), alpha=0.5, density=False, label=iris.target_names[2])\n",
    "\n",
    "    plt.legend()\n",
    "    if i == 0:\n",
    "        plt.title('Hypothesis: Sample belongs to species 0')\n",
    "    elif i == 1:\n",
    "        plt.title('Hypothesis: Sample belongs to species 1')\n",
    "    elif i == 2:\n",
    "        plt.title('Hypothesis: Sample belongs to species 2')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    #plt.yscale('log',nonposy='clip')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The [**Confusion Matrix**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) (Table of Confusion) gives for each class how many samples are classified correctly (principal diagonal) and how many classifications are false. In addition, it shows to which wrong class the samples are assigned. In our case we get a 3x3 matrix. The sum of a row are all members of each species and the sum of a column returns the predicted members of a class. A _perfect_ classificator would have only entries on the pricipal diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "truth = y_test\n",
    "\n",
    "cm = confusion_matrix(truth, y_pred_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of true members (row)\n",
    "(y_test.values==2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of predicted members (column)\n",
    "(y_pred_test==2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Confusion Matrix can be condensed to a binary classification for each class. The result is a 2x2 matrix. The sum of the first row are all true members (**Positives**, P) consisting of **True Positives** (TP) and **False Negatives** (FN). The sum of the second row are all false members (**Negatives**, N) consisting of the **False Positives** (FP) and **True Negatives** (TN).\n",
    "\n",
    "x | classified as Positives | classified as Negatives\n",
    "-|-|-\n",
    "**Positives (P)** | True Positives (TP) | False Negatives (FN)\n",
    "**Negatives (N)** | False Positives (FP)  | True Negatives (TN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to condensate to a binary classification\n",
    "def make_bina_class(model, X_sample, i, threshold=0.0, check_max=True):\n",
    "    proba = model.predict_proba(X_sample)\n",
    "    if check_max:\n",
    "        y_pred_test = model.predict(X_sample)\n",
    "        bina_class = [0 if (pred == i) and (proba[pos][i] >= threshold) else 1 for pos, pred  in enumerate(y_pred_test)]\n",
    "    else:\n",
    "        bina_class = [0 if (pred >= threshold) else 1 for pred in proba[:,i]]\n",
    "    return bina_class   \n",
    "\n",
    "# This is done in the List Comprehension:\n",
    "   #for pos, pred  in enumerate(y_pred_test):\n",
    "   #    if (pred == i) & (proba[pos][i] >= threshold):\n",
    "   #        bina_class.append(0)\n",
    "   #    else:\n",
    "   #        bina_class.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "truth = y_test\n",
    "cm = confusion_matrix(truth, y_pred_test)\n",
    "print(f'Confusion Matrix 3x3')\n",
    "print(cm)\n",
    "\n",
    "for i in range(3):\n",
    "    pred = make_bina_class(model, X_test, i)\n",
    "    truth_i = [0 if j == i else 1 for j in y_test]\n",
    "    cm= confusion_matrix(truth_i,pred)\n",
    "    print(f'\\n Confusion Matrix for Species {i}')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "The Receiver Operating Characteristics (ROC) are a slightly more sophisticated way to validate a model. A ROC curve shows the true positive rate as a function of the false positive rate. When given a certain Hypothesis and an acceptable false-positive rate, we see how many samples that truly fit the Hypothesis we can select. In addition, we show the results for the train and test dataset in comparison, to detect deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2]:\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    y_proba_train_i = y_proba_train[:,i]\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(*roc_curve(y_test == i, y_proba_test_i)[:2], label='test')\n",
    "    plt.plot(*roc_curve(y_train == i, y_proba_train_i)[:2], label='train')\n",
    "    plt.plot([0, 1],[0, 1], color='black', linestyle=':')\n",
    "    plt.title(f'ROC curve species {i}')\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate') \n",
    "    plt.legend(loc='best')\n",
    "    plt.show();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other performance indicators to validate the trained model. For example the area under the ROC Curve (**A**rea **U**nder **C**urve **AUC**) or the mean **Accuracy** $\\bigl(\\frac{TP + TN}{P + N}\\bigr)$ can be taken into account. The accuracy gives the overall correctly classified samples despite if they belong to the Positives or Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# How many are misclassified\n",
    "print(f'misclassified: {(y_pred_test != y_test).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in (0,1,2):\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    data.append(roc_auc_score(y_test.values == i, y_proba_test_i))\n",
    "\n",
    "pd.DataFrame(data,columns=['AUC'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Accuracy: {model.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Several machine learning models return a score for the feature importance within the classificator. This can be used to perform more training steps to improve the model, improve computing time or feedback this to the initial data acquisition. If we detect that one feature is very important for the classificator it maybe a good idea to improve the quality of this feature or engineer equivalent features. In addition, this step can highlight features which were not be be expected to be important and can lead to a rethinking of strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only works for GradientBoostingClassifier or RandomForestClassifier\n",
    "if (str(model)[0:3] != 'MLP'):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.barh(range(len(X.columns)), model.feature_importances_)\n",
    "    plt.yticks(range(len(X.columns)), X.columns)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Works only for Classifyer like GradientBoostingClassifier or RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already detected in the exploration step the petal width and length have the highest impact to the classification.\n",
    "\n",
    "#### *Remark: Iterative process!*\n",
    "In this case it is quite easy to get a valid model. However, training and validation has to be an iterative process in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
