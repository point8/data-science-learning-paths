{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanatory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we are going to train a feature-rich machine learning model for **explanatory purposes**. That means that our interest shifts from predictive power of the model to explanatory power: Not only do we want our model to make accurate predictions, we want it to **demonstrate the relationship between features and target variables in a robust and explainable way**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths\n",
    "data_science_learning_paths.setup_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [House Prices]() dataset for the following examples. Our goals is to model the sale price of a house given various attributes of the property. We preprocess the data set as follows:\n",
    "- exclude columns with mostly missing values\n",
    "- encode certain attributes measured on a quality scale as integers\n",
    "- one-hot-encode categorial attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v0 = data_science_learning_paths.datasets.read_house_prices(\n",
    "    encode_ordinal=True,\n",
    "    drop_sparse=True,\n",
    "    encode_categorial=True,\n",
    "    drop_first_level=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"SalePrice\"\n",
    "features = data_v0.columns.difference([target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model can benefit from scaling the attributes to a common interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled = pandas.DataFrame(\n",
    "    MinMaxScaler().fit_transform(data_v0[features]),\n",
    "    columns=data_v0[features].columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v0 = pandas.concat([features_scaled, data_v0[target]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we select a **linear model** and fit it with **ordinary least squares regression**. For explanatory modelling, the [`statsmodels`](https://www.statsmodels.org/stable/index.html) library is a good choice, since we get a large amount of statistical diagnostics information with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model and output the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(data_v0[target], data_v0[features]).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to inspect the parameters and metadata of the model, we need the result of the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.OLS(data_v0[target], data_v0[features]).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the parameters of the model, i.e. the estimated coefficients, as well as the associated $p$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.pvalues.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, sorted by magnitude of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params.sort_values().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.pvalues.reindex(result.params.sort_values().index).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symptom: Non-Significant Coefficients\n",
    "\n",
    "We observe that many of the coefficients have very high $p$-values - and this is not limited to the features that were assigned very small coefficients. If we want to use the coefficients of the model , this means trouble: Observe how the model weights the features when trained on two different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample_a = data_v0.sample(frac=0.5)\n",
    "data_sample_b = data_v0.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(\n",
    "    data_sample_a[target], \n",
    "    data_sample_a[features]\n",
    ").fit().params.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(\n",
    "    data_sample_b[target], \n",
    "    data_sample_b[features]\n",
    ").fit().params.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis: Multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the correlations between our numerous features. For this purpose, we plot a correlation matrix using the [`yellowbricks`](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) ML visualization library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features import Rank2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1,figsize=(16, 16))\n",
    "\n",
    "visualizer = Rank2D(features=data_v0.columns, algorithm='pearson', ax=ax)\n",
    "\n",
    "visualizer.fit(data_v0, None)                # Fit the data to the visualizer\n",
    "visualizer.transform(data_v0)             # Transform the data\n",
    "\n",
    "visualizer.poof()                   # Draw/show/poof the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is multicolinearity?**\n",
    "\n",
    "[Multicolinearity](https://en.m.wikipedia.org/wiki/Multicollinearity) means that the feature set is **redundant** in the sense that features can be predicted (linearly) from other features with high accuracy. **Perfect multicolinarity** exists when a feature is an exact [linear combination](https://en.m.wikipedia.org/wiki/Linear_combination) of other features.\n",
    "\n",
    "**Why is it problematic?**\n",
    "\n",
    "If features are correlated in this way, a model can rely on either one for its prediction - _arbitrarily_. More specifically, the model arbitrarily assign weight to either one of the colinear features. The correlated features may end up with non-significant coefficients. This also fits the definition of **overfitting**: Our model depends greatly on the specific training step, and its results are not generalizable. For example, we might get very different feature importances if we train the model with slightly different data.\n",
    "\n",
    "**Caveat**\n",
    "\n",
    "Multicolinearity is not the only possible root cause of the symptom (non-significant coefficients) that we saw. (E.g., trying to estimate too many model parameters with too few data points may be another one.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment: Reducing Multicolinearity\n",
    "\n",
    "In the following we will attempt feature selection to treat the multicolinearity problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminating Perfect Multicolinearity\n",
    "\n",
    "There is one obvious source of multicolinearity here, and we have introduced it ourselves in the preprocessing: By simple one-hot encoding of categorial features, we created _perfect_ multicolinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the attribute that represents the building type. Let's encode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pandas.read_csv(\"../.assets/data/house/prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[\"BldgType\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.get_dummies(\n",
    "    raw_data[[\"BldgType\"]]\n",
    ").sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an easy fix, provided by `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.get_dummies(\n",
    "    raw_data[[\"BldgType\"]],\n",
    "    drop_first=True\n",
    ").sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the situation when applying this in the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v1 = data_science_learning_paths.datasets.read_house_prices(\n",
    "    encode_ordinal=True,\n",
    "    drop_sparse=True,\n",
    "    encode_categorial=True,\n",
    "    drop_first_level=True,  # drop one level when one-hot-encoding!\n",
    ")\n",
    "target = \"SalePrice\"\n",
    "features = data_v1.columns.difference([target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: What changed after removing perfect multicolinearity?\n",
    "    \n",
    "Repeat the above modeling and diagnosis steps. Did dropping the first level of categorial variables improve the robustness of the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Instrument: Clustered Correlation Plot\n",
    "\n",
    "Diagnosing and fixing multicolinearity issues can get difficult when faced with a large number of variables. A diagnostic method that has been proposed is the use of a **clustered correlation plot**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute a correlation matrix of the features. \n",
    "\n",
    "We can use Pearson's correlation coefficient, but since we do not care about the direction of the correlation, we take the absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = data_v1[features]\n",
    "feature_correlations = feature_data.corr().abs()\n",
    "feature_correlations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute an affinity-based clustering of the features, treating the strength of their correlations as the measure of affinity.\n",
    "\n",
    "scikit-learn's [`AffinityPropagation`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) algorithm is well suited for this purpose:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = AffinityPropagation(\n",
    "    affinity=\"precomputed\"\n",
    ").fit_predict(feature_correlations)\n",
    "\n",
    "clusters = pandas.Series(\n",
    "    cluster_labels,\n",
    "    index=feature_data.columns\n",
    ")\n",
    "clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Order the columns and rows of the correlation matrix by their clusters.\n",
    "\n",
    "In the result, variables that belong to a correlation cluster are placed next to eachother. The cluster is now clearly visible around the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Clustered Correlation Plots\n",
    "\n",
    "Implement a clustered correlation plot function that helps you inspect clusters of colinear variables. Apply it to the house price regression problem and interpret the results. Can you identify variables that should be dropped as features to improve the model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Ended Exercise: Improve the Explanatory Model\n",
    "\n",
    "Use all data science tools available to you to obtain a more robust explanatory model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Lucas Bernardi @ PyData Amsterdam 2017: Diagnosing Machine Learning Models](https://www.youtube.com/watch?v=ZD8LA3n6YvI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
