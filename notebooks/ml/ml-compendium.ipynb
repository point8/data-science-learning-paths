{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Compendium\n",
    "\n",
    "The aim of this chapter is to understand and visualize the functionality of different machine learning algorithms by applying them to example data. Knowing the advantages and disadvantages of the respective method is essential when choosing a model. In addition, there will be always a trial-and-error part when finding the best model. Especially the validation step is necessary to decide whether the method of choice is the correct one and performs well enough.\n",
    "\n",
    "There are overviews of comparisons of different [classifier](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py) (supervised learning) and [clustering](http://scikit-learn.org/stable/modules/clustering.html) (unsupervised learning) methods in `sklearn`. Here, we will focus on some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16.0, 8.0)\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "from ml_functions import decision_boundaries, svm_decision_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colorcode\n",
    "There are several [different colorcode](https://matplotlib.org/users/colormaps.html) for the respective purpose. We are going to use `viridis` as it has a strong contrast or `Set1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorcode = \"viridis\"\n",
    "# colorcode = 'Set1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Example Data\n",
    "\n",
    "Several [samples generators](http://scikit-learn.org/stable/modules/classes.html#samples-generator) come with `sklearn`, and we can use them to illustrate different ML methods. In the following we use these types of example data points:\n",
    "\n",
    "- **Circles**: Two circles with noise of the magnitude of their distance to each other. Separating them is a tough task.\n",
    "\n",
    "- **Blobs**: Three Gaussian-distributed blobs with good separation. Most models should be able to tell them apart.\n",
    "\n",
    "- **Moons**: Two half moons with low noise level (`noise=0.09`). Both distributions are intertwined but they do not overlap. They will show how important tuning and validation are.\n",
    "\n",
    "- **Silly moons**: As above but with a higher noise level (`noise=0.3`). Those distributions show the differences between overfitted and well-performing models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons, make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "rdm_seed = 42\n",
    "\n",
    "X_circle, y_circle = make_circles(\n",
    "    n_samples=n_samples, noise=0.1, factor=0.8, shuffle=False, random_state=rdm_seed\n",
    ")\n",
    "X_blobs, y_blobs = make_blobs(\n",
    "    n_samples=n_samples, centers=3, shuffle=False, random_state=rdm_seed\n",
    ")\n",
    "X_moons, y_moons = make_moons(\n",
    "    n_samples=n_samples, noise=0.09, shuffle=False, random_state=rdm_seed\n",
    ")\n",
    "X_moons2, y_moons2 = make_moons(\n",
    "    n_samples=n_samples, noise=0.3, shuffle=False, random_state=rdm_seed\n",
    ")\n",
    "\n",
    "data = {\n",
    "    \"Circles\": (X_circle, y_circle),\n",
    "    \"Blobs\": (X_blobs, y_blobs),\n",
    "    \"Moons\": (X_moons, y_moons),\n",
    "    \"Silly moons\": (X_moons2, y_moons2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion to plot generated data.\n",
    "def plot_test_data(X, y, show_clusters=False, title=\"\", ax=None):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    if show_clusters:\n",
    "        c = y\n",
    "    else:\n",
    "        c = \"dodgerblue\"\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=c, cmap=colorcode)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "show_clusters = False\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "\n",
    "ax = plt.subplot(gs[0])\n",
    "plot_test_data(X_circle, y_circle, show_clusters=show_clusters, title=\"Circles\", ax=ax)\n",
    "ax = plt.subplot(gs[1])\n",
    "plot_test_data(X_blobs, y_blobs, show_clusters=show_clusters, title=\"Blobs\", ax=ax)\n",
    "ax = plt.subplot(gs[2])\n",
    "plot_test_data(X_moons, y_moons, show_clusters=show_clusters, title=\"Moons\", ax=ax)\n",
    "ax = plt.subplot(gs[3])\n",
    "plot_test_data(\n",
    "    X_moons2, y_moons2, show_clusters=show_clusters, title=\"Silly moons\", ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "- We can plot the test data with the given cluster assignment (**label**). The **`show_clusters`** variable has to be set to `True`.\n",
    "\n",
    "- Go back to the samples generator and change parameteres like **`n_samples`** or **`noise`** to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do not know to which cluster each data point belongs a typical task of machine learning is to find respective (possible) clusters. This kind of task is called unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning - Clustering\n",
    "\n",
    "**Clustering methods** help to detect clusters in unlabeled data sets:\n",
    "\n",
    "**K-Means** needs the number ***K*** of clusters as input. The algorithm sets *K* independent cluster centers as starting points. Each data point is assigned to the nearest center. After one iteration it calculates new cluster centers by averaging the respective assigned members. K-means performs very well on data showing a good separation and \"spherical\" clusters.\n",
    "\n",
    "The **Gaussian Mixture-**algorithm also requires the number *K* of clusters as input. The method is based on fitting multidimensional Gaussian distribution to the data set. Thereby it can handle distributions with \"non-spherical\" clusters.\n",
    "\n",
    "With the clustering algorithm **[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)** (**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise) we do not set the number of clusters, but two algorithm parameters that determine which clusters are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "n_clusters = 3\n",
    "max_iterations = 10\n",
    "\n",
    "model = KMeans(n_clusters, max_iter=max_iterations, init=\"random\", n_init=1)\n",
    "\n",
    "# Layout\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "cmap = cm.get_cmap(\"Set1\")\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    # Set input and target (not used)\n",
    "    X, y = data[d]\n",
    "    # Model fitting\n",
    "    model.fit(X)\n",
    "    predictions = model.predict(X)\n",
    "    # Visualization\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with K-Means\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=cmap(predictions), cmap=colorcode)\n",
    "    ax.scatter(\n",
    "        model.cluster_centers_[:, 0],\n",
    "        model.cluster_centers_[:, 1],\n",
    "        c=\"black\",\n",
    "        s=200,\n",
    "        label=\"Cluster centers\",\n",
    "    )\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture\n",
    "\n",
    "The results of a [**Gaussian Mixture**](http://scikit-learn.org/stable/modules/mixture.html) algorithm look like the K-means clusters but elliptical distribution can be handled. Typically correlated data show this type of distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "n_components = 2\n",
    "model = GaussianMixture(n_components)\n",
    "\n",
    "# Layout\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "cmap = cm.get_cmap(\"Set1\")\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    # Set input and target (not used)\n",
    "    X, y = data[d]\n",
    "    # Model fitting\n",
    "    model.fit(X)\n",
    "    predictions = model.predict(X)\n",
    "    # Visualization\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with Gaussian Mixture\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=cmap(predictions), cmap=colorcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "[**DBSCAN**](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) defines the number of cluster automatically as they depend on the set parameters. To get a reasonable result we have to tune them. \n",
    "The most important parameters are **eps** (maximum distance, radius) and **min_samples** (minimal number of points within radius) which define the neighbourhood.\n",
    "\n",
    "To set up the DBSCAN we need to know the general type of distribution. In addition the size of the data set and thereby the density can have a high influence on the parameter *eps*. A more detailed explanation is given [here](https://en.wikipedia.org/wiki/DBSCAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = DBSCAN(eps=0.163, min_samples=5)\n",
    "\n",
    "# Layout\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "cmap = cm.get_cmap(\"Set1\")\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    # Set input and target (not used)\n",
    "    X, y = data[d]\n",
    "    # Model fitting\n",
    "    predictions = model.fit_predict(X)\n",
    "    # Visualization\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with DBSCAN\")\n",
    "    cluster_points = predictions != -1\n",
    "    noise_points = predictions == -1\n",
    "    ax.scatter(\n",
    "        X[cluster_points][:, 0],\n",
    "        X[cluster_points][:, 1],\n",
    "        c=cmap(predictions[cluster_points]),\n",
    "        cmap=colorcode,\n",
    "    )\n",
    "    ax.scatter(X[noise_points][:, 0], X[noise_points][:, 1], c=\"black\", marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Try to find a set of parameteres to cluster the different data sets correcly. You may want to go back to the data set generators to change them as well!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions for blobs and moons:\n",
    "# model = DBSCAN(eps=2, min_samples=5) #Blobs\n",
    "# model = DBSCAN(eps=0.2, min_samples=5) #Moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "After a first try of clustering (unsupervised learning) we will go on with the same data sets but apply **Supervised Learning**. Now we will put in into the training to which class each data points belongs. There different classifier which can be applied on labeled data.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "A [**Support Vector Machine**](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (SVM) is an ML-algorithm which uses geometrical cuts within one or more (hyper)planes to classify the data.\n",
    "\n",
    "Complex distribution cannot be separated by straight lines. Therefore, the data set has to be transformed in higher dimensional space. In this new coordinate system there are *straight lines* which can separate the data easily. The vectors of the data points closest to the seperating plane are called ***support vectors***. The distance between support vectors and separating plane is maximized with the aim to have as much *free space* as possible between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition - linear kernel\n",
    "model = SVM(kernel=\"linear\")\n",
    "\n",
    "# Model fit\n",
    "model.fit(X_moons, y_moons)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"Moons with SVM (linear)\")\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=colorcode)\n",
    "svm_decision_function(clf=model, ax=plt.gca(), colors=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition - Radial basis function as kernel\n",
    "model = SVM(kernel=\"rbf\", gamma=2)\n",
    "\n",
    "# Model fit\n",
    "model.fit(X_moons, y_moons)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"Moons with SVM (rbf)\")\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=colorcode)\n",
    "svm_decision_function(clf=model, ax=plt.gca(), colors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Try out different kernel functions or data sets. In addition try out the parameter **`gamma`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = \"rbf\"\n",
    "gamma = 0.5\n",
    "model = SVM(kernel=kernel, gamma=gamma)\n",
    "\n",
    "dataset = X_moons  # X_circle X_moons X_moons2 #X_blobs\n",
    "target = y_moons  # y_circle y_moons y_moons2 #y_blobs\n",
    "\n",
    "model.fit(dataset, target)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=target, cmap=colorcode)\n",
    "\n",
    "if dataset[0][0] != X_blobs[0][0]:\n",
    "    svm_decision_function(clf=model, ax=plt.gca(), colors=\"black\")\n",
    "else:\n",
    "    decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbors\n",
    "\n",
    "The [**KNeighborsClassifier**](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) (kNN) method is a mathematical easy calculation for a classification task. For each data point a majority decision is made: Which is the most frequent class my *k-nearest-neighbors* have?\n",
    "\n",
    "Training is done by simply read in all training data points. As simple the calculation is for big data sets and a large *k* it is kind of brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "k = 1\n",
    "model = KNeighborsClassifier(k)\n",
    "\n",
    "# Model fit\n",
    "model.fit(X_moons, y_moons)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(f\"Moons with kNN (k={k})\")\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=colorcode)\n",
    "decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Try out different values for **`k`** (1, 5, 10, 20, 50, 100). Do you expect an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "k = 1  # (1, 5, 10, 20, 50, 100)\n",
    "model = KNeighborsClassifier(k)\n",
    "\n",
    "dataset = X_moons  # X_circle X_moons X_moons2 #X_blobs\n",
    "target = y_moons  # y_circle y_moons y_moons2 #y_blobs\n",
    "\n",
    "model.fit(dataset, target)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=target, cmap=colorcode)\n",
    "\n",
    "decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "A [**Decision Tree**](http://scikit-learn.org/stable/modules/tree.html) cuts the space with in rectangular sections. The **`max_depth`** parameter is the most important one. It defines how many decisions are allowed to be performed.  \n",
    "Blobs are separated by to lines. A depth of two should be sufficient. By having more and more *decisions* curved separations are represented.\n",
    "\n",
    "Already in this example we see a tendency to introduce overfitting with higher depth. The circles and silly moons show no smooth separation. Thereby new data following the same distributions are likely to be classified not correctly at the interface regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    X, y = data[d]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with Decision Tree\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colorcode)\n",
    "    decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest\n",
    "\n",
    "A [**Random Forrest**](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) consists of single *Decision Trees*. The final decision is made by a majority voting of all trees and we could get something like a probability of the made decision. Thereby overfitting can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    X, y = data[d]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} mit Random Forest\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colorcode)\n",
    "    decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Try out different **`max_depth`** for the single Decision Tree and the Randomforst and compare the results. In the case of a Randomforest you can also set the number of single trees by **`n_estimators`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron (MLP)\n",
    "\n",
    "Especially for larger training data sets and complex structures a [**Multi-Layer Perceptron**](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) can by very useful. But simple data set will show some limits as well.\n",
    "\n",
    "MLPs have a high amount of (hyper)parameters which have to be set. Having a single hidden layer and a small amount of neurons (e.g. 10) only high symmetrical distributions like the blobs and circles are well classified. We can extend a MLP in width (add more neurons per layer) and depth (adding additional layers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=10, max_iter=10000, activation=\"relu\", alpha=0.001  # 10, 50, 100\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    X, y = data[d]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with MLP\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colorcode)\n",
    "    decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Try to make up a ***Deep Neural Network*** by adding additional layer. The parameter **`hidden_layer_sizes`** has to be changed for that purpose. Does it perform better on some data sets? Do you see first indices of overtraining?\n",
    "\n",
    "In addition you can change the activation (**`activation`**) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN (overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before kNN performed gives quite reasonable results. With K=20 all sample data sets show a good classification. The edges are almost smooth for all distribution. Only the case of circles it shows a higher uncertainty. kNN struggles with overlaying distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    X, y = data[d]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with kNN (k=20)\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colorcode)\n",
    "    decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correct kernel function is essential for SVM. Thereby the underlaying distributions should be know when applying a SVM to the data. For all samples the SVM show quite reasonable results and has a smooth separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM(kernel=\"rbf\")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "gs.update(wspace=0.2, hspace=0.2)\n",
    "\n",
    "for i, d in enumerate(data.keys()):\n",
    "    X, y = data[d]\n",
    "    model.fit(X, y)\n",
    "\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_title(f\"{d} with SVM\")\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colorcode)\n",
    "    decision_boundaries(clf=model, ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_\n",
    "\n",
    "_The used functions_ `decision_boundaries` _and_ `svm_decision_function` _are licensed under a [MIT License](https://github.com/Python4AstronomersAndParticlePhysicists/PythonWorkshop-ICE/blob/master/LICENSE). Copyright © 2017 [Python4AstronomersAndParticlePhysicists](https://github.com/Python4AstronomersAndParticlePhysicists/PythonWorkshop-ICE)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
