{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning with marbles\n",
    "\n",
    "In this example we explore **unsupervised machine learning**  by clustering the marbles data set and evaluating the result. In unsupervised learning, the model does not have access to labelled data during the training phase. Instead it tries to assign labels by discovering structure in the data automatically, i.e. by clustering similar data points together.\n",
    "\n",
    "This approach is used to detect unknown patterns in data and to perform unbiased analyses. There are a variety of clustering methods implemented in `sklearn`. A broad overview is given in the [documentation](http://scikit-learn.org/stable/modules/clustering.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import, Preparation, Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lines(lines):\n",
    "    \"\"\" Parse strings of marble data\"\"\"\n",
    "    lines = lines[2:-2]\n",
    "    rows = [d.split(', ') for d in lines.split('), (')]\n",
    "    data = [[int(v.replace(')][(', '')) for v in r] for r in rows]\n",
    "    return pd.DataFrame(data)[[0, 1, 2]]\n",
    "\n",
    "files = [\n",
    "    'blue-white-glass.data',\n",
    "    'cyan-glass.data',\n",
    "    'glass-blue.data',\n",
    "    'glass-green.data',\n",
    "    'glass-red.data',\n",
    "    'glass-yellow.data',\n",
    "    'planet-black-blue.data',\n",
    "    'planet-green.data',\n",
    "    'planet-ocean.data',\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for i, fname in enumerate(files):\n",
    "    print(f'Load data {i}: {fname}')\n",
    "\n",
    "    with zipfile.ZipFile(f'../.assets/data/marbles/{fname}.zip', 'r') as zipf:\n",
    "        with zipf.open(f'{fname}', 'r') as infile:\n",
    "            content = infile.readlines()[0].decode()\n",
    "            dfs.append(parse_lines(content).assign(color=f'{fname}'.replace('.data', '')))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.columns=['R', 'G', 'B', 'color']\n",
    "\n",
    "def generate_xy_values(df):\n",
    "    df['X'] = 0.5 * np.sqrt(3) * df['G'] - 0.5 * np.sqrt(3) * df['B']\n",
    "    df['Y'] = df['R'] - (1 / 3 * df['G']) - (1 / 3 * df['B'])\n",
    "    \n",
    "def generate_intensity_values(df):\n",
    "    df['I'] = np.square(df['X']) + np.square(df['Y'])\n",
    "\n",
    "def generate_angles(df):\n",
    "    df['Phi'] = np.arctan2(df['Y'], df['X'])\n",
    "\n",
    "# Feature Engineering I     \n",
    "generate_xy_values(df)\n",
    "generate_intensity_values(df)\n",
    "generate_angles(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target ID\n",
    "ids = {'blue-white-glass': 0,\n",
    "       'cyan-glass': 1,\n",
    "       'glass-blue': 2,\n",
    "       'glass-green': 3,\n",
    "       'glass-red': 4,\n",
    "       'glass-yellow': 5,\n",
    "       'planet-black-blue': 6,\n",
    "       'planet-green': 7,\n",
    "       'planet-ocean': 8,}\n",
    "\n",
    "df['cat'] = df['color'].map(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "K-Means is an unsupervised machine learning method, which only needs the number of clusters ($k$) as input. It creates $k$ random clusters in the beginning and assigns each data point to one of it. After one iteration it calculates for each cluster a new center by averaging all assigned data points. In this case the assignment is based on the distance to the nearest cluster.\n",
    "\n",
    "Let's see how the [**K-Means**](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm performs on two marbles types with a very good separation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two marbles as input\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "cat1=1\n",
    "cat2=4\n",
    "\n",
    "var1='X'\n",
    "var2='Y'\n",
    "\n",
    "plt.scatter(df[df['cat']==cat1][var1], df[df['cat']==cat1][var2], s=10, alpha=0.01)\n",
    "plt.scatter(df[df['cat']==cat2][var1], df[df['cat']==cat2][var2], s=10, alpha=0.01)\n",
    "plt.xlabel(var1)\n",
    "plt.ylabel(var2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use two clusters as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a better visualization we use just part of the dataset.\n",
    "dataset = pd.concat([\n",
    "    df[df['cat']==cat1][[var1,var2]].head(1000),\n",
    "    df[df['cat']==cat2][[var1,var2]].head(1000)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of model\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(\n",
    "    n_clusters=2,  # number of clusters\n",
    "    max_iter=1,  # number of itereations\n",
    "    \n",
    "    n_init=1, \n",
    "    init='random', \n",
    "    precompute_distances=False, \n",
    "    random_state=20 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "kmeans = model.fit(X)\n",
    "predictions = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results _basics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot of samples\n",
    "plt.scatter(df[df['cat']==cat1][var1].head(1000), df[df['cat']==cat1][var2].head(1000), s=10, alpha=0.2)\n",
    "plt.scatter(df[df['cat']==cat2][var1].head(1000), df[df['cat']==cat2][var2].head(1000), s=10, alpha=0.2)\n",
    "\n",
    "# Plot of clusters\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='black', marker='o', s=250)\n",
    "\n",
    "plt.xlabel(var1)\n",
    "plt.ylabel(var2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "What happens when we increase the **`max_iter`** parameter? Does the model performs better? Are all samples assigned correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It's your turn!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results _validation_\n",
    "For a first validation, we use a test data set of the next thousand samples of each marbles type. We show the wrongly assigned samples in the plot with a red marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "marbles_test = pd.concat([\n",
    "    df[df['cat']==cat1].loc[1001:2000][[var1,var2]],\n",
    "    df[df['cat']==cat2].loc[1001:2000][[var1,var2]]\n",
    "]).values\n",
    " \n",
    "# Apply trained model test dataset\n",
    "predictions = kmeans.predict(marbles_test)\n",
    "\n",
    "# Truth of test dataset\n",
    "marbles_true = np.array([1 for i in range(1000)] + [0 for i in range(1000)])\n",
    "\n",
    "# Check which sample is wrongly assigned\n",
    "marbles_false = marbles_test[(predictions - marbles_true) != 0]\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(marbles_test[:,0][:1000], marbles_test[:,1][:1000], s=20, alpha=0.2)\n",
    "plt.scatter(marbles_test[:,0][1000:2000], marbles_test[:,1][1000:2000], s=20, alpha=0.2)\n",
    "\n",
    "plt.scatter(marbles_false[:,0], marbles_false[:,1], color='red', marker='v', s=50)\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='black', marker='o', s=250)\n",
    "plt.xlabel(var1)\n",
    "plt.ylabel(var2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Give it another try and check what happen in each iteration. The red data points show the wrongly assigned samples. Maybe you can find other parameters which improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More marble types\n",
    "\n",
    "We are going to give it a try to see how K-means performs when we have more possible clusters. Now we use all marbles types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "X = df[df['cat'].isin(cat)][['X','Y','cat']]\n",
    "\n",
    "# Reduced data set size\n",
    "X = X.sample(10000)\n",
    "\n",
    "# Define target for visualiztion\n",
    "target = X['cat']\n",
    "X=X.drop(['cat'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], color=cmap(target), s=5, label ='Truth')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(\n",
    "    n_clusters=9,  # number of clusters\n",
    "    max_iter=5,  # number of itereations\n",
    "    \n",
    "    n_init=1, \n",
    "    init='random', \n",
    "    precompute_distances=False, \n",
    "    random_state=20 \n",
    ")\n",
    "kmeans = model.fit(X)\n",
    "predictions = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, color = cmap(predictions), alpha=0.5, label='Predictions K-Means')\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='black', marker='o', s=150, label ='Cluster centers')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], color=cmap(target), s=5, label='Truth')\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='black', marker='o', s=150, label ='Cluster centers')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Reduce the number of marbles types and rerun K-means. Do not forget to adapt the `n_cluster` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "K-means is an easy to use unsupervised classifier but we see that it is highly dependent on the distribution itself. Even clearly separated samples are hard to cluster especially when having a complicated geometry. In addition, overlaying data is not possible to handle at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture\n",
    "\n",
    "The results of a [**Gaussian Mixture**](http://scikit-learn.org/stable/modules/mixture.html) algorithm look like the K-means clusters but elliptical distribution can be handled. Instead of using only distances like in K-means it assumes that all data points belong to a mixture of Gaussian distributed clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data set\n",
    "size = 1000\n",
    "cat1=1\n",
    "cat2=4\n",
    "var1='X'\n",
    "var2='Y'\n",
    "\n",
    "dataset = pd.concat([\n",
    "    df[df['cat']==cat1][[var1,var2]].head(size),\n",
    "    df[df['cat']==cat2][[var1,var2]].head(size)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "model = GaussianMixture(n_components=2)\n",
    "X = X.sample(1000).values\n",
    "model.fit(X)\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c = cmap(predictions), cmap='Set1', label='Predictions Gaussian Mixture')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More marble types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 9000\n",
    "cat=[0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "X = df[df['cat'].isin(cat)][['X','Y','cat']]\n",
    "\n",
    "# Reduced data set size\n",
    "X = X.sample(size)\n",
    "\n",
    "# Define target for visualiztion\n",
    "target = X['cat']\n",
    "X=X.drop(['cat'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cmap(target), s=5, alpha = 0.5, label ='Truth')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianMixture(n_components=9, init_params='kmeans')\n",
    "model.fit(X)\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, color = cmap(predictions), alpha=0.5, label='Predictions Gaussian Mixture')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "The results of the clustering of all marbles are highly dependent on the `init_params` parameter which can be set to `random` or `kmeans`. Some of the detected clusters look quite promising, but as one would expect, overlapping samples can not be clearly separated.\n",
    "\n",
    "## Task\n",
    "- Try to find as much as possible types of marbles which can be clustered with Gaussian Mixture. You may have to tune the parameters of the model!\n",
    "\n",
    "- In addition, try to use more features to train the model. For visualization you should stick to X and Y <br>`X = df[df['cat'].isin(cat)][['X','Y','R','G','B','I','Phi','cat']]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "[DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.dbscan.html) (**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise) is another clustering algorithm. One important feature is that we do not have to set the number of clusters. Rather, we need to tune the key parameter `eps` (minimal distance between two data points), which will determine how many clusters are foudn. This can vary highly with the sample size. A good illustration of the strategy can be found [here](https://en.wikipedia.org/wiki/DBSCAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data set\n",
    "size = 1000\n",
    "cat1=1\n",
    "cat2=4\n",
    "var1='X'\n",
    "var2='Y'\n",
    "\n",
    "dataset = pd.concat([\n",
    "    df[df['cat']==cat1][[var1,var2]].head(size),\n",
    "    df[df['cat']==cat2][[var1,var2]].head(size)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "model = DBSCAN(eps=5, min_samples=10)\n",
    "X = X.sample(1000).values\n",
    "predictions = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sort=np.sort(predictions)\n",
    "if pred_sort[-1]+1 == 0:\n",
    "    print('There are no clusters')\n",
    "else:\n",
    "    print(f'Number of clusters: {pred_sort[-1]+1}')\n",
    "    print(f'There are {sum(pred_sort==-1)} samples without an assigned cluster (Noise).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, c = cmap(predictions), cmap='Set1', label='Predictions DBSCAN')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More marble types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 9000\n",
    "cat=[0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "X = df[df['cat'].isin(cat)][['X','Y','cat']]\n",
    "\n",
    "# Reduced data set size\n",
    "X = X.sample(size)\n",
    "\n",
    "# Define target for visualiztion\n",
    "target = X['cat']\n",
    "X=X.drop(['cat'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cmap(target), s=5, alpha = 0.5, label ='Truth')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=5, min_samples=10)\n",
    "predictions = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sort=np.sort(predictions)\n",
    "if pred_sort[-1]+1 == 0:\n",
    "    print('There are no clusters')\n",
    "else:\n",
    "    print(f'Number of clusters: {pred_sort[-1]+1}')\n",
    "    print(f'There are {sum(pred_sort==-1)} samples without an assigned cluster (Noise).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=20, color = cmap(predictions), alpha=0.5, label='Predictions DBSCAN')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for K-Means, the method struggles with overlapping distributions. However, it is capable of clustering more difficult geometries of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "- Try to find as much as possible types of marbles which can be clustered with DBSCAN. You may have to tune the parameters of the model!\n",
    "\n",
    "- In addition, try to use more features to train the model. For visualization you should stick to X and Y <br>`X = df[df['cat'].isin(cat)][['X','Y','R','G','B','I','Phi','cat']]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's your turn!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
