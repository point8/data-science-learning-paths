{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Regression\n",
    "\n",
    "This chapter introduces essential concepts for applying machine learning to **regression problems**, including **regressor quality metrics**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We speak of **regression** if the model outputs a _continuous_ variable, i.e. a numeric value. The machine learning algorithm often performs this task by **fitting** a function or curve to the data points so that it describes the data well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/500px-Linear_regression.svg.png)\n",
    "\n",
    "**regression** \n",
    "Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Linear_regression.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning **regressor** outputs a statistical model that outputs a numeric value for the given data point. Just as a classifier, model is trained by supervised learning from a set of labelled examples, the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few examples of ML algorithms used for regression. You will recognize some of them, as many algorithms can be applied to learn regression as well as classification models:\n",
    "\n",
    "- **linear regression**\n",
    "- **decision tree**\n",
    "- **random forest**\n",
    "- **gradient boosted trees**\n",
    "- **support vector machine**\n",
    "- **artificial neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider **decision tree learning**. When using this ML technique for classification, the **leaves** of the learned decision tree contain prediction for classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/en/4/4f/GEP_decision_tree_with_numeric_and_nominal_attributes.png) \n",
    "\n",
    "_A decision tree for classification._\n",
    "\n",
    "Source: [Wikipedia](https://upload.wikimedia.org/wikipedia/en/4/4f/GEP_decision_tree_with_numeric_and_nominal_attributes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the leaves of the tree can just als well contain numeric values. In the 1-dimensional example below, we see that a decision tree can output a kind of step function that approximates the curve of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_001.png) \n",
    "\n",
    "\n",
    "\n",
    "Source: [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating a regression model, most of the concepts and techniques from the chapter [ðŸ““ About Classification](../ml/ml-classification-intro.ipynb) are the same: Evaluate the model using **train-test split** or **cross-validation**, avoid overfitting, etc. However, since our regression model outputs a number from a continuous range, and not a class prediction, we have to look at other error metrics not based on counting true/fase positives/negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor Performance/Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All **error metrics** (also called **loss functions**) receive as input the true values $y$ and the values $\\hat{y}$ that the model predicts. We write the number of values in each as $n$.\n",
    "\n",
    "In general, a performance/error metric is a function $M$ that takes  actual values of the time series $y$ and the coresponding forecasted values $\\hat{y}$.\n",
    "\n",
    "$$M(y, \\hat{y}) = \\dots$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error (MAE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the simplest possible metrics: The mean difference between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{MAE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=0}^{n-1} \\left| y_i - \\hat{y}_i \\right|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error**\n",
    "\n",
    "It is common to 'square' error terms for the following reasons:\n",
    "- avoid positive and negative errors cancelling each other out\n",
    "- penalize large errors more than small errors\n",
    "\n",
    "This leads to the **Mean Squared Error (MSE)** metric. One drawback is that the resulting values are not of the same magnitude as the time series values. To mitigate this, we take the root of the score, and end up with **Root Mean Squared Error (RMSE)**:\n",
    "\n",
    "$$R M S E(y, \\hat{y}) =\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-{\\hat{y}}_{i}\\right)^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R^2 Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ score or **coefficient of determination** has the nice property that a perfect model has a score of 1.0, a trivial model that always outputs the **expected value** of the has a score of 0, and scores can be negative for arbitrarily bad models. It is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n- 1} (y_i - \\bar{y})^2}$$ where $$\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Percentage Error**\n",
    "\n",
    "Absolute error metrics like RMSE work well for comparing models applied to the same data set. But what about comparing effectiveness across multiple datasets, especially those of different magnitude? Here, relative error metrics are more suitable.\n",
    "\n",
    "One relative metric is **Mean Average Percentage Error (MAPE)**:\n",
    "\n",
    "$$M A P E(y, \\hat{y}) = \\frac{100}{n} \\sum_{i=1}^{Nn}\\left|\\frac{y_{i}-\\hat{y}_{i}}{y_{i}}\\right|$$\n",
    "\n",
    "A limitation of MAPE is that it is not defined if the actual values contain zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return numpy.mean(numpy.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few commonly used metrics, since the possibilities for defining an error metric are endless. Depending on the definition, the metric penalizes some errors more than others, and will steer your model into a certain direction. Always think about whether the error metric makes sense for the given regression task. Perhaps even a custom error metric, specifically chosen for the domain in which you are working, can be the right choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
