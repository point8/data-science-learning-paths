{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "This chapter serves as an introduction to feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths\n",
    "data_science_learning_paths.setup_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Feature Selection?\n",
    "\n",
    "There are a number of reasons for explicitly limiting the number of features used for a model:\n",
    "\n",
    "- **generalization/predictive performance**: Avoid overfitting due to a large number of features.\n",
    "- **interpretability**: A model that relies on few features and can be easier to explain and interpret.\n",
    "- **saving resources**: Fewer input features can result in significantly shorter training times and less memory use - depending on the learning algorithm\n",
    "- **avoiding the curse of dimensionality**: The [curse of dimensionality](https://en.m.wikipedia.org/wiki/Curse_of_dimensionality) refers to problems resulting from high dimensionality of the data. As the volume of the attribute space increases strongly with the number of dimensions/features, data quickly becomes sparse, making it difficult to detect structure or get statistically significant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of feature selection, we can decide to remove features for two different reasons:\n",
    "- **irrelevance**: There is no important association of that feature and the target\n",
    "- **redundancy**: Another feature is present that contains the same information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Feature Selection vs. Dimensionality Reduction**\n",
    "\n",
    "Feature selection should be distinguished from **dimensionality reduction** methods (like Principal Component Analysis). In both cases we reduce the number of attributes in the dataset, but \n",
    "- dimensionality reduction methods do so by creating new combinations of attributes\n",
    "- feature selection methods include and exclude attributes present in the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: House Price Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_science_learning_paths.datasets.read_house_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"SalePrice\"\n",
    "features = data.columns.difference([target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [target, \"1stFlrSF\", \"LotArea\", \"OverallQual\", \"BedroomAbvGr\"]\n",
    "seaborn.pairplot(\n",
    "    data = data[selected_columns],\n",
    "    plot_kws={\"alpha\": 0.1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features import Rank2D\n",
    "\n",
    "f, ax = plt.subplots(1, 1,figsize=(10, 10))\n",
    "\n",
    "# Instantiate the visualizer with the Covariance ranking algorithm\n",
    "visualizer = Rank2D(\n",
    "    features=features,\n",
    "    algorithm='pearson',\n",
    "    ax=ax\n",
    ")\n",
    "visualizer.fit(data[features], data[target])                # Fit the data to the visualizer\n",
    "visualizer.transform(data)             # Transform the data\n",
    "visualizer.poof()                   # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Algorithms\n",
    "\n",
    "- **filter**: Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable. An example: Filtering features by correlation to the target variable.\n",
    "- **search**: Here the selection is considered as a combinatorial search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model us used to evaluate a combination of features and assign a score based on model performance. An example is the Recursive Feature Elimination algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recursive Feature Elimination via Cross-Validation (RFECV)](https://www.scikit-yb.org/en/latest/api/features/rfecv.html) is a feature set search algorithm. Its basic mechanism is that it \n",
    "\n",
    "> ... fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model’s `coef_` or `feature_importances_` attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_science_learning_paths.mlp import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to limit search time, we select a fraction of the available features as the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_features = pandas.Series(features).sample(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`yellowbrick`](https://www.scikit-yb.org/en/latest/) library provides a wrapper over the `sklearn` implementation that visualizes the feature selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from yellowbrick.features import RFECV\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # needed due to flood of deprecation warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    viz = RFECV(\n",
    "        model=RandomForestRegressor(),\n",
    "        scoring=make_scorer(root_mean_squared_error, greater_is_better=False),\n",
    "        cv=3,\n",
    "    )\n",
    "    viz.fit(data[sampled_features], data[target])\n",
    "    viz.poof()\n",
    "    print(\"selected features:\\n \", sampled_features[viz.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection as Part of Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A mistake would be to perform feature selection first to prepare your data, then perform model selection and training on the selected features.\n",
    "\n",
    "-- [An Introduction to Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection should be treated as an integral part of model selection. One should be careful not to do feature selection on the same data that the model is tested on, since this may lead to overfitting and poor generalization. For example, this implies that when using cross-validation to select a model, feature selection should happen within the cross-validation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Model Engineering with Feature Selection\n",
    "\n",
    "Apply feature selection methods to engineer a better house price prediction model. Experiment with different feature selection methods. Use RMSE as the error function and properly evaluate model performance using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/Further Reading\n",
    "\n",
    "- [An Introduction to Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/)\n",
    "- [scikit-learn: Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2022 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
