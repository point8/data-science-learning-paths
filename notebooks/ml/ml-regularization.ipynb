{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization: Occam's Razor for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Frustra fit per plura quod potest fieri per pauciora. \n",
    ">\n",
    "> (It is in vain to do with many things what can be done with few things.)\n",
    "\n",
    "-- William of Ockham (c. 1287â€“1347)\n",
    "\n",
    "**Regularization** is a technique that aims to prevent overfitting: Since excessive model complexity can lead to overfitting, we introduce a **preference for simplicity** into the objective function when training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_learning_paths.setup_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chapter [ðŸ““ Algorithm Selection and Hyperparameter Tuning](../ml/ml-algo-hyperparameter.ipynb) we have already discussed model complexity and how it can contribute to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10)\n",
    "y = -x**2 + np.random.normal(scale=10.0, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(\n",
    "    x, \n",
    "    np.poly1d(\n",
    "        np.polyfit(x, y, 2)\n",
    "    )(x),\n",
    "    color=\"green\"\n",
    ")\n",
    "plt.plot(\n",
    "    x, \n",
    "    np.poly1d(\n",
    "        np.polyfit(x, y, 42)\n",
    "    )(x),\n",
    "    color=\"red\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is more likely to generalize? Clearly we should prefer the simpler model to the overfitted complex model in this case. In practice, we could treat the order of the polynomial as a hyperparameter and do an explicit parameter search, iteratively increasing the order and testing the performance. \n",
    "\n",
    "However, there is another possible approach: **We can give the model plenty of degrees of freedom while also pushing it in the direction of simplicity during training**. That is regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization** means adding a term to our objective function that penalizes high model complexity. Before, we were only interested in minimizing the error of the predictions \n",
    "\n",
    "$$\\min_f \\sum_{i=1}^{n} E(f(x_i), y_i)$$\n",
    "\n",
    "where:\n",
    "- $f(x_i)$: the output of the model for input $x_i$\n",
    "- $y_i$: the target value for input $x_i$\n",
    "- $E$: the error function \n",
    "\n",
    "Now consider a new, regularized form for the objective function that has an additional term:\n",
    "\n",
    "$$\\min_f \\sum_{i=1}^{n} E(f(x_i), y_i) + \\lambda R(f)$$\n",
    "\n",
    "\n",
    "- $R$: the regularization function\n",
    "- $\\lambda$: a weight controlling the strength of regularization\n",
    "\n",
    "$R$ can be any function that penalizes complexity of the model - usually the number or magnitude of parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Methods Performing Feature Selection\n",
    "\n",
    "Of course, a vast number of regularization terms fit the general form above, and many have been proposed to work well in practice. Among those that **effectively do feature selection**, i.e. that shrink the contribution of unimportant features to zero. They are also referred to as **embedded feature selection**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO or L1 Regularization\n",
    "\n",
    "Let $x$ be the vector of $k$ parameters for the model $f$. Then L1 regularization adds the regularization term\n",
    "\n",
    "$$R(f) = |x|_1 = \\sum_{i=0}^k |x_i|$$\n",
    "\n",
    "\n",
    "This penalizes the absolute magnitude of the parameters, or equivalently, the [L1 norm](http://mathworld.wolfram.com/L1-Norm.html) of the parameter vector.\n",
    "\n",
    "> Lasso is able to achieve both objectives [reducing overfitting and making the model more interpretable] by forcing the sum of the absolute value of the regression coefficients to be less than a fixed value, which forces certain coefficients to be set to zero, effectively choosing a simpler model that does not include those coefficients.\n",
    "\n",
    "-- [Wikipedia](https://en.m.wikipedia.org/wiki/Lasso_(statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: House Price Regression\n",
    "\n",
    "1. Work on the feature-rich house price dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_science_learning_paths.datasets.read_house_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"SalePrice\"\n",
    "features = data.columns.difference([target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore the influence of regularization for types of models that include regularization and their non-regularized counterparts (e.g. `sklearn.linear_model.LinearRegression` vs `sklearn.linear_model.Lasso`).\n",
    "\n",
    "Evaluate the model performance with cross-validation and appropriate error metrics. An example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.scorer import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = make_scorer(mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(\n",
    "    estimator=LinearRegression(),\n",
    "    X=data[features],\n",
    "    y=data[target],\n",
    "    scoring=scoring,\n",
    "    cv=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References/Further Reading\n",
    "\n",
    "- [Cross Validated discussion: Do we still need to do feature selection while using Regularization algorithms?](https://stats.stackexchange.com/questions/149446/do-we-still-need-to-do-feature-selection-while-using-regularization-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2022 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
