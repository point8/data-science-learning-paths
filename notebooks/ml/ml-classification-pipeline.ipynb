{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Machine Learning Pipeline for Classification with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to build a simple predictive model using machine learning. We are going to revisit the Titanic passenger list data set, and use it to train a classifier that tries to determine whether a passenger survived the disaster, based on the person's attributes in the passenger list. This is obviously an educational example using small data, but a similar sequence of steps can be applied to solve real-world predictive analytics tasks on large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths\n",
    "data_science_learning_paths.setup_plot_style(dark=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../.assets/data/titanic/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and always keep the documentation close for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../.assets/data/titanic/titanic-documentation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning pipeline is a sequence of processing steps or stages that leads from the raw data to the desired result, e.g. a trained model or a prediction. **[scikit-learn](https://scikit-learn.org/)** provides an API to map this concept to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphics/sklearn-elements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Estimator` is a learning algorithm, any algorithm that trains on data to generate predictions. An estimator implements a method `fit(X, y)`, which accepts data points or features `X` to learn from, as well as labels `y` on these data points. Unsupervised learning is also supported by some estimators: In this case, one can omit the labels `y.\n",
    "\n",
    "An estimator also implements a `predict(X)` method that accepts data points and returs predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "- [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html): fits a simple linear model to the data to perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pandas.DataFrame(\n",
    "    {\n",
    "        \"x\": numpy.linspace(0, 100, 100),\n",
    "        \"y\": 2 * numpy.linspace(0, 100, 100) + numpy.random.normal(scale=25, size=100),\n",
    "    }\n",
    ")\n",
    "toy_data[\"y_predict\"] = LinearRegression()\\\n",
    "    .fit(\n",
    "        X=toy_data[[\"x\"]], \n",
    "        y=toy_data[\"y\"]\n",
    "    )\\\n",
    "    .predict(toy_data[[\"x\"]])\n",
    "\n",
    "ax = toy_data.plot(kind=\"scatter\", x=\"x\", y=\"y\")\n",
    "toy_data.plot(ax=ax, color=\"r\", kind=\"scatter\", x=\"x\", y=\"y_predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Transformer` implements a method `transform(X)` which converts a tabular dataset (`pandas.DataFrame`, `numpy.ndarray`) into another, and a method `fit(X)` to learn from data how to perform the transformation. This is typically used for preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**\n",
    "\n",
    "- [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler): scales the feature to a given interval\n",
    "- [sklearn.preprocessing.OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html): encodes categorical variables by giving them an integer label\n",
    "- [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html): performs **one-hot-encoding**, i.e. transforms a value of a categorical variable into a binary indicator vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attributes_scaled = pandas.DataFrame(\n",
    "    MinMaxScaler(\n",
    "        feature_range=(0,1)\n",
    "    )\\\n",
    "        .fit(data[[\"Age\", \"Fare\"]])\\\n",
    "        .transform(data[[\"Age\", \"Fare\"]]),\n",
    "    columns=[\"Age scaled\", \"Fare scaled\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attributes_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_attributes_onehot = pandas.DataFrame(\n",
    "    OneHotEncoder(sparse=False)\\\n",
    "        .fit(data[[\"Sex\"]].dropna())\\\n",
    "        .transform(data[[\"Sex\"]].dropna()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_attributes_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Pipeline` is a sequence of steps or stages, which can be `Transformer`s followed by an `Estimator`. A `Pipeline` also behaves like an `Estimator`, implementing the `predict(X)` method.\n",
    "\n",
    "Pipelines are a great tool for making machine learning workflows - including the preprocessing steps - explicit as Python objects. Putting our preprocessing steps that may influence model performance into pipelines, we are on the way to becoming true scikit-learn power users - fully utilizing the tools that the library provides. It also improves reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"regressor\", LinearRegression())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(\n",
    "    example_pipeline.fit(X=toy_data[[\"x\"]], y=toy_data[\"y\"]).predict(toy_data[[\"x\"]])\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocesssing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a classifier that predicts the target variable `Survived` - whether the passenger survived the Titanic disaster - depending on the input columns `Age`, `Fare`, `Sex` and `Embarked`. `Age` and `Fare`  contain numeric values, `Sex` and `Embarked` contain categorical values in the form of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns used in this example\n",
    "data = data[[\"Survived\", \"Age\", \"Fare\", \"Sex\", \"Embarked\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there are a few missing values some of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    print(col, \" : \", data[data[col].isna()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are several strategies to deal with missing values in machine learning, including replacement with suitable **imputed values**, or simply dropping the affected rows.\n",
    " \n",
    "We notice that a significant fraction of passengers have not given their age. Since we don't want to throw away so many passenger records, we will try to estimate a replacement value. On the other hand, throwing away a few rows where the `Embarked` info is missing will probaly not make a difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data[\"Embarked\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, choosing the right strategy for replacing missing values depends on domain knowledge. A simple and common strategy is replacing with the mean of the other observations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(data[[\"Age\"]].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(\n",
    "    SimpleImputer(strategy=\"mean\")\\\n",
    "        .fit(data[[\"Age\"]])\\\n",
    "        .transform(data[[\"Age\"]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorial Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorial attributes in the form of strings, such as `Embarked`, need to be encoded numerically before being readable by the machine learning algorithm. Among different strategies available for this task, a common one is **one-hot-encoding**: The categorical value is replaced by a binary indicator vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False).fit(data[[\"Sex\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(\n",
    "    encoder.transform(data[[\"Sex\"]]),\n",
    "    columns=encoder.categories_\n",
    ").head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go on to the training phase in which a machine learning algorithm ingests the training data to build a predictive model - here, a classifier that predicts yes or no for survival.\n",
    "\n",
    "For this **supervised learning problem**, we split the data into a **feature matrix** $X$ and a **label vector** $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[data.columns.difference([\"Survived\"])], data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many types of classification algorithms exist, each with their own strengths and weaknesses whose discussion goes beyond the scope of this examples. A simple choice is building a single **decision tree**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is an `Estimator` that expects numeric feature matrix and labels as inputs to `fit`, and then features as input to `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simple = data[[\"Age\", \"Fare\", \"Survived\"]].dropna()\n",
    "X_simple, y_simple = data_simple[data_simple.columns.difference([\"Survived\"])], data_simple[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = DecisionTreeClassifier()\\\n",
    "    .fit(X_simple, y_simple)\\\n",
    "    .predict(X_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As discusse in **[ðŸ““ ML for Classification](ml-classification-intro.ipynb)**, we can use a **train-test-split** and the **precision** and **recall** error metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# TODO: replace this with your ML pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"dummy\", DummyClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Assembling the Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the model training workflow and implement it again, ideally as a single `Pipeline` that starts from the raw data and outputs a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn - implement the model training \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint: Consider using the [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to apply different preprocessing steps to different columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
