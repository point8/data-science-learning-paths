{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of marbles - Part II\n",
    "Let's go on with our marble examples. So far we performed the first three general steps of ML.\n",
    "* Data Import and Preparation\n",
    "* Data Exploration\n",
    "* Feature Selection and Engineering\n",
    "* [Model Definition](#Model-Definition-and-Training)\n",
    "* [Training](#Model-Definition-and-Training)\n",
    "* [Basics: Validation and Performance](#Basics:-Validation-and-Performance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, clear_output, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lines(lines):\n",
    "    \"\"\" Parse strings of marble data\"\"\"\n",
    "    lines = lines[2:-2]\n",
    "    rows = [d.split(', ') for d in lines.split('), (')]\n",
    "    data = [[int(v.replace(')][(', '')) for v in r] for r in rows]\n",
    "    return pd.DataFrame(data)[[0, 1, 2]]\n",
    "\n",
    "files = [\n",
    "    'blue-white-glass.data',\n",
    "    'cyan-glass.data',\n",
    "    'glass-blue.data',\n",
    "    'glass-green.data',\n",
    "    'glass-red.data',\n",
    "    'glass-yellow.data',\n",
    "    'planet-black-blue.data',\n",
    "    'planet-green.data',\n",
    "    'planet-ocean.data',\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for i, fname in enumerate(files):\n",
    "    print(f'Load data {i}: {fname}')\n",
    "\n",
    "    with zipfile.ZipFile(f'../.assets/data/marbles/{fname}.zip', 'r') as zipf:\n",
    "        with zipf.open(f'{fname}', 'r') as infile:\n",
    "            content = infile.readlines()[0].decode()\n",
    "            dfs.append(parse_lines(content).assign(color=f'{fname}'.replace('.data', '')))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.columns=['R', 'G', 'B', 'color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xy_values(df):\n",
    "    df['X'] = 0.5 * np.sqrt(3) * df['G'] - 0.5 * np.sqrt(3) * df['B']\n",
    "    df['Y'] = df['R'] - (1 / 3 * df['G']) - (1 / 3 * df['B'])\n",
    "    \n",
    "def generate_intensity_values(df):\n",
    "    df['I'] = np.square(df['X']) + np.square(df['Y'])\n",
    "\n",
    "def generate_angles(df):\n",
    "    df['Phi'] = np.arctan2(df['Y'], df['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_xy_values(df)\n",
    "generate_intensity_values(df)\n",
    "generate_angles(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "df['color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add target ID\n",
    "ids = {'blue-white-glass': 0,\n",
    "       'cyan-glass': 1,\n",
    "       'glass-blue': 2,\n",
    "       'glass-green': 3,\n",
    "       'glass-red': 4,\n",
    "       'glass-yellow': 5,\n",
    "       'planet-black-blue': 6,\n",
    "       'planet-green': 7,\n",
    "       'planet-ocean': 8,}\n",
    "\n",
    "df['cat'] = df['color'].map(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "If we stick to some conventions it is quite easy to use the same workflow to switch between models and compare the results.\n",
    "\n",
    "We use as features our R/G/B values or/and our engineered features. As our data set is prepared, we only have to set our `training_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix data set\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for training\n",
    "training_features = ['R', 'G', 'B']\n",
    "\n",
    "# Set target\n",
    "target = ['cat']\n",
    "\n",
    "# Define input and target & Use only part of the data set to save computing time\n",
    "X = df[training_features + target].dropna().head(300000)\n",
    "y = X[target] # \n",
    "X.drop(target, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an easy start in this example we are going to use the whole data set for training. In a later step we will go through the proper workflow and split the data set into parts for validation and performance checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y\n",
    "\n",
    "X_test = X\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Training\n",
    "\n",
    "Let's start with the actual ML, that is, the fitting of a model to the training data using a machine learning algorithm. In `sklearn` we can import  different algorithms for ML, from simple [decision trees](http://scikit-learn.org/stable/modules/tree.html) to more advanced models like [random forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) or [multi layer perceptron (MLP)](http://scikit-learn.org/stable/modules/neural_networks_supervised.html). `sklearn` provides these algorithms via a consistent interface, so it is easy to switch between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of ML models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step an instance of the algorithm is created with certain settings that influence the learning process (and thereby the model performance). These are called **hyper-parameters**. Hyper-parameters are parameters of the learning algorithm, rather than parameters of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model with key parameters\n",
    "model = RandomForestClassifier(n_estimators=10, max_depth=10)\n",
    "#model = MLPClassifier(hidden_layer_sizes=(20,), activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the actual training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train.values[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the Model\n",
    "\n",
    "To get the best performing model, we would need to tune the algorithms' **hyper-parameters**, that is, experimentally trying out different combinations while measuring model performance. For now, this is an advanced concept that we will revisit later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: Validation and Performance\n",
    "\n",
    "The actual machine learning training is done. Let's have a look at our results and measure how well our model performs.\n",
    "\n",
    "### Hypothesis Test\n",
    "There is an easy way to check the results by visualization. Each chart gives the probability of all samples to belong to one marble type. In addition, each color gives the true membership. A good classifier will show a good splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the probability for a given data set\n",
    "y_proba_test = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat = [0,1,2,3,4,5,6,7,8]\n",
    "cat = [0, 4, 8]\n",
    "\n",
    "for i in cat:\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    for j in range(9):\n",
    "        plt.hist(y_proba_test_i[y_test['cat'] == j], \n",
    "                 bins=np.linspace(0,1,100), \n",
    "                 alpha=0.5, \n",
    "                 density=False, \n",
    "                 label=f'Type {j}')        \n",
    "    \n",
    "    plt.title(f'Hypothesis: Marble belongs to type {i}')\n",
    "    plt.xlabel('Probability')   \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "The [**Receiver Operating Characteristics (ROC**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)) are a slightly more condensed way to validate a model. A ROC curve shows the **true positive rate** (TPR, $\\frac{TP}{P} = \\frac{TP}{TP+FN}$) as a function of the **false positive rate** (FPR, $\\frac{FP}{N} = \\frac{FP}{FP+TN}$) for each class. For each sample the class with the highest probability is chosen for the curve. When given a certain hypothesis and an acceptable false-positive rate, we see how many samples that truly fit the hypothesis we can select. Typically the ROC curve raises quickly and flattens to (1,1). The diagonal would reflect a *random guess*. Keep in mind that both axes show rates and the overall absolute sample size do (can) differ significantly. In addition, the ROC curve can be used to compare within one condensed plot \n",
    "* the performance of different data sets (e.g. training and test data set),\n",
    "* different sets of hyper-parameter of one model \n",
    "* different models.\n",
    "\n",
    "Here, we show the results for the train and test data set in comparison, to detect deviations. Are there significant deviations this could be an indice for overfitting to the train data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat = [0,1,2,3,4,5,6,7,8]\n",
    "cat = [0, 4, 8]\n",
    "\n",
    "y_proba_test = model.predict_proba(X_test)\n",
    "   \n",
    "for i in cat:\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(*roc_curve(y_test == i, y_proba_test_i)[:2], label='test data')\n",
    "    plt.plot([0, 1],[0, 1], color='black', linestyle=':')\n",
    "    plt.title(f'ROC curve type {i}')\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate') \n",
    "    plt.legend(loc='best')\n",
    "    plt.show();   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The [**Confusion Matrix**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) (Table of Confusion) gives for each class how many samples are classified correctly (principal diagonal) and how many classifications are false. In addition, it shows to which wrong class the samples were assigned. In our case we get a 9x9 matrix. The sum of a row are all members of each marble type and the sum of a column returns the predicted members of a class. A _perfect_ classificator would have only entries on the pricipal diagonal. Keep in mind that each sample will be assigned to the class with the highest probability regardless how high it is (worst case: 100%/9 = ~11%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "truth = y_test \n",
    "cm = confusion_matrix(truth,y_pred_test)\n",
    "\n",
    "pd.DataFrame(data=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below outputs a more readable visualization of the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='viridis',vmin=0, vmax=cm[4, 4])\n",
    "plt.colorbar()\n",
    "for i, j in itertools.product(range(9), range(9)):\n",
    "        plt.text(j, i, f'{cm[i, j]:.0f}', horizontalalignment=\"center\",color=\"white\" if not i==j else \"black\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class');        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduced performance indicators\n",
    "\n",
    "There are several performance indicators which only reflect single rates. For example the **True Positive Rate** (TPR, Sensitivity, Hit Rate, Recall) is the rate between True Positives and Positives. It's counterpart is the **True Negative Rate** (TNR, Specificity).\n",
    "\n",
    "* True Positive Rate (TPR, Sensitivity) : $\\frac{TP}{P}$\n",
    "\n",
    "\n",
    "* True Negative Rate (TNR, Specificity) : $\\frac{TN}{N}$ \n",
    "\n",
    "Thereby, we should always take both rates into account to get something like an average. In addition, the **Accuracy** (ACC) can give a hint for that purpose as it covers Positives and Negatives\n",
    "\n",
    "* Accuracy (ACC): $\\frac{TP + TN}{P + N}$\n",
    "\n",
    "The area under the ROC Curve **AUC** (**A**rea **U**nder **C**urve) can be used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc=[]\n",
    "tpr=[]\n",
    "\n",
    "for i in range(9):\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    auc.append(roc_auc_score(y_test.values == i, y_proba_test_i))\n",
    "    tpr.append(model.score(X_test[y_test.values == i], y_test[y_test.values == i]))\n",
    "# Displaying\n",
    "pd.DataFrame({'AUX': np.array(auc), 'TPR': np.array(tpr)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Accuracy: {model.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple model we can obtain a quite reasonable result. However, training and validation has to be an iterative process in machine learning. Getting the last 10-20% percent can be the challenge in the end. One other interesting fact is that all types show different performances. \n",
    "\n",
    "For example a RandomForstClassifier (`RandomForestClassifier: n_estimators=10, max_depth=10`) gives us a accuracy of ~85%. Just by using RGB values without any feature engineering and training the model. Some of the marbles types perform even better and we already get something like around 99% (type 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Several machine learning models return a score for the feature importance within the classificator. This can be used to perform more training steps to improve the model, improve computing time or feedback this to the initial data acquisition. If we detect that one feature is very important for the classificator it maybe a good idea to improve the quality of this feature or engineer equivalent features. In addition, this step can highlight features which were not be be expected to be important and can lead to a rethinking of strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (type(model) is not MLPClassifier):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.barh(range(len(X.columns)), model.feature_importances_)\n",
    "    plt.yticks(range(len(X.columns)), X.columns)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"MLP does not support feature importances in this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More functionality\n",
    "There are other tools in [**sklearn.metrics**](http://scikit-learn.org/stable/modules/model_evaluation.html) to perform general performance and validation analyses. With some models there comes a [**classification_report**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) which can be take into account. Another often applied strategy is a [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) in which the train-test-split is performed several times on a data set. Thereby averaged performance indicators can be estimated and we get some hints how stable the system is. \n",
    "\n",
    "\n",
    "\n",
    "### Attention\n",
    "All performance indicators are rates and can be much more interesting when checking absolut values. Especially when the different classes have not the same amount of class members.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Export trained model\n",
    "It is possible to save a trained model or open it. Thereby the user can distribute or compare models from different states or types in another instance. More details are [availible online](http://scikit-learn.org/stable/modules/model_persistence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "load_model = pickle.load(open('model.pkl', 'rb'))\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model.pkl') \n",
    "load_model = joblib.load('model.pkl')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "Let's try out different models and tune some of the standard hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### It's your turn!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
