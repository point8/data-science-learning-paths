{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of [ðŸ““Feature Engineering](ml-feature-engineering.ipynb), it can be helpful to incorporate feedback from the machine learning algorithm about which features it relied on for its prediction. Several algorithms therefore provide **feature importance scores** after training. These are also frequently used to explain the predictions of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths\n",
    "data_science_learning_paths.setup_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Feature Importance in Titanic Survival Model\n",
    "\n",
    "In the following, we build a simple classifier on the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../.assets/data/titanic/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many implementations of machine learning models (e.g. found in `scikit-learn`) provide a way to access weights given to the features, depending on how important the features are for the model's decision. Computing and visualizing **feature importance** after model training is a helpful step in feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a simplistic classification model for survival on the Titanic using `scikit-learn`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Sex\"]\n",
    "target =  \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[features + [target]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sex\"] = LabelEncoder().fit_transform(data[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[features], data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance in Various ML Algorithms\n",
    "\n",
    "There is single definition of feature importance that applies universally. Depending on the internals of the ML algorithm, different metrics can be used to quantify how important a feature is for the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithms\n",
    "\n",
    "Recall how a decision tree-based algorithm splits the samples at its nodes to arrive at a decision. This [visual introduction to decision tree learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) might help with that.\n",
    "\n",
    "So each node of the decision tree is associated with a specific feature. Nodes closer to the root of the tree are applied to more samples. Feature importance can therefore be measured by **error reduction in that node, weighted by the number of samples that are routed through it**. More specifically:\n",
    "1. initialize an array `feature_importances` of all zeros with size `n_features`.\n",
    "2. traverse the tree: for each internal node that splits on feature `i`,  compute the error reduction of that node multiplied by the number of samples that were routed to the node and add this quantity to `feature_importances[i]`\n",
    "\n",
    "The algorithm [was first described in 1984](https://books.google.de/books/about/Classification_and_Regression_Trees.html?id=JwQx-WOmSyQC&redir_esc=y). It is easy to imagine how it can be generalized to ensembles of decision trees, such as _Random Forest_ or _Gradient-boosted Trees_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(\n",
    "    classifier,\n",
    "    X, \n",
    "    y,\n",
    "    scoring=\"f1\",\n",
    "    cv=5\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(\n",
    "    dict(zip(X.columns, classifier.feature_importances_)),\n",
    "    index=X.columns\n",
    ").sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(\n",
    "    classifier,\n",
    "    X, \n",
    "    y,\n",
    "    scoring=\"f1\",\n",
    "    cv=5\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(\n",
    "    dict(zip(\n",
    "        X.columns, \n",
    "        classifier.fit(X, y).feature_importances_\n",
    "    )),\n",
    "    index=X.columns\n",
    ").sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, different models require different approaches. For example, **Logistic Regression** is a linear model for binary classification. To gauge feature importance, it is [recommended to extract the coefficients of the model and multiply by the standard deviation of the feature](https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(\n",
    "    classifier,\n",
    "    X, \n",
    "    y,\n",
    "    scoring=\"f1\",\n",
    "    cv=5\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.Series(\n",
    "    dict(zip(\n",
    "        X.columns, \n",
    "        classifier.fit(X, y).coef_[0] * numpy.std(X, axis=0)\n",
    "    )),\n",
    "    index=X.columns\n",
    ").sort_values().plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of using feature importances for explaining a model is proposed in the form of the [SHAP](https://github.com/slundberg/shap) library. The authors describe it as \n",
    "\n",
    "> a unified approach to explain the output of _any_ machine learning model\n",
    "\n",
    "The authors [argue that SHAP has several advantages over other feature attribution methods](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27), besides being algorithm-agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP values\n",
    "# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n",
    "explainer = shap.TreeExplainer(classifier)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Feature Importance in the House Price Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a regression model of your choice on the feature-rich house price data set.\n",
    "2. Inspect the feature importance values provided by different methods\n",
    "3. Experiment with a few model engineering choices and observe how they affect the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = data_science_learning_paths.datasets.read_house_prices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We have already done basic preprocessing on the dataset (dropped very sparse variables, encoded ordinal and categorial variables, etc.). If you don't agree with these steps, feel free to start from the unprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../.assets/data/house/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute documentation is a necessary part of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 30 ../.assets/data/house/data_description.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
