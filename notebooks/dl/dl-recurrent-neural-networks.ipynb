{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent vs Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All network architectures in the previous chapters have been **feed-forward neural networks**: The input passes through the network, is processed through the network's connections and weights, and results in an output. Such a network can be denoted as a simple function:\n",
    "\n",
    "$$f(x_i) \\to y_i$$ \n",
    "\n",
    "A **recurrent neural network (RNN)** on the other hand can be described as a function with a slightly different form:\n",
    "\n",
    "$$f(x_t, h_t) \\to (y_t, h_{t+1})$$\n",
    "\n",
    "Note the indices $t$ and $t+1$, suggesting that there is an order to inputs and outputs: Recurrent neural networks are strong candidates for **sequence learning** - learning from data points whose order matters (like text, audio, or time series data). Unlike feedforward neural networks, a recurrent neural network can use its internal state $h$ to process sequences of inputs, and update it according to new inputs. That gives the network the ability of **sequential memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://miro.medium.com/max/964/1*xn5kA92_J5KLaKcP7BMRLA.gif)\n",
    "\n",
    "_Source: animation by [Raimi Karim](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are therefore, in principle, very powerful in what they can learn and compute. One could say they are trainable general-purpose computers. To say that in technical language, they have been shown to be **Turing complete**, i.e. being able to implement **any algorithm**.\n",
    "\n",
    "Like the one for playing Mario Kart:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<iframe width=\"800\" height=\"600\" src=\"https://www.youtube.com/embed/Ipi40cb_RsI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are frequently applied in\n",
    "\n",
    "- speech recognition\n",
    "- [language translation](http://translate.google.com)\n",
    "- image description\n",
    "- time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Short Memory Problem\n",
    "\n",
    "When training _deep_ neural networks, we can experience an issue - the **[vanishing gradient problem](https://www.quora.com/What-is-the-vanishing-gradient-problem?share=1)**: During gradient-based training, some layers may receive only tiny updates to their weights, leading to a lengthy or unsuccessful learning process. For feedforward neural networks, the issue is caused by the depth of the network and the choice of activation functions, and can be fixed accordingly (popular: the ReLU activation).\n",
    "\n",
    "\n",
    "Now consider how an RNN processing a long sequence of input is similar to a deep neural network - in fact, we can replace every RNN with a feed-forward neural network that is its _unrolled_  version, in which the layers share the weights. \n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/0/05/RNN.png)\n",
    "\n",
    "_Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:RNN.png)_\n",
    "\n",
    "Such a network also suffers from the vanishing gradient problem. And when we use the RNN for sequence learning, we see the problem as **short memory**: _As the network processes more input, it has trouble retaining information from earlier steps._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions: LSTM and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search of networks that do not suffer so much from short memory, but can remember information from arbitrarily long sequences, brings us to two new network building blocks: LSTM and GRU. \n",
    "\n",
    "**Long Short-Term Memory** is a recurrent neural network architecture. LSTM networks contain **LSTM units**, composed of a **cell** and three **gates** - an **input gate**, an **output gate** and a **forget gate**. The cell can store values and the gates regulate read, write and delete operations on the cell. _During training, the LSTM network can learn how to operate memory in order to remember the information that is relevant for the task._  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1125/1*goJVQs-p9kgLODFNyhl9zA.gif)\n",
    "\n",
    "_Source: animation by [Raimi Karim](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ability make LSTM especially suitable for processing sequences of data, including time series data for classification and prediction. LSTM cells that remember information make LSTM networks good at extracting patterns over long input sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative architecture is the **GRU network**: **Gated Recurrent Units (GRU)** is a mechanism that works in a way similar to LSTM cells. They have fewer parameters, and are therefore easier to train that LSTMs, but also have more limited capabilities in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks with `keras` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` provides a number of recurrent neural network architectures as layers:\n",
    "\n",
    "- `keras.layers.SimpleRNN`: fully-connected RNN where the output is to be fed back to input\n",
    "- `keras.layers.LSTM`: LSTM layer\n",
    "- `keras.layers.GRU`: GRU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Illustrated Guide to Recurrent Neural Networks: Understanding the Intuition](https://www.youtube.com/watch?v=LHXXI4-IEns)\n",
    "- [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://www.youtube.com/watch?v=8HyCNIVRbSU)\n",
    "- [Meaning (and proof) of “RNN can approximate any algorithm”](https://stats.stackexchange.com/questions/220907/meaning-and-proof-of-rnn-can-approximate-any-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
