{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Walkthrough: Handwriting Recognition with Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter contains a step-by-step tutorial on how to build a simple image classifier for handwriting recognition using artificial neural networks. Like many tutorials, we are going to use the famous [MNIST](https://en.m.wikipedia.org/wiki/MNIST_database) dataset of handwritten digits. Unlike many tutorials, we are not just going to walk through the code, but use it to introduce the fundamentals of neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build a neural network with the [**TensorFlow**](https://www.tensorflow.org/) framework, mainly through the [**Keras**](https://keras.io/) API. TensorFlow itself is a fairly low-level engine for efficient numerical computing with tensors - the mathematical foundation of neural networks. Keras is a more high-level API for working with neural network models, and it uses TensorFlow as its backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths\n",
    "data_science_learning_paths.setup_plot_style(dark=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MNIST](https://en.m.wikipedia.org/wiki/MNIST_database) dataset is a famous benchmark for handwriting recognition performance. The task is to recognize a handwritten digit from a small grayscale image, i.e. classification with 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train),(X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(X_train[i], cmap=\"binary\")\n",
    "    plt.title(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preprocessing step before training, the grayscale pixel data is scaled to the interval $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the complete workflow for neural network construction, training and classifying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = keras.models.Sequential([\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(128, activation=\"relu\"),\n",
    "  keras.layers.Dropout(0.2),\n",
    "  keras.layers.Dense(n_classes, activation=\"softmax\")\n",
    "])\n",
    "net.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "net.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    ")\n",
    "y_pred = numpy.argmax(net.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(X_test[i], cmap=\"binary\")\n",
    "    plt.title(f\"pred: {y_pred[i]} actual: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of lines of Python - that's it. Now you are ready for deep learning. \n",
    "\n",
    "Just kidding. There is a lot going on in these few lines. Let's start again and go step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **artificial neuron** is the fundamental unit of artificial neural networks. Similar to how biological neurons receive signals and fire when triggered, an artificial neuron computes a weighted sum of inputs, passes it through an activation function and outputs a value. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7d/Artificial_neuron-2.gif/320px-Artificial_neuron-2.gif)\n",
    "\n",
    "\n",
    "The output of the $k$-th neuron is\n",
    "\n",
    "$$y_k =  \\varphi \\left( \\sum_{j=0}^m w_{kj} x_j \\right)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\varphi$: activation function\n",
    "- $w_{kj} \\in w$: weights \n",
    "- $x_j \\in x$: inputs\n",
    "\n",
    "We can also write this more briefly in vector notation:\n",
    "\n",
    "$$y_k = \\varphi \\left ( w x \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Single Neuron with Keras\n",
    "\n",
    "Keras is certainly overkill if a single neuron with one input is all we want, but here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = keras.models.Sequential(\n",
    "    [\n",
    "      keras.layers.Dense(\n",
    "          units=1, \n",
    "          input_shape=(1,),\n",
    "          activation=\"tanh\",\n",
    "          kernel_initializer=\"uniform\", \n",
    "        )  \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `keras`, we think about neural networks as composed of **layers** - more on them later. We have defined a `Sequential` model, a **feed-forward neural network** in which the input passes through a sequence of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have added one layer with the following parameters:\n",
    "\n",
    "- **units**: the number of neurons in the layer\n",
    "- **input shape**: The dimensions of the input array that the layer expects. (Much of network architecture debugging will be about getting the data into the right shape.)\n",
    "- **activation**: The _activation function_\n",
    "- **kernel initializer**: How to set the initial weights - here: uniformly at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the weights for the edges connecting the inputs to the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neuron is now ready to receive inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.eye(3,1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method call sets up the model for training. (We will get to what the parameters mean shortly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some simple data that we want to model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "X = numpy.linspace(0.0, 1.0, n)\n",
    "y = 0.8 * X + numpy.random.normal(0.0, 0.2, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we give the data points to the untrained network and ask for a prediction, we do not get a very useful answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neuron.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, '.')\n",
    "plt.plot(X, y_pred, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed this data to the neuron in the `fit` step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.fit(\n",
    "    X, \n",
    "    y,\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the values for the loss function go down. It seems the neuron has learned something. Now we can use this single-neuron network to predict the target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neuron.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, '.')\n",
    "plt.plot(X, y_pred, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the data more than a few times, the network has (sort of) learned to model the underlying linear relationship. And a look at the weight for the input - the parameter we have optimized in the learning phase - reflects this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing a weighted sum of the inputs, the result is passed through an **activation function** (also called **transfer function**) to determine the final output of the neuron. \n",
    "\n",
    "This is somewhat similar to a biological neuron being triggered if the inputs exceed a certain threshold. In practice a simple step function that switches from 0 to 1 at some threshold value would not work with gradient-based optimization methods, since it is not continuously differentiable and has a derivative of 0 otherwise. \n",
    "\n",
    "Some desirable properties in an activation function include:\n",
    "\n",
    "- _nonlinear_ – When the activation function is non-linear, then a two-layer neural network can be proven to be a [universal function approximator](https://en.m.wikipedia.org/wiki/Universal_approximation_theorem).\n",
    "- _range_: When the range of the activation function is finite, gradient-based training methods tend to be more stable. When the range is infinite, training is generally more efficient.\n",
    "- _continuously differentiable_: Desirable for gradient-based optimization methods. \n",
    "\n",
    "\n",
    "Below we visualize examples of a few commonly used activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(-2.0, 2.0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(\n",
    "    {\n",
    "        \"sigmoid\": keras.backend.eval(\n",
    "            keras.activations.sigmoid(x)\n",
    "        ),\n",
    "        \"RELU\": keras.backend.eval(\n",
    "            keras.activations.relu(x)\n",
    "        ),\n",
    "        \"tanh\": keras.backend.eval(\n",
    "            keras.activations.tanh(x)\n",
    "        )\n",
    "    },\n",
    "    index=x\n",
    ").plot(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the number of trainable parameters for the one-input neuron above is not one but two. This is due to a mechanism called **bias**. One can think of **bias** as receiving an additional weighted input from a neuron that always outputs 1. [As explained here in detail](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks), this is helpful for shifting the activation function left/right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/t2mC3.png)\n",
    "\n",
    "_The effect of bias on the activation function. [(Source)](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the image classifier network and discuss each of its layers. Here is a summary of the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer a graphical summary - this one was made with the [Netron](https://github.com/lutzroeder/netron) application:\n",
    "\n",
    "![](graphics/MNIST_classifier.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flatten**\n",
    "\n",
    "The `Flatten` layer does what it says: In goes a multidimensional array, out comes a 1D-vector. In this case we turn the image into a vector of grayscale values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = X_train[0].copy()\n",
    "plt.imshow(example_img, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(28,28))\n",
    "flat = keras.models.Model(\n",
    "    inputs=inputs, \n",
    "    outputs=keras.layers.Flatten()(inputs)\n",
    ").predict(\n",
    "    example_img.reshape(1,28,28)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20,1))\n",
    "plt.imshow(flat, cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense**\n",
    "\n",
    "A **fully-connected layer**. Every following neuron receives the input from every preceding neuron. Fully-connected layers are highly flexible in what they can learn. They are also computationally expensive, resulting in many weights to be trained.\n",
    "\n",
    "![](graphics/fully-connected-layer.png)\n",
    "\n",
    "[Source](https://www.researchgate.net/figure/The-structure-of-the-fully-connected-layer-a-b-and-the-data-dependency-c_fig2_318025612)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**\n",
    "\n",
    "Adding a **dropout layer** is a **regularization** technique, i.e. a strategy against overfitting. When the `Dropout` layer has been added, randomly selected neurons from the previous layer are ignored - \"dropped out\" - during training. On the forward pass, these neurons do not pass output to the following layer, and any weight updates are not applied to the neuron on the backward pass. [This article](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/) goes into more detail on why this is beneficial for training robust models that are less prone to overfitting.\n",
    "\n",
    "The **dropout rate** is one more parameter that can be tuned, but using a small rate such as 0.2 is a good starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Activation**\n",
    "\n",
    "The last layer of the network is a fully-connected layer with as many neurons as there are classes to predict. Since we are doing classification, the activation function of this layer is special: The [softmax](https://en.m.wikipedia.org/wiki/Softmax_function) function maps the outputs of the neurons in this layer to **class probabilities** - numbers between 0.0 and 1.0 that sum to 1.0. The network then predicts the most likely class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the network architecture, it needs to be compiled to be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the parameters:\n",
    "- **optimizer**: The optimization algorithm that iteratively updates the network's weights to minimize the loss function, usually by some version of gradient descent. Algorithms can differ in their performance. The [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) optimizer is a good default choice. Selecting and tuning optimizers is an advanced topic.\n",
    "- **loss**: The objective function that is minimized to train the network.\n",
    "- **metrics**: Additional error metrics tracked during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training phase, the network is provided with labelled data points. It is evaluated and its output is compared to the desired labels. The **loss function** quantifies the error. \n",
    "\n",
    "While training, the optimizer iteratively adjusts the weights of the network - easily thousands of them - to minimize the loss function. It usually does this by some variant of **gradient descent**: In the search for a minimum, the gradient or slope of the error function is computed at the current parameters, then a step is taken in the negative direction of the gradient. The length of the step is called the **learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*KQVi812_aERFRolz_5G3rA.gif) \n",
    "\n",
    "_Gradient descent for fitting a linear model with the squared error as loss function. [Source](https://medium.com/onfido-tech/machine-learning-101-be2e0a86c96a)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "net.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the additional parameters:\n",
    "- **epochs**: A epoch has passed when the network has processed the entire training set. Multiple passes over the training set are usually necessary to train the network.\n",
    "- **batch size**:  Rather than feeding one observation to the network, computing the output, computing the loss, and updating the weights, we can feed multiple inputs in a batch and update the network weights based on the entire batch. Processing one batch is called an **iteration**. Batches enable efficient use of the GPU, and the size of the GPU sets a limit to useful batch sizes. The size of the batch influences the training time, the error achieved, and the gradients during training. There is no general rule of thumb as to which batch size works out best. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it already knows its objective function and metrics, the network has a simple method to evaluate its performance with a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.evaluate(\n",
    "    X_test, \n",
    "    y_test,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to obtain the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = numpy.argmax(net.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(X_test[i], cmap=\"binary\")\n",
    "    plt.title(f\"pred: {y_pred[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [McCullough-Pitts Neuron with Keras](https://chatbotslife.com/keras-in-a-single-mcculloch-pitts-neuron-317397cccd45)\n",
    "- [Deep Learning in Python](https://www.datacamp.com/community/tutorials/deep-learning-python)\n",
    "- [Dropout Regularization in Deep Learning Models With Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2026 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
