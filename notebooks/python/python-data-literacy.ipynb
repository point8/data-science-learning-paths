{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Literacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gartner defines data literacy as the ability to read, write and communicate data in context, including an understanding of data sources and constructs, analytical methods and techniques applied — and the ability to describe the use case, application and resulting value.\n",
    ">\n",
    "> This all boils down to a simple question, “Do you speak data?” \n",
    "\n",
    "_Source: [A Data and Analytics Leader’s Guide to Data Literacy](https://www.gartner.com/smarterwithgartner/a-data-and-analytics-leaders-guide-to-data-literacy/)_\n",
    "\n",
    "\n",
    "Data literacy is the basic competence to **examine, explore, interpret and reason from data**. Developing data literacy is the foundation of a data-driven organization. Data literacy eventually enables success stories in data science and artificial intelligence. For the aspiring data scientist, it is the basic requirement on which to build more advanced skills. However, not just data experts, but many stakeholders and decision makers can benefit from improving their data literacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an overview of data literacy skills but is not comprehensive. The best way to improve and broaden your skills is to start working with data: Gather experience from various projects and discuss them with colleagues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a small python module, which will support us with some basic functionality.\n",
    "import data_science_learning_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_learning_paths.setup_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Science?\n",
    "\n",
    "![](graphics/data_science.png)\n",
    "\n",
    "The essential ingredients for a successful data science project comprise **software design**, **statistics** and **domain knowledge**. If you leave out one of these skills you either \n",
    "- find yourself in the realm of **Machine Learning** without a relevant use case or with useless results, in the sole combination of software design and statistics, \n",
    "- end up in **traditional research**, if you only combine statistics and domain knowledge,\n",
    "- or you find yourself in the **danger zone**, if you leave out the statistical understanding and the ability to validate your results properly. \n",
    "\n",
    "Thus, all in all, you need a **combination of all three** areas of expertise. This does not mean, that you must find a single person that incorporates all those skills, but rather a **well-mixed team of experts**, who are able to challenge one another and can thus successfully implement projects.\n",
    "\n",
    "Here, we will focus on the most relevant statistical skills one has to bring into data science projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do if we are handed a new, unknown data set? We go and explore.\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** aims to summarize a data sets main characteristics, often with visual methods. It is generally recommended as a preliminary stage to other, more goal-directed types of data analysis, such as **hypothesis testing** and **modelling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python world knows powerful tools for data analysis, and here we import some of them: \n",
    "- The `numpy` library for work with big arrays and generation of distributions.\n",
    "- The `pandas` library for handling tabular data.\n",
    "- The `matplotlib` library for plotting and customizing graphics.\n",
    "- The `seaborn` library for statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will only focus on the output, stay on a higher level and will go into more detail in dedicated notebooks at a later stage of the course.\n",
    "\n",
    "Let't start with a dataset. For more information on the dataset see kaggle challenge: [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_descr = data_science_learning_paths.datasets.read_house_prices_seattle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column descriptions\n",
    "data_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement Scales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables in a data set can have different _measurement scales_ or _levels of measurement_. They tell us something about the nature of the information stored in the values of a variable. Statistics traditionally defines the following four scales:\n",
    "\n",
    "- **nominal**: A _nominal_ variable is a set of labels that are mutually exclusive and have no numeric meaning. They can denote a **category** into which the data item belongs. Classic example: Gender - people might identify as either male, female, or nonbinary on a questionaire.\n",
    "- **ordinal**: In an _ordinal_ variable, the values are **ordered** (>) - their order is meaningful, but calculating an exact difference between them is not known. Classic example: School grades - an A is better than an F.\n",
    "- **interval**: With an _interval scale_, the **difference** of numeric values is meaningful (however,  not their ratio.) Classic example: Temperature in degrees Celsius - 20°C is 10 degrees warmer than 10 °C, but not \"twice as warm\".\n",
    "- **ratio**: A variable on a _ratio scale_ has an **absolute zero** point, so ratios are meaningful. Classic example: Temperature in Kelvin - 20K is indeed twice as warm as 10K.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine our data set and find an example for each measurement scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Measurement scales may not be immediately visible from the *data types* of the variables. An integer may encode a nominal, ordinal, interval or ratio scale variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nominal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postal codes (`zipcode`) are mutually exclusive category labels on each of the houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"zipcode\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we could count how many houses we have in each area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"zipcode\"].value_counts().plot(kind=\"bar\", figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ordinal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grade` variable gives a quality rating to each house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"grade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of ratings. We are taking care that the x-axis shows the grades in their proper order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"grade\"].value_counts().sort_index().plot(kind=\"bar\", title=\"condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**interval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The year that the building was built (`yr_built`) is an example for an interval-scaled variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many years older is the second house than the first house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0][\"yr_built\"] - data.iloc[1][\"yr_built\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"yr_built\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the number of houses built in a specific time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"yr_built\"].plot(kind=\"hist\", bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The house price (`price`) is clearly a ratio scale variable - there is an absolute zero (0$) and we can meaningfully speak of a house being twice as expensive as another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0][\"price\"] / data.iloc[1][\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(data[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measurement scales are important to understand:\n",
    "- they allow us to characterize the kind of information stored in a variable\n",
    "- they influence which methods and operations can be meaningfully applied to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "Keep in mind that we just have a look on the raw data. Working with data will always need some **_handwork_**, where you have to apply your data literacy skils and domain knowhow. For example: Do you have an idea how to make the zipcode to an interval-scaled variable? Maybe you get a much better and more meaningful variable for your analysis by this transformation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, data exploration on a new data set is essential for coming up with ideas for analysis, raise questions, and to derive use cases. Data literacy is more than knowing what kinds of plots and styles do exist, but moreover, one has to know how a plot can be interpreted and which kind of plot will represent one's findings best.\n",
    "\n",
    "Besides visualisation some statistical properties of your collected data set can help you to interpret it. The field of **descriptive statistics** provides some **measures** to help describe the fundamental properties of a given data set.\n",
    "\n",
    "Let's start small and have a look at a simple example. Think of a class of 30 students. They all get lunch money everyday. Let's assume they all get 1 or 2 € per day at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lunch-money distribution\n",
    "student_count = 30\n",
    "np.random.seed(0)\n",
    "lunch_money_fair = np.random.randint(\n",
    "    1,\n",
    "    3,\n",
    "    size=student_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_money_fair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To viualize this, we plot the Student ID (a number between 1 and 30) on the x axis and the respective lunch money as a bar, where the height is the amount in €. We get a distribution that looks quite uniform, thus the distribution is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(np.linspace(1, 30, 30), lunch_money_fair)\n",
    "ax.set(xlabel=\"Student ID\", ylabel=\"Lunch money\", title=\"Distribution of lunch money\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine some properties. \n",
    "* For example we can calculate the **arithmetic mean**, $\\mu$, which is the sum of all elements divided by the number of elements. \n",
    "\t* Mean: $\\mu=\\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "* Or we can calculate the **standard deviation (spread around the mean, std)**, $\\sqrt{\\sigma^2}=\\sigma$, which is given by the square root of the average of the squared deviations from the mean. \n",
    "\t* Standard deviation: $\\text{Std}(x)=\\sigma=\\sqrt{\\sigma^2}=\\sqrt{\\frac{1}{(N-1)}\\sum_{i=1}^N (x_i-\\mu)^2}$\n",
    "* The standard deviation is derived from the **variance**, $\\sigma^2$, the overall spread of our data points, as the square root of it.\n",
    "\t* Variance: $\\text{Var}(x)=\\sigma^2=\\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^2$\n",
    "    \n",
    "Furthermore we can determine the **minimum** and **maximum** value of the lunch-money distribution to get a better feeling of the overall range of values.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_money_fair_mean = np.mean(lunch_money_fair)\n",
    "lunch_money_fair_std = np.std(lunch_money_fair, ddof=1)\n",
    "lunch_money_fair_max = np.max(lunch_money_fair)\n",
    "lunch_money_fair_min = np.min(lunch_money_fair)\n",
    "print(f\"Mean: {np.round(lunch_money_fair_mean,2)} €\")\n",
    "print(f\"Std: {np.round(lunch_money_fair_std,2)} €\")\n",
    "print(f\"Min: {np.round(lunch_money_fair_min,2)} €\")\n",
    "print(f\"Max: {np.round(lunch_money_fair_max,2)} €\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All students seem to get almost the same amount of lunch money. The average is 1.57 € and the values spread ± 0.5 € on average around the mean. Besides this the standard deviation is smaller than the determined mean. By construction the minimum value is 1 € and the maximum is 2€.\n",
    "\n",
    "What happens if we _change the conditions \"slightly\"_? We now assume that one student gets 91 € and all the others get 1 € each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_money_unfair = np.append(np.array(91), np.ones(29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_money_unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(np.linspace(1, 30, 30), lunch_money_unfair)\n",
    "ax.set(xlabel=\"Student ID\", ylabel=\"Lunch money\", title=\"Distribution of lunch money\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a distribition with a clear peak for the first student. Let's see how the properties change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunch_money_unfair_mean = np.mean(lunch_money_unfair)\n",
    "lunch_money_unfair_std = np.std(lunch_money_unfair, ddof=1)\n",
    "lunch_money_unfair_max = np.max(lunch_money_unfair)\n",
    "lunch_money_unfair_min = np.min(lunch_money_unfair)\n",
    "print(f\"Mean: {np.round(lunch_money_unfair_mean,2)} €\")\n",
    "print(f\"Std: {np.round(lunch_money_unfair_std,2)} €\")\n",
    "print(f\"Min: {np.round(lunch_money_unfair_min,2)} €\")\n",
    "print(f\"Max: {np.round(lunch_money_unfair_max,2)} €\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a completely different picture than before. The mean is 4 € and the spread around the mean is on average 16 €, which is 4 times higher than the mean itself. For 29 out of 30 students we overestimate the lunch money by a factor of 4 and for 1 out of 30 students we underestimate the lunch money by a factor of almost 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a more accurate picture of the situation we can calculate the **median**, which is the middle value of the sorted data set. If the number of values is even then the median is calculated as the arithemtic mean of the two data points in the middle. Here, a short example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd number of values\n",
    "np.median(np.array([0, 1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even number of values\n",
    "np.median(np.array([0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's see what the median is for our two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Median fair: {np.round(np.median(lunch_money_fair),2)}\")\n",
    "print(f\"Median unfair: {np.round(np.median(lunch_money_unfair),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives you a more complete picture for the unfair distributed lunch money as it is **not susceptible to outliers** in the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example data set of a class of 30 students is by far not the **basic population** if we want to make statements about the overall distribution of lunch money for example of the whole school or the whole country. So we only have a **random sample** of the **basic population**. Thus, the true average of the basic population can only be approximated by the sample mean.\n",
    "\n",
    "If we now want to calculate the uncertainty of the sample mean, the **Standard error of the mean** is determined by the standard deviation divided by square root of the number of elements\n",
    "* Standard error of the mean: $\\text{SEM}(x)=\\frac{\\sigma}{\\sqrt{N}}=\\sqrt{\\frac{1}{N(N-1)}\\sum_{i=1}^N (x_i-\\mu)^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Fair mean (± sem): ({np.round(lunch_money_fair_mean,2)}  ± {np.round(scipy.stats.sem(lunch_money_fair),2)}) €\"\n",
    ")\n",
    "print(\n",
    "    f\"Unfair mean (± sem): ({np.round(lunch_money_unfair_mean,2)} ± {np.round(scipy.stats.sem(lunch_money_unfair),2)}) €\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.sem(lunch_money_fair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentile & Quantiles\n",
    "Some other common values, which describe your data are **percentile** (or quantiles). They go from zero to one (or 0% to 100% in the case of quantiles) and cut your data quantitatively. The 0.5-percentile (or 50% quantile) is the median, the 0.0-percentile the minimum and 1.0-percentile the maximun of the variable, respectively. All percentiles in between can be described equivalently. \n",
    "If you want to discard all values smaller than 0.05 and bigger than the 0.95-percentile for a visualization to get rid of some strange outlayers you can use **percentiles**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum\n",
    "np.percentile(lunch_money_unfair, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum\n",
    "np.percentile(lunch_money_unfair, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median\n",
    "np.percentile(lunch_money_unfair, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Histograms to probabilities and distributions\n",
    "\n",
    "One of the most common visualization of an obtained data set from a particular collection of *things* is a **histogram** (e.g. see above the number of houses built in a specific time range). Our data set can be something like the income or the heights of individuals within a group people or the year houses were built in a city or a district. \n",
    "A histogram shows the division of the feature (e.g. income, height or year built) into different classes and their frequency. On the x axis directly adjacent rectangles of the width of the respective class are drawn, whose areas represent the (relative or absolute) class frequencies. The height of each rectangle then represents the (relative or absolute) frequency density, i.e. the (relative or absolute) frequency divided by the width of the corresponding class. Thus, a histogram visualizes the **frequency distribution** of the given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Rolling dice\n",
    "Now, let's have a look at a very simple experiment/data set: **rolling the dice**. We generate a data set, where one or more dice can be rolled at once and the average of the number of dice eyes per roll is calculated and saved. We use a single die and then ten dice. For each configuration we will roll 10 000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sum_of_eyes_per_roll_of_dice(dice_count):\n",
    "    return np.sum([np.random.randint(1, 7) for i in range(dice_count)]) / dice_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_counts = 10000\n",
    "# single dice\n",
    "result_single_dice = [\n",
    "    average_sum_of_eyes_per_roll_of_dice(dice_count=1) for i in range(roll_counts)\n",
    "]\n",
    "# ten dices\n",
    "result_ten_dice = [\n",
    "    average_sum_of_eyes_per_roll_of_dice(dice_count=10) for i in range(roll_counts)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can now have a look at the frequency distributions of the determined averages. We can see clearly that when we roll a single die the average number of eyes per roll is quite **uniformly distributed**. This is confirmed by the field of stochastic, where the probability of rolling a dice is the same for each number, i.e 1/6th. Here we speak of **probability distributions**.\n",
    "\n",
    "\n",
    "When rolling ten dice for 10000 times and calculate the average number of eyes per roll we get a **Normal distribution** (Gaussian distribution) with its typical bell-like shape. A normal distribution is the underlying distribution of many scientific and technical measurements. A fundamental mathematical concept that explains why normal distributions are so common is the [**central limit theorem**](https://en.wikipedia.org/wiki/Central_limit_theorem), which we have kind of confirmed with this little dice experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=200)\n",
    "ax.hist(\n",
    "    result_single_dice,\n",
    "    bins=np.linspace(1, 7, 7),\n",
    "    alpha=0.5,\n",
    "    density=False,\n",
    "    label=\"Single dice\",\n",
    "    color=\"C0\",\n",
    ")\n",
    "ax.hist(\n",
    "    result_ten_dice,\n",
    "    bins=np.linspace(1, 7, 7),\n",
    "    alpha=0.5,\n",
    "    density=False,\n",
    "    label=\"Ten dice\",\n",
    "    color=\"C1\",\n",
    ")\n",
    "ax.set(xlabel=\"Average number of eyes per roll\", ylabel=\"Frequency / bin\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), dpi=200)\n",
    "ax.hist(\n",
    "    result_single_dice,\n",
    "    bins=np.linspace(1, 7, 7),\n",
    "    alpha=0.5,\n",
    "    density=False,\n",
    "    label=\"Single dice\",\n",
    "    color=\"C0\",\n",
    ")\n",
    "ax.set(xlabel=\"Average number of eyes per roll\", ylabel=\"Frequency / bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5), dpi=200)\n",
    "ax.hist(result_ten_dice, bins=20, alpha=0.5, density=False, label=\"Ten dice\", color=\"C1\")\n",
    "ax.set(xlabel=\"Average number of eyes per roll\", ylabel=\"Frequency / bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(result_ten_dice), np.std(result_ten_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **normal distribution** is defined by its **mean** $\\mu$ (or *expected value* of the distribution; in the example from above it is equal to ~3.5) and its **standard deviation**, which is $\\sqrt{\\sigma^2}=\\sigma$ (here around 0.5). For example one $\\sigma$ is often assumed to be half the width of the interval covering the middle two thirds of the values in a sample, thus cover 68.2% of the data.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are many more [probability distributions](https://en.wikipedia.org/wiki/List_of_probability_distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution\n",
    "An exponential distribution occurs often in real life. It describes the amount of waiting time until the next event, i.e. it is the probability distribution of the time between events in a [Poisson process](https://en.wikipedia.org/wiki/Poisson_point_process). \n",
    "\n",
    "When we look for example at the rate of incoming phone calls in a call center, which differs according to the time of day, we find that we can use the exponential distribution as a good approximation for the time until the next phone call arrives, when focussing on an interval where the rate is roughly constant.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(np.random.exponential(scale=5.0, size=100000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples from the dataset\n",
    "It is obvious that we do not see the very clear distributions from above, but rather a superposition or combination of more than one distribution. Thus, we have different influencing factors.\n",
    "\n",
    "When looking at the square-footage of the living area, we see a non-symmetric distribution with a large „tail“ to higher values, i.e. we have a lot of houses with smaller living areas, but also few houses with very large square-footage. But we also see a clear peak between 1000 and 2000 sq. ft. It can be modeled by a normal distribution combined with an exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(data[\"sqft_living\"], bins=100)\n",
    "plt.title(\"function from the dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just assume, you would find a second smaller peak around 3500 square-footage. How do you interpret this? What can this tell you? \n",
    "\n",
    "The answer could be that there are a number of townhouses with exactly the same size of in one area of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 3))\n",
    "plotting_data = data[\"sqft_living\"].copy()\n",
    "plotting_data = pd.concat(\n",
    "    [\n",
    "        plotting_data,\n",
    "        pd.Series(np.random.normal(loc=3500, scale=50, size=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "seaborn.histplot(plotting_data, bins=100)\n",
    "plt.title(\"function from the dataset with additional data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the distribution of the _longitudinal coordinates of the houses_ it is obvious, that we do not have this very prominent peak, but a superpositon of several distributions. Quite interesting, that the distribution has a lot of local minima, i.e. drops often. It could be an indication for a barrier running from north to south. A river could be the cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(data[\"long\"], bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last example, let's have a look at the price. It shows a quite similar shape like the first example of the square-footage. In the next part we will compare both variables and find out if the square-footage has an influence on the overall price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(data[\"price\"], bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "In statistics, [**correlation**](https://en.wikipedia.org/wiki/Correlation_and_dependence) is any **statistical relationship between two random variables**. In common language, a correlation exists if one variable does not vary independently of another.\n",
    "\n",
    "Correlations are useful because they **can indicate a predictive relationship** that can be exploited in practice. For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e. **correlation does not imply causation**, see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphics/spurious-correlation.svg)\n",
    "_Source: [Spurious Correlations](https://www.tylervigen.com/spurious-correlations)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficients\n",
    "\n",
    "A **correlation coefficient** is a number that expresses the strength of a correlation.\n",
    "\n",
    "The [*Pearson correlation coefficient*](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a statistic that measures linear correlation between two variables. It has a value between +1 and −1. A value of +1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/3/34/Correlation_coefficient.png)\n",
    "\n",
    "_Source: [Wikimedia Commons](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the square footage of the home (`sqft_living`) or footage of the lot (`sqft_lot`) with the overall `price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "seaborn.scatterplot(x=\"sqft_living\", y=\"price\", data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"sqft_living\", \"price\"]].corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "seaborn.scatterplot(x=\"sqft_lot\", y=\"price\", data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"sqft_lot\", \"price\"]].corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis and Model Fitting \n",
    "\n",
    "[**Regression analysis**](https://en.wikipedia.org/wiki/Regression_analysis) is a set of statistical processes for estimating the **relationships between a dependent variable** (often called the 'outcome variable' or 'target variable') and one or more **independent variables** (often called 'predictors' or 'features'). The most common form of regression analysis is **linear regression**, in which a line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion is found. \n",
    "\n",
    "**Regression analysis** is primarily used for two conceptually distinct purposes. First, it is widely used for **prediction and forecasting**, where its use has substantial **overlap with the field of Machine Learning**. Second, regression analysis can be used to suggest a causal **relationships between** the independent and dependent **variables**.\n",
    "\n",
    "A basic principle of **regression analysis** is [**Model Fitting**](https://en.wikipedia.org/wiki/Curve_fitting), the process of finding a mathematical function, that has the **best fit to a series of data points**. There are several ways (e.g. using [*Least-Squares Method*](https://en.wikipedia.org/wiki/Least_squares) or the [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)) to used to find the **best fit** of the model's (a simple line, a polynomial of degree $n$ or machine learning model) parameters  to your data. \n",
    "\n",
    "In general, this process is always a kind of **minimization task**. The *Least-Squares Method* minimizes the sum of the squares of the residuals, i.e. the difference of the used model and the data points, when varying the model's parameters. The parameter selection is a crucial step - be it choosing the appropriate degree of freedom of a polynomial or the appropriate set of parameters for a more complex machine learning algorithm.\n",
    "\n",
    "In Python there are several modules for **fitting** and **regression**. A small selection:\n",
    "* [SciPy optimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html), \n",
    "* [Numpy polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html),\n",
    "* [Sklearn  Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Regression\n",
    "\n",
    "The idea is simple: Find the set of parameters of a given function (_Model_) so the sum of the squared **residuals** is minimized.\n",
    "\n",
    "We use some generated data point. In general they follow a linear relationship of `y = 1 + 3x` but we will add some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model or function we want to fit\n",
    "def linear(x, intercept, slope):\n",
    "    return intercept + slope * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate example data set\n",
    "sample_size = 20\n",
    "error = 1\n",
    "\n",
    "## Generate example data\n",
    "x = np.linspace(-2, 2, sample_size)\n",
    "# Get y-values with random noise\n",
    "noise = np.random.normal(0, error, size=sample_size)\n",
    "y = 1 + 3 * x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a linear fit\n",
    "params, cov = opt.curve_fit(linear, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.scatter(x, y, label=\"data\", marker=\"x\")\n",
    "ax.plot(x, linear(x, *params), label=\"Linear fit\", color=\"orange\")\n",
    "ax.legend()\n",
    "print(f\"=== Result: intercept={params[0]:.2f}, slope={params[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Underfitting (bias–variance dilemma)\n",
    "\n",
    "In principle, regardless of the fitting method, the choice of the function will have the highest impact on the performance of a regression or fit. There are two extremes when chosing a model or function:\n",
    "\n",
    "- **Underfitting** (bias) - Underfitting occurs when our model does not have enough parametric \"freedom\" to describe the data.\n",
    "\n",
    "- **Overfitting** (variance) - The opposite problem is called overfitting: The model describes the data points \"too well\", e.g. it describes random noise instead of simplifying and describing the general relationships in the data.\n",
    "\n",
    "It is your task to validate your function and model, if it _interprets_ the data correctly. For sure there are tools and methods to care for that. We will come to that at a later stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](graphics/OverUnderfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
