{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel and Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid gray; padding:10px; width: 95%;\">\n",
    "\n",
    "ðŸ’¡ **Distributed Parallelism vs Shared-Memory Parallelism**\n",
    "\n",
    "- **Distributed Parallelism**: A computing paradigm where a collection of independent computers (nodes), typically interconnected through a network, work together on a task. Each node operates using its own local memory and communicates with other nodes to achieve a common goal.\n",
    "\n",
    "- **Shared-Memory Parallelism**: A computing model where multiple processors (cores) within a single machine access a common shared memory space, allowing for high-speed data exchange and coordination between the processors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared-Memory Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of the Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's standard library (and underlying libraries like OpenBLAS or MKL) is already optimized to take advantage of multiple cores for many operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Parallel matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two large random matrices\n",
    "A = rand(10000, 10000)\n",
    "B = rand(10000, 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This multiplication will run in parallel on all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time C = A * B  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Distributed` package in Julia provides functionality for parallel and distributed computing, including:\n",
    "\n",
    "- Management of worker processes.\n",
    "- Remote execution of functions.\n",
    "- Inter-process communication.\n",
    "- Parallel execution of loops and tasks.\n",
    "- Data movement and aggregation across workers.\n",
    "- Asynchronous programming support.\n",
    "- Error handling in a distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Embarrassingly Parallel Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid gray; padding:10px; width: 95%;\">\n",
    "\n",
    "\n",
    "ðŸ’¡ **Estimating $\\pi$ via Monte Carlo approximation**\n",
    "\n",
    "Curious why this works? Read more on [how to calculate $\\pi$ via Monte Carlo approximation](https://curiosity-driven.org/pi-approximation))\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function calculate_pi(n)\n",
    "    inside = 0\n",
    "    for i = 1:n\n",
    "        x = rand()\n",
    "        y = rand()\n",
    "        inside += (x^2 + y^2) <= 1.0 ? 1 : 0\n",
    "    end\n",
    "    return 4 * inside / n\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time calculate_pi(1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "# Add worker processes equal to the number of available cores\n",
    "addprocs(Sys.CPU_THREADS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @everywhere macro in Julia is used to execute a command on all available worker processes in a distributed computing environment. When you're working with multiple processes (for example, in parallel computing tasks), the @everywhere macro ensures that the enclosed expression is evaluated on each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere println(\"Hello from process $(myid())\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefixing a function definition with @everywhere is done to define the function across all worker processes in a distributed computing environment. Each process has its own separate workspace and does not automatically have access to the functions and variables defined in the main process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelization 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin\n",
    "    \"\"\"\n",
    "        count_inside(n::Int)\n",
    "\n",
    "    Count the number of points that fall inside the unit circle by generating `n` random points.\n",
    "    A point (x, y) is inside the unit circle if x^2 + y^2 <= 1.\n",
    "\n",
    "    # Arguments\n",
    "    - `n::Int`: The number of random points to generate.\n",
    "\n",
    "    # Returns\n",
    "    - `Int`: The count of points that fall inside the unit circle.\n",
    "    \"\"\"\n",
    "    function count_inside(n::Int)\n",
    "        inside = 0\n",
    "        for i = 1:n\n",
    "            x = rand()\n",
    "            y = rand()\n",
    "            inside += (x^2 + y^2) <= 1.0 ? 1 : 0\n",
    "        end\n",
    "        return inside\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    calculate_pi_parallel(total_points::Int)\n",
    "\n",
    "Calculate an estimate of Ï€ using the Monte Carlo method, in parallel.\n",
    "\n",
    "The function distributes the task of generating random points and checking whether they fall\n",
    "inside the quarter of a unit circle across multiple worker processes. It then collects the\n",
    "results from all workers to calculate the final estimate of Ï€.\n",
    "\n",
    "# Arguments\n",
    "- `total_points::Int`: The total number of random points to use for the estimation.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: An estimate of Ï€.\n",
    "\"\"\"\n",
    "function calculate_pi_parallel(total_points::Int)\n",
    "    # Split the work across the workers\n",
    "    points_per_worker = div(total_points, nworkers())\n",
    "    remaining_points = total_points % nworkers()\n",
    "    \n",
    "    # Use @distributed for parallel reduction, summing up the results from each worker\n",
    "    inside_total = @distributed (+) for i = 1:nworkers()\n",
    "        # Handle any remaining points in the last worker\n",
    "        if i == nworkers()\n",
    "            count_inside(points_per_worker + remaining_points)\n",
    "        else\n",
    "            count_inside(points_per_worker)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Calculate pi using the aggregated result\n",
    "    return 4 * inside_total / total_points\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = Int(1e10)  \n",
    "@time pi_estimate = calculate_pi_parallel(n)\n",
    "\n",
    "println(\"Estimate of Ï€: $pi_estimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelization 2.0\n",
    "\n",
    "In our first attempt, we have explicitly partitioned the set of samples and assigned it to the available workers.\n",
    "\n",
    "Is there a way to avoid that? Can we make the solution more elegant by having the samples automatically assigned to workers? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    calculate_pi_parallel_direct(total_points::Int) -> Float64\n",
    "\n",
    "Calculate an estimate of Ï€ using the Monte Carlo method, in parallel, by directly distributing the random point generation across workers.\n",
    "\n",
    "Instead of assigning a fixed number of points to each worker, this function parallelizes the task of generating and checking each point. Each iteration of the internal loop can be run on any available worker, which allows for improved load balancing.\n",
    "\n",
    "# Arguments\n",
    "- `total_points::Int`: The total number of random points to use for the estimation.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: An estimate of Ï€.\n",
    "\"\"\"\n",
    "function calculate_pi_parallel_direct(total_points::Int)\n",
    "    # Use @distributed for parallel reduction with a (+) operator, iterating over each point\n",
    "    inside_total = @distributed (+) for i = 1:total_points\n",
    "        x = rand()\n",
    "        y = rand()\n",
    "        (x^2 + y^2) <= 1.0 ? 1 : 0\n",
    "    end\n",
    "    \n",
    "    # Calculate pi using the aggregated result\n",
    "    return 4 * inside_total / total_points\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time calculate_pi_parallel_direct(Int(1e10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would we use `Distributed` in a cluster environment to parallelize across multiple machines?\n",
    "\n",
    "In this training environment, we do not have a cluster with multiple nodes available. But suppose we had one - configured with a cluster manager, such as [SLURM](https://en.wikipedia.org/wiki/Slurm_Workload_Manager). How would we need to change the above example to parallelize across multiple nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few changes are necessary at the beginning of our code example. We could call `addprocs` while requesting distributed worker processes from the cluster manager via the  `ClusterManagers` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"ClusterManagers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "using ClusterManagers\n",
    "\n",
    "\n",
    "# Request 10 workers from the Slurm scheduler\n",
    "addprocs(\n",
    "    SlurmManager(10), \n",
    "    t=\"00:30:00\"\n",
    ")\n",
    "\n",
    "# the rest of the code remains unchanged!\n",
    "@everywhere begin\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fails in a non-cluster environment, but would give you distributed parallelism with the right cluster configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2026 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
