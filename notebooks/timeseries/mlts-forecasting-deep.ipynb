{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning** is a fashionable term referring to machine learning with artificial neural networks, particularly networks with many layers, so called **deep networks**. Advances in algorithm design, hardware (GPUs...), software tools (TensorFlow etc.) and the availability of training data on the internet have led to a renaissance for neural networks and the ability to train complex models to perform tasks that can rightly be called intelligent.\n",
    "\n",
    "The following notebooks assume that you are familiar with the concepts of deep learning, including:\n",
    "- artificial neurons\n",
    "- artificial neural network architectures and layers\n",
    "- activation functions\n",
    "- training in batches and epochs\n",
    "- ...\n",
    "\n",
    "These concepts are the focus of our course **[ðŸ““ Deep Learning with TensorFlow](index/dlt2-intro-dl-tensorflow-2day.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the chapter [**ðŸ““ Forecasting with \"Shallow\" Learning**](../timeseries/mlts-forecasting-shallow.ipynb), _basically any regressor can be applied to recursive time series forecasting by transforming the time series data into a supervised learning problem._ That of course includes any neural network capable of regression, including **feed-forward neural networks**.\n",
    "\n",
    "In this chapter however, we are going to foucs on types of networks especially suited for learning from sequences of data: **recurrent neural networks**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_science_learning_paths\n",
    "import forecast_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_learning_paths.setup_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the chapter [**ðŸ““ Forecasting with \"Shallow\" Learning**](../timeseries/mlts-forecasting-shallow.ipynb), _basically any regressor can be applied to recursive time series forecasting by transforming the time series data into a supervised learning problem._ That of course includes any neural network capable of regression, including **feed-forward neural networks**.\n",
    "\n",
    "In this chapter however, we are going to foucs on types of networks especially suited for learning from sequences of data: [**ðŸ““ Recurrent Neural Networks**](../dl/dl-recurrent-neural-networks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` provides a number of recurrent neural network architectures as layers:\n",
    "\n",
    "- `keras.layers.SimpleRNN`: fully-connected RNN where the output is to be fed back to input\n",
    "- `keras.layers.LSTM`: LSTM layer\n",
    "- `keras.layers.GRU`: GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Engineering an LSTM Forecasting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the taxi trips dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips = data_science_learning_paths.datasets.read_chicago_taxi_trips_daily()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than other machine learning algorithms, neural networks are sensitive to the magnitude of the input values. It is generally recommended to **scale the input variables to the range of the activation function.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler(\n",
    "    feature_range=(0,1)\n",
    ")\n",
    "scaler.fit(\n",
    "    taxi_trips\n",
    ")\n",
    "scaled_values = scaler.transform(\n",
    "    taxi_trips\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips_scaled = pandas.Series(\n",
    "    sklearn.preprocessing.MinMaxScaler(\n",
    "        feature_range=(0,1)\n",
    "    ).fit(\n",
    "        taxi_trips\n",
    "    ).transform(\n",
    "        taxi_trips\n",
    "    ).reshape(-1, ),\n",
    "    index=taxi_trips.index,\n",
    ")\n",
    "taxi_trips_scaled.freq = \"d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming to Supervised Learning Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training an LSTM network to forecast this time series, we are first preparing the training data by following the approach described in [**Forecasting with \"Shallow\" Learning**](../timeseries/mlts-forecasting-shallow.ipynb): Transforming the time series to a set of labelled data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = forecast_lab.transform_to_labelled_points(taxi_trips[\"Trips\"][:1000], w)\n",
    "X_test, y_test = forecast_lab.transform_to_labelled_points(taxi_trips[\"Trips\"][1000:2000], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NN expects the input to be in shape $(n, k, w)$, where:\n",
    "\n",
    "- $n$: number of samples\n",
    "- $k$: number of features per time step\n",
    "- $w$: number of time steps in window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1, 1, w)\n",
    "X_test = X_test.values.reshape(-1, 1, w)\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.LSTM(\n",
    "            batch_input_shape=(1, 1, w),\n",
    "            units=w,\n",
    "            stateful=True,\n",
    "        ),\n",
    "        keras.layers.Dense(units=1,  activation=\"linear\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the parameters of the LSTM layer:\n",
    "\n",
    "- **stateful**: The documentation states that  \"If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\" We want the layer to maintain state across the training series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    loss=\"mean_absolute_percentage_error\",\n",
    "    optimizer=\"adam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Parameters**\n",
    "\n",
    "\n",
    "- **don't shuffle**: Order of data points matters, so keep the training data by passing `shuffle=False`\n",
    "- **batch size: 1**\n",
    "- **reset states**: When using a [stateful recurrent network](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/) want it to maintain state during the epoch (one pass through the training series), but not between multiple passes - therefore the state is reset manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    network.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        epochs=1\n",
    "    )\n",
    "    network.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.predict(X_test[-1].reshape(1, 1, w).astype(\"float\"), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping the Network for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the `RNNWrapper` class to wrap the code above and make it easy to use the recurrent network in the `ForecastEvaluation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_forecasting = forecast_lab.RNNWrapper(\n",
    "    estimator_class=keras.models.Sequential,\n",
    "    estimator_params={\n",
    "        \"layers\": [\n",
    "            keras.layers.LSTM(\n",
    "                batch_input_shape=(1, 1, w),\n",
    "                units=w,\n",
    "                stateful=True,\n",
    "            ),\n",
    "            keras.layers.Dense(units=1,  activation=\"linear\"),\n",
    "        ]\n",
    "    },\n",
    "    sliding_window_size=w,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following a few recurrent neural networks are passed to the `ForecastEvaluation`. Let's see whether they do well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"MAPE\": data_science_learning_paths.mlp.mean_absolute_percentage_error,\n",
    "    \"RSME\": data_science_learning_paths.mlp.root_mean_squared_error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_size = 365\n",
    "test_window_size = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "w = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layers = [\n",
    "    keras.layers.LSTM(\n",
    "        batch_input_shape=(1, 1, w),\n",
    "        units=128,\n",
    "        stateful=True,\n",
    "    ),\n",
    "    keras.layers.Dense(units=1,  activation=\"linear\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_lab.ForecastEvaluation(\n",
    "        ts=taxi_trips_scaled,\n",
    "        forecasting=forecast_lab.RNNWrapper(\n",
    "            estimator_class=keras.models.Sequential,\n",
    "            estimator_params={\n",
    "                \"layers\": lstm_layers\n",
    "            },\n",
    "            epochs=epochs,\n",
    "            sliding_window_size=w\n",
    "        ),\n",
    "        train_window_size=train_window_size,\n",
    "        test_window_size=test_window_size,\n",
    "        metrics=metrics\n",
    ").evaluate(\n",
    "    k=3, \n",
    "    plot_segments=True,\n",
    "    plot_residuals=True,\n",
    "    plot_pulls=True\n",
    ").get_metrics().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layers = [\n",
    "    keras.layers.SimpleRNN(\n",
    "        batch_input_shape=(1, 1, w),\n",
    "        units=128,\n",
    "        stateful=True,\n",
    "    ),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_lab.ForecastEvaluation(\n",
    "    ts=taxi_trips_scaled,\n",
    "    forecasting=forecast_lab.RNNWrapper(\n",
    "        estimator_class=keras.models.Sequential,\n",
    "        estimator_params={\n",
    "            \"layers\": rnn_layers\n",
    "        },\n",
    "        epochs=epochs,\n",
    "        sliding_window_size=w\n",
    "    ),\n",
    "    train_window_size=train_window_size,\n",
    "    test_window_size=test_window_size,\n",
    "    metrics=metrics\n",
    ").evaluate(\n",
    "    k=2, \n",
    "    plot_segments=True,\n",
    "    plot_residuals=True,\n",
    "    plot_pulls=True\n",
    ").get_metrics().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layers = [\n",
    "    keras.layers.GRU(\n",
    "        batch_input_shape=(1, 1, w),\n",
    "        units=128,\n",
    "        stateful=True,\n",
    "    ),\n",
    "    keras.layers.Dense(units=1,  activation=\"linear\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_lab.ForecastEvaluation(\n",
    "        ts=taxi_trips_scaled,\n",
    "        forecasting=forecast_lab.RNNWrapper(\n",
    "            estimator_class=keras.models.Sequential,\n",
    "            estimator_params={\n",
    "                \"layers\": gru_layers,\n",
    "            },\n",
    "            epochs=epochs,\n",
    "            sliding_window_size=w\n",
    "        ),\n",
    "        train_window_size=train_window_size,\n",
    "        test_window_size=test_window_size,\n",
    "        metrics=metrics\n",
    ").evaluate(\n",
    "    k=2, \n",
    "    plot_segments=True,\n",
    "    plot_residuals=True,\n",
    "    plot_pulls=True\n",
    ").get_metrics().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Engineering Options\n",
    "\n",
    "With neural networks we enter a vast space of possibilities for engineering better models. Parameters to experiment with include:\n",
    "- network architecture: combine RNN layers with the full range of neural network architecture patterns (layers, activation functions...) \n",
    "- dropout: a technique to combat overfitting that is implemented in `keras` RNN layers\n",
    "- [training parameters](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\n",
    "- preprocessing: experiment with [different scaling methods](https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks)\n",
    "- [stacking LSTM layers](https://machinelearningmastery.com/stacked-long-short-term-memory-networks/)\n",
    "- [different forecasting strategies](https://machinelearningmastery.com/multi-step-time-series-forecasting/) (e.g. direct forecasting of multi-step sequence) \n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Incorporating External Variables\n",
    "\n",
    "We can use a similar approach as described in [**Forecasting with \"Shallow\" Learning**](../timeseries/mlts-forecasting-shallow.ipynb) to incorporate multivariate time series or other external variables into the training and forecasting of our model. \n",
    "\n",
    "**Extend the class `forecast_lab.RNNWrapper` to enable passing the `ext_vars` parameter to the `fit` and `forecast` methods!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Pros**\n",
    "\n",
    "+ powerful learning algorithm that can in principle learn any time series pattern\n",
    "+ extensible to multivariate time series: the expected input is a 3D tensor\n",
    "+ vast space of model engineering options \n",
    "\n",
    "**Cons**\n",
    "\n",
    "- vast space of model engineering options\n",
    "- randomness: training results may vary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Time Series Forecasting with the Long Short-Term Memory Network in Python](https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/)\n",
    "- [Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
