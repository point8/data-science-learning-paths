{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Solution: Predicting House Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../.assets/data/house/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After creating a `SparkSession`, we read the contents of the .csv file into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HousePricePredictor\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(f\"{data_dir}/prices.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a schema for this large dataframe beforehand is a daunting task, so we leave the types a the default (string) and cast later as needed. We know however that the prices should be floating point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"SalePrice\", data[\"SalePrice\"].cast(\"DOUBLE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has a large number of columns - let's select some to take a brief look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Solution 1: A Simple First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a minimalistic model to get us started. We use a bit of domain knowledge to select a couple of promising features that are already in a format which the ML algorithm can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features = [\n",
    "                \"OverallQual\",\n",
    "                \"OverallCond\",\n",
    "                \"YearBuilt\",\n",
    "                ]\n",
    "\n",
    "continuous_features = [\n",
    "                \"LotArea\",\n",
    "                \"GrLivArea\",\n",
    "                \"PoolArea\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in discrete_features:\n",
    "    data = data.withColumn(col_name, data[col_name].cast(\"INT\"))\n",
    "    \n",
    "for col_name in continuous_features:\n",
    "    data = data.withColumn(col_name, data[col_name].cast(\"DOUBLE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = discrete_features + continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemble_features = VectorAssembler(inputCols=feature_cols, \n",
    "                                    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "regression = DecisionTreeRegressor(featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[assemble_features, regression])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we evaluate the performance of the regression model. A better model produces smaller errors in the predicted price. The two error metrics we use are **Root-Mean-Squared-Error (RMSE)** and **Mean Average Error (MAE)** between the predicted value and the observed sales price. In order to get robust scores with less random fluctuation, we apply **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                    numFolds=4) \\\n",
    "                    .fit(data.withColumnRenamed(\"SalePrice\", \"label\")) \\\n",
    "                    .avgMetrics[0]\n",
    "\n",
    "mae = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=RegressionEvaluator(metricName=\"mae\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                    numFolds=4) \\\n",
    "                    .fit(data.withColumnRenamed(\"SalePrice\", \"label\")) \\\n",
    "                    .avgMetrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "team_name = \"Minimalists\"\n",
    "print(\"\\t\".join([\"time\", \"team\", \"RMSE\", \"MAE\"]))\n",
    "print(\"\\t\".join([datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), team_name, \"{0:.4f}\".format(rmse), \"{0:.4f}\".format(mae)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better understanding of the error made by the model, plot the distribution of prices, predicted prices, and errors. This can provide useful feedback for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipeline.fit(data.withColumnRenamed(\"SalePrice\", \"label\")).transform(data)\n",
    "predicted[[\"SalePrice\", \"prediction\"]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pd = predicted[[\"SalePrice\", \"prediction\"]].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"SalePrice\"] - predicted_pd[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Solution 2: Philipp's Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientist Philipp also gave house price prediction a try. Experienced in machine learning and predictive modeling, but new to Spark, he spent a few hours and built a more elaborate model. He also described his workflow for us. Let's follow the steps he took - particularly his feature engineering - and see how his model is doing compared to the minimalistic one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Estimator, Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First I take a look at all the columns and decide which one I want to use at first. I choose numeric attributes like number of square feet of different parts of the house or the number of bedrooms, but also \"string\" attributes like the quality description of different parts of the house. In this case, the latter are quite easy to translate to numeric attributes the machine will understand (f.e. for Basement Quality \"Ex\" (Excellent) is a 1, \"Po\" (Poor) is a 6). I even take one categorical attribute into account, the \"SaleCondition\". Because there are only a handful of options for this attribute, I will choose One-Hot-Encoding to prepare it for machine learning. I also split the features into numeric features, features which are some kind of rating and easily convertible to numbers, and the categorical feature. This way it should be easy to add more features later on if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['OverallQual', 'OverallCond', 'YearBuilt', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', \n",
    "                    '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n",
    "                    'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n",
    "                    'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n",
    "                    'PoolArea']\n",
    "rating_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "                   'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish',\n",
    "                   'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n",
    "categorical_features = ['SaleCondition']\n",
    "\n",
    "features = numeric_features+rating_features+categorical_features\n",
    "\n",
    "target = ['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Unfortunately, we cannot simply use show to display the features in an easy way. We have to split the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[:10]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[10:20]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[20:30]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[30:40]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features[40:]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So let's build a simple transformer which shortens our data to only the features we specified here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropper(Transformer):\n",
    "    \"\"\"\n",
    "    Reduce data to only a subset of columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataAfterDrop = data[self.cols] \n",
    "        return dataAfterDrop\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_pipeline = Pipeline(stages=[ColumnDropper(cols=features+target)])\n",
    "data_short = select_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking for Missing Values**\n",
    "\n",
    "> Let's use the code from the previous example to check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_short.columns:\n",
    "    print(col, \" : \", data_short.filter(f\"{col} is NULL\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the attribute data types**\n",
    "\n",
    "> Now I can start converting the data types of the attributes. For numeric features this is simple, it gets hard however when I try convert a rating-type attribute to numeric values. Unfortunately, the data does not follow a uniform convention on how to rate things. For every type of rating, I have to build a translation table... This could be automated using the documentation file, but in this case (because I do not expect to do this again) I choose the manual approach. I create a table index to specify which column uses which table, and then I create the translation tables itself, where every string value is given a numeric value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice example for how much work could be saved by already thinking about later uses of the data when collecting it - a uniform rating scheme would make things simpler. Well, as data scientist we are used to spending most of our time cleaning data, so let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_index = {'ExterQual' : 0,\n",
    "               'ExterCond' : 0,\n",
    "               'BsmtQual' : 0,\n",
    "               'BsmtCond' : 0,\n",
    "               'BsmtExposure' : 1,\n",
    "               'BsmtFinType1' : 2,\n",
    "               'HeatingQC' : 0,\n",
    "               'CentralAir' : 3,\n",
    "               'KitchenQual' : 0,\n",
    "               'Functional' : 4,\n",
    "               'FireplaceQu' : 0,\n",
    "               'GarageFinish' : 5,\n",
    "               'GarageQual' : 0,\n",
    "               'GarageCond' : 0,\n",
    "               'PavedDrive' : 6,\n",
    "               'PoolQC' : 7,\n",
    "               'Fence' : 8}\n",
    "\n",
    "\n",
    "translation_tables = []\n",
    "translation_tables.append({'Ex' : '0', # I have to use strings, because df.na.replace does\n",
    "                       'Gd' : '1', # not support mixed type replacing\n",
    "                       'TA' : '2',\n",
    "                       'Fa' : '3',\n",
    "                       'Po' : '4',\n",
    "                       'NA' : '5'})\n",
    "\n",
    "translation_tables.append({'Gd' : '0',\n",
    "                       'Av' : '1',\n",
    "                       'Mn' : '2',\n",
    "                       'No' : '3',\n",
    "                       'NA' : '4'})\n",
    "\n",
    "translation_tables.append({'GLQ' : '0',\n",
    "                       'ALQ' : '1',\n",
    "                       'BLQ' : '2',\n",
    "                       'Rec' : '3',\n",
    "                       'LwQ' : '4',\n",
    "                       'Unf' : '5',\n",
    "                       'NA' : '6'})\n",
    "\n",
    "translation_tables.append({'N' : '0',\n",
    "                       'Y' : '1'})\n",
    "\n",
    "translation_tables.append({'Typ' : '0',\n",
    "                       'Min1' : '1',\n",
    "                       'Min2' : '2',\n",
    "                       'Mod' : '3',\n",
    "                       'Maj1' : '4',\n",
    "                       'Maj2' : '5',\n",
    "                       'Sev' : '6',\n",
    "                       'Sal' : '7'})\n",
    "\n",
    "translation_tables.append({'Fin' : '0',\n",
    "                       'RFn' : '1',\n",
    "                       'Unf' : '2',\n",
    "                       'NA' : '3'})\n",
    "\n",
    "translation_tables.append({'Y' : '0',\n",
    "                       'P' : '1',\n",
    "                       'N' : '2'})\n",
    "\n",
    "translation_tables.append({'Ex' : '0',\n",
    "                       'Gd' : '1',\n",
    "                       'TA' : '2',\n",
    "                       'Fa' : '3',\n",
    "                       'NA' : '4'})\n",
    "\n",
    "translation_tables.append({'GdPrv' : '0',\n",
    "                       'MnPrv' : '1',\n",
    "                       'GdWo' : '2',\n",
    "                       'MnWw' : '3',\n",
    "                       'NA' : '4'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TypeConverter(Transformer):\n",
    "    \"\"\"\n",
    "    Converts set of columns to numeric types\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, numeric_features=None, rating_features=None, table_index=None, translation_tables=None):\n",
    "        self.numeric_features = numeric_features # List of all numeric columns\n",
    "        self.rating_features = rating_features # List of all rating columns\n",
    "        self.table_index = table_index # What translation table to use for what rating column\n",
    "        self.translation_tables = translation_tables\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        for col in data.columns:\n",
    "            if col in self.numeric_features:\n",
    "                data = (data.withColumn(col, data[col].cast(\"double\")))\n",
    "            elif col in self.rating_features:\n",
    "                trans_table_nr = self.table_index[col]\n",
    "                data = data.na.replace(self.translation_tables[trans_table_nr], value=None, subset=col)\n",
    "                data = (data.withColumn(col, data[col].cast(\"double\")))\n",
    "            #else:\n",
    "                #print(\"Did not convert column \" + col + \" with transformer TypeConverter.\")\n",
    "        return data\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All we need now is to convert the attribute SaleCondition with OneHotEncoding. We also drop the temporarily created column SaleCondition_index and the column SaleCondition and proceed only with the One-Hot encoded variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "stages.append(StringIndexer(inputCol='SaleCondition',\n",
    "                            outputCol='SaleCondition_index')) # One-Hot Encoding does not work with strings...\n",
    "stages.append(OneHotEncoderEstimator(inputCols=['SaleCondition_index'],\n",
    "                                     outputCols=['SaleCondition_onehot']))\n",
    "stages.append(TypeConverter(numeric_features=numeric_features+target, rating_features=rating_features, \n",
    "                                                  table_index=table_index, translation_tables=translation_tables))\n",
    "features_new = features.copy()\n",
    "features_new.remove('SaleCondition')\n",
    "features_new.append('SaleCondition_onehot')\n",
    "stages.append(ColumnDropper(cols=features_new+target))\n",
    "preproc_pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time data_preproc = preproc_pipeline.fit(data_short).transform(data_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Assembling all feature columns into one single columns and renaming the target column to label** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelRenamer(Transformer):\n",
    "    \"\"\"\n",
    "    Drops rows with at least one not-a-number element\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, col=None):\n",
    "        self.col = col\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataRenamed = data.withColumnRenamed(self.col, 'label')\n",
    "        return dataRenamed\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "stages = []\n",
    "stages.append(LabelRenamer(col=target[0]))\n",
    "stages.append(VectorAssembler(inputCols=features_new, outputCol='features'))\n",
    "spark_needs_this_pipeline = Pipeline(stages=stages)\n",
    "data_sparked = spark_needs_this_pipeline.fit(data_preproc).transform(data_preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using regression to predict the target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "model_pipeline = Pipeline(stages=[RandomForestRegressor(featuresCol = 'features', labelCol = 'label')])\n",
    "data_model = model_pipeline.fit(data_sparked).transform(data_sparked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining all pipelines to a single pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[select_pipeline, preproc_pipeline, spark_needs_this_pipeline, model_pipeline])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Philipp's model perform? We run our evaluation code. Compare the results to the minimal model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we evaluate the performance of the regression model. A better model produces smaller errors in the predicted price. The two error metrics we use are **Root-Mean-Squared-Error (RMSE)** and **Mean Average Error (MAE)** between the predicted value and the observed sales price. In order to get robust scores with less random fluctuation, we apply **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                    numFolds=4) \\\n",
    "                    .fit(data) \\\n",
    "                    .avgMetrics[0]\n",
    "\n",
    "mae = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=RegressionEvaluator(metricName=\"mae\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                    estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                    numFolds=4) \\\n",
    "                    .fit(data) \\\n",
    "                    .avgMetrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "team_name = \"Philipp\"\n",
    "print(\"\\t\".join([\"time\", \"team\", \"RMSE\", \"MAE\"]))\n",
    "print(\"\\t\".join([datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), team_name, \"{0:.4f}\".format(rmse), \"{0:.4f}\".format(mae)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better understanding of the error made by the model, plot the distribution of prices, predicted prices, and errors. This can provide useful feedback for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipeline.fit(data).transform(data)\n",
    "predicted[[\"label\", \"prediction\"]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pd = predicted[[\"label\", \"prediction\"]].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.distplot(predicted_pd[\"label\"] - predicted_pd[\"prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
