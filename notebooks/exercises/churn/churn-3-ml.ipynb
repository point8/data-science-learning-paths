{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models\n",
    "We will now apply several machine learning models to our data. First of all we need a bunch of python packages to do the model building and the validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Data Import and Preparation](#Fetch-the-data)\n",
    "* Data Exploration (see notebooks [churn-1](churn-1-exploration.ipynb) and [churn-2](churn-2-exploration-II.ipynb))\n",
    "* [Feature Selection](#Feature-Selection) and Engineering\n",
    "* ...\n",
    "* [Exercise](#Exercise): It will be your tasked to finish the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "You can now choose between different sets of clients. Each one with different issues to solve. We will start with a very basic sample. You can then try out the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../../.assets/data/churn/churn_persona.pkl.zip'\n",
    "try:\n",
    "    df = pd.read_pickle(input_file)\n",
    "    print(('SUCCESS: Everything seems fine, we are good to go.'))\n",
    "except FileNotFoundError:\n",
    "    print(Markdown(f'ERROR: File {input_file} not found. Did you forget to run the create_churn_persona notebook first?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the columns in the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to ensure data quality. Due to some operations on the datasets there might be some NaN values (e.g. from divide-by-zero operations). We have to get rid of them, as they might confuse our machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[np.isnan(df.mail_r), 'mail_r'] = 0\n",
    "df.loc[np.isnan(df.mail_s), 'mail_s'] = 0\n",
    "df.loc[np.isnan(df.bank_r), 'bank_r'] = 0\n",
    "df.loc[np.isnan(df.bank_s), 'bank_s'] = 0\n",
    "df.loc[np.isnan(df.contacts_r), 'contacts_r'] = 0\n",
    "df.loc[np.isnan(df.contacts_s), 'contacts_s'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "We have had a very close look into our data. You can select the relevant features from our dataset here. In this case, you might choose to take them all into account. In reality, you might want to select the most important ones, as in real life data is nearly infinite and ressources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just comment/uncomment the lines you like to select. \n",
    "# Keep the \"churn\" variable. It is needed for the training.\n",
    "\n",
    "training_features = [\n",
    "    'age',\n",
    "    'amount',\n",
    "    'churn', # we will delete it later from our data, as we want to predict it\n",
    "    'contacts',\n",
    "    'd_amount',\n",
    "    'd_pay',\n",
    "    'pay',\n",
    "    'size',\n",
    "    'year',\n",
    "    'bank_r',\n",
    "    'bank_s',\n",
    "    'bank_n',\n",
    "    'mail_r',\n",
    "    'mail_s',\n",
    "    'mail_n',\n",
    "    'contacts_r',\n",
    "    'contacts_s',\n",
    "    'contacts_n'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and results\n",
    "We now split our dataset into the variables used for our predictive model and the result that should be predicted (our churn state). We call the variables X and the results y.\n",
    "\n",
    "In the last line of this block, all datasets with a NaN value are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[training_features].dropna()\n",
    "y = X.churn\n",
    "X.drop('churn', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Set up the machine learning pipeline.\n",
    "\n",
    "1. Prepare the dataset for validation by performing a resonable `train-test-split`\n",
    "2. Define the ML model you want to use and set some standard hyperparameters.\n",
    "3. Perform the training by fitting the model to your train data. Try out to find a way to add a `sample_weight` in this step.\n",
    "4. Do a proper validation by using hypothesis test, roc curves, confusion matrix, scores and feature importance\n",
    "5. Save your model to disc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
