{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore a churn dataset\n",
    "\n",
    "We have extracted customer data from our ERP system. Our aim is to develop a simple churn model. That means we want to identify customers who are likely terminating their contract soon so we can target these customers directly with a marketing campaign. We have to avoid targeting satisfied customers though. Therefore, it is important to reach a good separation between dissatisfied and loyal customers.\n",
    "\n",
    "## Overview:\n",
    "- [Read in data](#Prerequisites)\n",
    "- [Exploring data](#Exploring-data)\n",
    "- [Visualize the data](#Visualize-the-data)\n",
    "- [Significance and uncertainties](#Significance-and-uncertainties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:\n",
    "All features are generated by a simulation from random distributions with some underlying assumption of how people (could) behave.\n",
    "In the end, all customers have two possible states. Either they churn or they do not churn. Under the hood we implemented three types of costumers. These types determine how customers behave if they churn and how the features for our simulation are set. For example, an angry costumer has a higher churn probability than a standard costumer. But still, these standard costumers will have churn rates greater than zero. In addition, we have \"sleepy\" costumers which behave like standard costumers but have a higher churn rate, if woken up (by a call or an e-mail). Sleepy customers should not be woken up in most churn scenarios. They make it more difficult to generate an efficient model to detect churn. See the [create_churn_persona](create_churn_persona.ipynb) notebook for more details.\n",
    "\n",
    "#### Some Words About Toy Data\n",
    "High quality datasets are hard to find in reality. As a matter of fact, in many cases the preparations for high quality data taking take a lot more time, than large parts of the actual data analysis. However, starting early with analysis projects ensure, that you know at least some of the traps before you start datataking.\n",
    "\n",
    "Thus, building simplified models to generate datasets from first principles is a usual way to get around. With such models you can learn bringing up the machinery and start data taking at the same time. Our dataset is such a toy set. So in several aspects it might not reflect reality at 100%. But, it still holds some key features of real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "Let's get everything we need and run some checks…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.75)\n",
    "\n",
    "input_file = '../../.assets/data/churn/churn_persona.pkl.zip'\n",
    "\n",
    "try:\n",
    "    df = pd.read_pickle(input_file)\n",
    "    display(Markdown('**SUCCESS:** Everything seems fine, we are good to go.'))\n",
    "except FileNotFoundError:\n",
    "    display(Markdown(f'**ERROR:** File {input_file} not found. Did you forget to run the create_churn_persona notebook first?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data\n",
    "\n",
    "We have the condensed ERP information already transformed into a [Pandas](http://pandas.pydata.org) [dataframe](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). The dataframe is called `df` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in this dataframe are:\n",
    "- **age:** customer age in years (as a floating number for simplicity)\n",
    "- **amount:** energy consumption of last accounting period (e.g. in kWh)\n",
    "- **bank:** the name of the customer's bank\n",
    "- **churn:** whether the customer ended their contract\n",
    "- **contacts:** number of contacts the customer had with us (everything from phone calls, mails, bills, meter reading and so on)\n",
    "- **d_amount:** _delta_ amount, i.e. the change in energy consumption compared to next-to-last accounting period (positive: the customer consumed more energy)\n",
    "- **d_pay:** _delta_ pay, i.e. the change in invoice amount compared to next-to-last accounting period (positive: the customer pays more than last year)\n",
    "- **mail:** the customer's mail provider\n",
    "- **pay:** the invoice amount of last accounting period\n",
    "- **size:** number of people living in customer's household\n",
    "- **year:** year since when the customer is out customer\n",
    "- **bank_r, bank_s, mail_r, mail_s, contacts_r, contacts_s:** deduced quantities, we'll cover these later\n",
    "\n",
    "Start by exploring the dataset. This gives you some feeling of the data and also helps to do some consistency checks (you should always be sure your data is sensible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### Extract basic statistical quantities\n",
    "\n",
    "Get means, standard deviations and medians of your dataset. You may start with the feature `age`. Check if the data is consistence and and statistical quantities make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_age = df.age.mean()\n",
    "stddev_age = df.age.std()\n",
    "median_age = df.age.median()\n",
    "min_age = df.age.min()\n",
    "max_age = df.age.max()\n",
    "print(f'The average customer is {mean_age:.1f} years old. \\n'\n",
    "      f'The standard deviation of the age distribution is {stddev_age:.1f} years. \\n'\n",
    "      f'The median age is {median_age:.1f} years.')\n",
    "\n",
    "print(f'The youngest customer is {min_age:.1f} years old, the oldest {max_age:.1f} years.')\n",
    "print()\n",
    "\n",
    "print(f'It is important sense to check your data for sensibility like this. '\n",
    "      f'It is plausible that \\nour average customer is {mean_age:.1f} years old and '\n",
    "      f'that most customers are around {mean_age:.2f}–{stddev_age:.2f}={mean_age - stddev_age:.2f} '\n",
    "      f'\\nand {mean_age:.2f}+{stddev_age:.2f}={mean_age + stddev_age:.2f} '\n",
    "      f'years old (and there are no absurdly young or old customers in the dataset).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all sounds sensible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore non-numeric columns\n",
    "\n",
    "The dataset also contains non-numeric values like the customer's bank, mail provider and total contacts. We can explore these values as well. \n",
    "* What are the unique entries (`unique()`)?\n",
    "* What are the `value_counts()` of each feature?\n",
    "* Can you find out what are the feature columns with the suffix *_r*, *_s* and *_n* stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'bank' # 'bank', mail', 'contacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The list of all unique values in feature `{feature}` is {df[feature].unique()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[feature].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=feature)[feature].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduced quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The suffix `_r` describes the ratio r between costumers who churn and do not churn.\n",
    "* The suffix `_s` describes the corresponding uncertainty of this ratio. This uncertainty is highly dependent on sample size.\n",
    "* The suffix `_n` is an unique id for each entry in the respective feature. When you have five different features it goes from one to five. Keep in mind that you induce a type of ordinal scale. Often it is recommended to use [**One Hot Encoding**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) instead and create a new column per unique entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = df[df['churn'] == True].groupby(feature)[feature].count()\n",
    "n2 = df[df['churn'] == False].groupby(feature)[feature].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Costumer who churn having unique entry in feature {feature}:')\n",
    "print(n1)\n",
    "print(f'\\nCostumer who do not churn having unique entry in feature{feature}:')\n",
    "print(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio r\n",
    "r = n1/n2\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error s\n",
    "n = n1+n2\n",
    "s = np.sqrt((r*(1-r)/n))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare value_counts() of suffix `_n` with the respective feature\n",
    "df[feature+'_n'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "\n",
    "After performing basic operations on the data, continue with visualizations. This helps to get a good feeling for the dataset. Start with a histogram of the numeric and non discrete features.\n",
    "\n",
    "Here are some task to perform:\n",
    "* set the size of the figure to a width of 10 and a height of 4\n",
    "* set the `bins` with [np.linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n",
    "* set title and axis labeling\n",
    "* set the limits of the axes and/or use a logscaled axis\n",
    "* normalize your data\n",
    "* add a grid to the plot\n",
    "* go on with another feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'age'\n",
    "plt.figure(figsize = (10,4))\n",
    "do_normalize = False\n",
    "do_logscale = False\n",
    "\n",
    "plt.hist(df[feature], bins = np.linspace(10,110,31), density = do_normalize)\n",
    "plt.xlabel(f'{feature}')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Histogramm of feature {feature}')\n",
    "plt.grid()\n",
    "if (not do_normalize) & (not do_logscale):\n",
    "    plt.ylim(0,10000)\n",
    "if do_logscale:\n",
    "    plt.yscale('log')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the age distribution of our customers, i.e. how many (y axis) of or customers are in which age segment (x axis). Again, this looks plausible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[feature].hist(\n",
    "    figsize=(10,4),\n",
    "    bins = np.linspace(10,110,31),\n",
    "    density = do_normalize,\n",
    ")\n",
    "ax.set_xlabel(f'{feature}')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Histogramm of feature {feature}')\n",
    "if (not do_normalize) & (not do_logscale):\n",
    "    ax.set_ylim(0,10000)\n",
    "if do_logscale:\n",
    "    ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for a non-numeric column which is a bit more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new figure to plot in\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# group the customers by bank name, count number of customers for each \n",
    "# bank and plot this distribution\n",
    "ax = df.groupby('bank')['bank'].count().plot(kind='bar')\n",
    "ax.set_xlabel(\"Customer's bank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows us how many customers are with each bank.\n",
    "\n",
    "Adapt the plot for one other feature with discrete values like contacts or the e-mail provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different subsets of the data in one plot (e.g. by churn)\n",
    "\n",
    "Now compare a distribution for two different subsets of costumers. In our case it makes sense to compare customers who churn with customers who do not churn.\n",
    "\n",
    "* create two boolean lists for loyal customers (`churn == False`) and terminated customers (`churn = True`)\n",
    "* plot different features and compare deduced quantities like mean or median\n",
    "* make sure that you have the same bins for both subsets\n",
    "* you may have to set the transparency (see setting for *alpha*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = df['churn']\n",
    "not_churn = ~churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,4))\n",
    "ax = df[not_churn]['age'].plot(kind='hist', bins=50, density=True, legend=True, label='Loyal customers', alpha = 0.5)\n",
    "df[churn]['age'].plot(kind='hist', bins=50, density=True, alpha=0.5, legend=True, label='Terminated customers', ax=ax)\n",
    "ax.set_xlabel(\"Customer Age\")\n",
    "\n",
    "mean_loyal_age = df[churn].age.mean()\n",
    "mean_terminated_age = df[not_churn].age.mean()\n",
    "\n",
    "print(f'The average loyal customer is      {mean_loyal_age:.1f} years old.\\n'\n",
    "      f'The average terminated customer is {mean_terminated_age:.1f} years old.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the age distribution of loyal customers vs. those who have terminated their contract. We clearly see that terminated customers are on average younger than loyal customers. This is the first feature we can use to discriminate between loyal and unsatisfied customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer comparision with non-numeric data\n",
    "We can also gain insight from the non-numeric data. For example, we might be interested what the ratio of terminated contracts is depending on the customer's bank. In other words, a customer from a small public local bank might be more loyal than a customer from a large international direct bank. \n",
    "\n",
    "To do that: \n",
    "* select again loyal/terminated customers\n",
    "* group these subsets by bank \n",
    "* count the total number for each bank\n",
    "* devide these numbers to get the ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,4))\n",
    "ax = (df[churn].groupby('bank')['bank'].count()/\n",
    "df[not_churn].groupby('bank')['bank'].count()).plot(kind='bar')\n",
    "ax.set_xlabel(\"Customer's Bank\")\n",
    "ax.set_ylabel(\"Ratio of terminated contracts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, customers from different banks show a different behavior when it comes to loyalty. Another interesting feature to evaluate for our churn model.\n",
    "\n",
    "## Significance and uncertainties\n",
    "\n",
    "The plot below shows that customers with an Interbank account are more likely to terminate their account than customers with a Volkskasse account (It is the same plot as you performed before. Here, we used the deduced quantities instead). **The crucial question is: Is this increase in terminations significant?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = df.groupby('bank').first()['bank_r'].plot(kind='bar')\n",
    "ax.set_xlabel(\"Customer's Bank\")\n",
    "ax.set_ylabel(\"Ratio of terminated contracts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider the following example:** Imagine your dataset contains a few 100000 customers, but only 10 with a bank account from \"Landwirtschaftskasse Gammelsberg\". Out of these 10 customers, 2 have terminated their contract. Thus, the rate of terminated contracts for \"Landwirtschaftskasse Gammelsberg\" (20%) is far higher than the 3%–8% we've seen above. It is very likely that this effect is just a statistical fluctuation. The number of customers with an account at \"Landwirtschaftskasse Gammelsberg\" is too low to draw any conclusions.\n",
    "\n",
    "The quantities we are dealing with here are basically measurements. One can never measure anything with perfect precision. That said, each measurement always comes with an **uncertainty**. The uncertainty is a measure how precisely (or rather unprecisely) the quanitity is known. \n",
    "\n",
    "This is closely related to **[significance](https://en.wikipedia.org/wiki/Statistical_significance)**. In this example, the significance tells us whether the increased contract termination ratio is actually due to customers with bank \"Interbank\" actually being more likely to terminate their contracts or if this increased ratio is just due to low statistics. In general, the larger the statistical sample, the higher the significance of conclusions we can draw from the sample.\n",
    "\n",
    "### Example from the customer dataset\n",
    "\n",
    "The following example is our plot from above with the added uncertainties of each ratio (as a black bar). The height of the bar quantifies how much we expect the ratio to fluctuate just from statistical effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = df.groupby('bank').first()['bank_r'].plot(kind='bar', yerr=df.groupby('bank').first()['bank_s'])\n",
    "ax.set_xlabel(\"Customer's Bank\")\n",
    "ax.set_ylabel(\"Ratio of terminated contracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notes:_ \n",
    "- Here, we use the columns `bank_r` and `bank_s` we saw earlier. These columns carry the pre-calculated termination ratios for the corresponding bank (`bank_r`) and the corresponding uncertainty of this ratio (`bank_s`).\n",
    "\n",
    "We can see how significance depends on the sample size if we consider a smaller sample. Here, we randomly select a 1% subset of our original data and recalculate the `bank_r` and `bank_s` quantities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions comes from the dataset generation notebook. \n",
    "# It adds the three columns of deduced quantities for one feature\n",
    "# We use it to generate new features for a subset of our dataset\n",
    "\n",
    "def add_ratios(df, column):\n",
    "\n",
    "    n1 = df[df['churn'] == True].groupby(column)[column].count()\n",
    "    n2 = df[df['churn'] == False].groupby(column)[column].count()\n",
    "    r=n1/n2\n",
    "    n=n1+n2\n",
    "    s = np.sqrt((r*(1-r)/n))\n",
    "   \n",
    "    index = np.arange(len(df.groupby(column)[column].count().index))+1\n",
    "    dtest = pd.DataFrame(np.transpose([r,s,index]))\n",
    "    dtest.columns=[column+'_r',column+'_s',column+'_n']\n",
    "    dtest.index=df.groupby(column)[column].count().index\n",
    "    return df.join(dtest, on=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_sample = df.sample(frac=0.01)[['age', 'amount', 'bank', 'churn', 'contacts', 'd_amount', 'd_pay',\n",
    "       'mail', 'pay', 'size', 'year']]\n",
    "\n",
    "# Add deduced quantities\n",
    "df_small_sample = add_ratios(df_small_sample, 'bank')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ax = df_small_sample.groupby('bank').first()['bank_r'].plot(kind='bar', yerr=df_small_sample.groupby('bank').first()['bank_s'])\n",
    "ax.set_xlabel(\"Customer's Bank\")\n",
    "ax.set_ylabel(\"Ratio of terminated contracts\")\n",
    "df_small_sample.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ratios that clearly differ from the earlier values. The uncertainty bars are enlarged showing us that we are less certain about the ratios. The ratio for \"Solidbank\" compared to \"Sparbank\", \"Stadtbank, and \"Volkskasse\" is no longer statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to adapt the same plots to check out \n",
    "* how the e-mail providers are distributed between terminated and loyal customers\n",
    "* and how certain you are with these ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = df.groupby('mail').first()['mail_r'].plot(kind='bar', yerr=df.groupby('mail').first()['mail_s'])\n",
    "ax.set_xlabel(\"Customer's e-mail provider\")\n",
    "ax.set_ylabel(\"Ratio of terminated contracts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2024 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
