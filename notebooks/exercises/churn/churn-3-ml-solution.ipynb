{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models\n",
    "We will now apply several machine learning models to our data. First of all we need a bunch of python packages to do the model building and the validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Data Import and Preparation](#Fetch-the-data)\n",
    "* Data Exploration (see notebooks [churn-1](churn-1-exploration.ipynb) and [churn-2](churn-2-exploration-II.ipynb))\n",
    "* [Feature Selection](#Feature-Selection) and Engineering\n",
    "* ...\n",
    "* [Exercise](#Exercise): It will be your tasked to finish the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "You can now choose between different sets of clients. Each one with different issues to solve. We will start with a very basic sample. You can then try out the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../../.assets/data/churn/churn_persona.pkl.zip'\n",
    "try:\n",
    "    df = pd.read_pickle(input_file)\n",
    "    print(('SUCCESS: Everything seems fine, we are good to go.'))\n",
    "except FileNotFoundError:\n",
    "    print(Markdown(f'ERROR: File {input_file} not found. Did you forget to run the create_churn_persona notebook first?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the columns in the dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to ensure data quality. Due to some operations on the datasets there might be some NaN values (e.g. from divide-by-zero operations). We have to get rid of them, as they might confuse our machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[np.isnan(df.mail_r), 'mail_r'] = 0\n",
    "df.loc[np.isnan(df.mail_s), 'mail_s'] = 0\n",
    "df.loc[np.isnan(df.bank_r), 'bank_r'] = 0\n",
    "df.loc[np.isnan(df.bank_s), 'bank_s'] = 0\n",
    "df.loc[np.isnan(df.contacts_r), 'contacts_r'] = 0\n",
    "df.loc[np.isnan(df.contacts_s), 'contacts_s'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "We have had a very close look into our data. You can select the relevant features from our dataset here. In this case, you might choose to take them all into account. In reality, you might want to select the most important ones, as in real life data is nearly infinite and ressources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just comment/uncomment the lines you like to select. \n",
    "# Keep the \"churn\" variable. It is needed for the training.\n",
    "\n",
    "training_features = [\n",
    "    'age',\n",
    "    'amount',\n",
    "    'churn', # we will delete it later from our data, as we want to predict it\n",
    "    'contacts',\n",
    "    'd_amount',\n",
    "    'd_pay',\n",
    "    'pay',\n",
    "    'size',\n",
    "    'year',\n",
    "    #'bank_r',\n",
    "    #'bank_s',\n",
    "    'bank_n',\n",
    "    #'mail_r',\n",
    "    #'mail_s',\n",
    "    'mail_n',\n",
    "    #'contacts_r',\n",
    "    #'contacts_s',\n",
    "    'contacts_n'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and results\n",
    "We now split our dataset into the variables used for our predictive model and the result that should be predicted (our churn state). We call the variables X and the results y.\n",
    "\n",
    "In the last line of this block, all datasets with a NaN value are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[training_features].dropna()\n",
    "y = X.churn\n",
    "X.drop('churn', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Set up the machine learning pipeline.\n",
    "\n",
    "1. Prepare the dataset for validation by performing a resonable `train-test-split` [Go to solution](#Train-Test-Split)\n",
    "2. Define the ML model you want to use and set some standard hyperparameters. [Go to solution](#Model)\n",
    "3. Perform the training by fitting the model to your train data. Try out to find a way to add a `sample_weight` in this step. [Go to solution](#Fiting)\n",
    "4. Do a proper validation by using [hypothesis test](#Hypothesis-Test), [roc curves](#ROC-Curves), [confusion matrix](#Confusion-Matrix), [scores](#Scores-and-deduced-performance-indicators) and [feature importance](#Feature Importance)...\n",
    "5. Save your model to disc. [Go to solution](#Import-and-export-trained-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the data mixed well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_all_data = df['churn'].value_counts().values[1]/df['churn'].value_counts().values[0]\n",
    "ratio_train_data = y_train.sum()/len(y_train)\n",
    "ratio_test_data = y_test.sum()/len(y_test)\n",
    "\n",
    "print(f'{ratio_all_data:.3}')\n",
    "print(f'{ratio_train_data:.3}')\n",
    "print(f'{ratio_test_data:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We choose our Model to be a GradientBoostingClassifier. You could play around with the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Defining the model with key parameters\n",
    "#model = RandomForestClassifier(n_estimators=50, max_depth=7)\n",
    "#model = MLPClassifier(hidden_layer_sizes=(20,), activation='relu')\n",
    "model = GradientBoostingClassifier(n_estimators=50, max_depth=4)\n",
    "#model = AdaBoostClassifier(n_estimators=50)\n",
    "#model = GaussianNB()\n",
    "#model = KNeighborsClassifier(n_neighbors=300)\n",
    "#model = SVC(kernel='rbf', probability=True) #takes longtime, reduce dataset!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_weight = 1 + (y_train > 0) * 10\n",
    "    model.fit(X_train, y_train, sample_weight = sample_weight)\n",
    "except:\n",
    "    print('Exception: sample weights are not supported')\n",
    "    print('Exception: using no weights for fit')\n",
    "    # For a MLPClassifier there is no sample weight directly implemented\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The actual machine learning training is done. Let's have a look at our results and measure how well our model performs on our training data and the test data sets. If we see the same performance on both sets we can take this as a strong indicator for a valid model. If the models performs much better on our training data set, there is something wrong (-> **overtraining**)!\n",
    "\n",
    "### Hypothesis Test\n",
    "There is an easy way to check the results by visualization. Each chart gives the probability of all samples to belong to one marble type. In addition, each color gives the true membership. A good classifier will show a good split: Ideally, all items of the current class will be on the right side and all others on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the probability for a given data set\n",
    "y_proba_test = model.predict_proba(X_test)\n",
    "y_proba_train = model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1]:\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    do_normalize = True\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    \n",
    "    plt.hist(y_proba_test_i[(y_test == 0).values], bins=30, alpha=0.5, density=do_normalize, label='Clients staying')\n",
    "    plt.hist(y_proba_test_i[(y_test == 1).values], bins=30, alpha=0.5, density=do_normalize, label='Clients terminating')\n",
    "    plt.legend()\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.title('Hypothesis: Client will stay with probability p')\n",
    "    elif i == 1:\n",
    "        plt.title('Hypothesis: Client will terminate with probability p')\n",
    "    plt.xlabel('Probability p')\n",
    "    \n",
    "    #plt.yscale('log')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "The [**Receiver Operating Characteristics (ROC**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)) are a slightly more condensed way to validate a model. A ROC curve shows the **true positive rate** (TPR, $\\frac{TP}{P} = \\frac{TP}{TP+FN}$) as a function of the **false positive rate** (FPR, $\\frac{FP}{N} = \\frac{FP}{FP+TN}$) for each class. For each sample the class with the highest probability is chosen for the curve. When given a certain hypothesis and an acceptable false-positive rate, we see how many samples that truly fit the hypothesis we can select. Typically the ROC curve raises quickly and flattens to (1,1). The diagonal would reflect a *random guess*. Keep in mind that both axes show rates and the overall absolute sample size do (can) differ significantly. In addition, the ROC curve can be used to compare within one condensed plot \n",
    "* the performance of different data sets (e.g. training and test data set),\n",
    "* different sets of hyper-parameter of one model \n",
    "* different models.\n",
    "\n",
    "Here, we show the results for the train and test data set in comparison, to detect deviations. Are there significant deviations this could be an indice for overfitting to the train data! In addition, we plot a line for the rates describing the same amount of customers between true positives and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_proba_test = model.predict_proba(X_test)\n",
    "#y_proba_train = model.predict_proba(X_train)\n",
    "   \n",
    "for i in [0,1]:\n",
    "    y_proba_test_i = y_proba_test[:,i]\n",
    "    y_proba_train_i = y_proba_train[:,i]\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(*roc_curve(y_test == i, y_proba_test_i)[:2], label='test')\n",
    "    plt.plot(*roc_curve(y_train == i, y_proba_train_i)[:2], label='train')\n",
    "    \n",
    "    # Add line for same absolut size of customers\n",
    "    if True:\n",
    "        if i == 0:\n",
    "            m = (y_test == True).sum()/(y_test == False).sum()\n",
    "        else:\n",
    "            m = (y_test == False).sum()/(y_test == True).sum()\n",
    "        x = np.linspace(0,1,21)\n",
    "        plt.plot(x, m*x, linestyle= '--', color='red', label='same number \\nof customers')\n",
    "        \n",
    "    plt.plot([0, 1],[0, 1], color='black', linestyle=':')\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.title(f'Clients staying')\n",
    "    if i == 1:\n",
    "        plt.title(f'Clients terminating')\n",
    "        \n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show();   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The [**Confusion Matrix**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) (Table of Confusion) gives for each class how many samples are classified correctly (principal diagonal) and how many classifications are false. In addition, it shows to which wrong class the samples were assigned. In our case we get a 2x2 matrix. The sum of a row either all customers who do not churn (*Negatives*) and all customers who churn (*Positives*). A _perfect_ classificator would have only entries on the pricipal diagonal. Keep in mind that each sample will be assigned to the class with the highest probability regardless how high it is (worst case: 100%/2 = ~50.001%).\n",
    "\n",
    "The sum of the first row are all true members (**Positives**, P) consisting of **True Positives** (TP) and **False Negatives** (FN). The sum of the second row are all false members (**Negatives**, N) consisting of the **False Positives** (FP) and **True Negatives** (TN).\n",
    "\n",
    "x | classified as Negatives | classified as Positives\n",
    "-|-|-\n",
    "**Negatives (N)** | True Negatives (TN) | False Positives (FP) \n",
    " **Positives (P)** |  False Negatives (FN) | True Positives (TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "truth = y_test \n",
    "cm = confusion_matrix(truth,y_pred_test)\n",
    "\n",
    "pd.DataFrame(data=cm, columns=['Predict as staying', 'Predict as terminating'], index=['Stays', 'Terminating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores and deduced performance indicators\n",
    "\n",
    "There are several performance indicators which only reflect single rates. For example the **True Positive Rate** (TPR, Sensitivity, Hit Rate, Recall) is the rate between True Positives and Positives. It's counterpart is the **True Negative Rate** (TNR, Specificity).\n",
    "\n",
    "* True Positive Rate (TPR, Sensitivity, Recall) : $\\frac{TP}{P}$\n",
    "\n",
    "* True Negative Rate (TNR, Specificity) : $\\frac{TN}{N}$ \n",
    "\n",
    "Thereby, we should always take both rates into account to get something like an average. In addition, the **Accuracy** (ACC) can give a hint for that purpose as it covers Positives and Negatives\n",
    "\n",
    "* Accuracy (ACC): $\\frac{TP + TN}{P + N}$ \n",
    "\n",
    "There are other meaningful indicators like **Positive Predicted Value** (PPV, Precision) which describes the rate of True Positives to all as positive classified samples (TP+FP). A good average (harmonic mean) of Precision and Sensitivity comes with the **F1 score**.\n",
    "\n",
    "* Positive Predicted Value (PPV, Precision) : $\\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "* F1 score (harmonic mean of ACC & PPV) : $\\frac{2*PPV * TPR}{PPV + TPR} = \\frac{2 TP}{2 TP + FP + FN}$\n",
    "\n",
    "* Area under the ROC Curve (**AUC**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "target_names = ['loyal customers', 'terminating customers']\n",
    "report = classification_report(y_test, y_pred_test, target_names=target_names)\n",
    "print(report)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "report = classification_report(y_train, y_pred_train, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "i = 0\n",
    "y_proba_test_i = y_proba_test[:,i]\n",
    "y_proba_train_i = y_proba_train[:,i]\n",
    "\n",
    "data.append(roc_auc_score(y_test.values == i, y_proba_test_i))\n",
    "data.append(roc_auc_score(y_train.values == i, y_proba_train_i))\n",
    "    \n",
    "# Displaying\n",
    "pd.DataFrame(np.array(data), columns=['AUC'], index = ['test', 'train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Accuracy of train: {model.score(X_train, y_train):.3f}')\n",
    "print(f'Mean Accuracy of test: {model.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More functionality\n",
    "There are other tools in [**sklearn.metrics**](http://scikit-learn.org/stable/modules/model_evaluation.html) to perform general performance and validation analyses. With some models there comes a [**classification_report**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) which can be take into account. Another often applied strategy is a [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) in which the train-test-split is performed several times on a data set. Thereby averaged performance indicators can be estimated and we get some hints how stable the system is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Several machine learning models return a score for the feature importance within the classificator. This can be used to perform more training steps to improve the model, improve computing time or feedback this to the initial data acquisition. If we detect that one feature is very important for the classificator it maybe a good idea to improve the quality of this feature or engineer equivalent features. In addition, this step can highlight features which were not be be expected to be important and can lead to a rethinking of strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Only works for GradientBoostingClassifier or RandomForestClassifier\n",
    "try:\n",
    "    print(model.feature_importances_)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.barh(range(len(X.columns)), model.feature_importances_)\n",
    "    plt.yticks(range(len(X.columns)), X.columns)\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Model does not support feature importances in this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and export trained model\n",
    "It is possible to save a trained model or open it. Thereby the user can distribute or compare models from different states or types in another instance. More details are [availible online](http://scikit-learn.org/stable/modules/model_persistence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "load_model = pickle.load(open('model.pkl', 'rb'))\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model.pkl') \n",
    "load_model = joblib.load('model.pkl')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2022 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
