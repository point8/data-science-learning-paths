{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Team Exercise: Predicting House Prices\n",
    "\n",
    "![](graphics/house-for-sale-sign.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to build another predictive model using machine learning. Our goal is to predict real estate prices, given various attributes of the building.  The main difference to our previous example is that the target variable we are interested in, the sale price, is now a continuous range of values rather than a discrete set of classes. Time to recall the concepts of **classification** and **regression**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs Regression\n",
    "\n",
    "We speak of **classification** if the model outputs a _categorical_ variable, i.e. assigns labels to data points that divide them into groups. The machine learning algorithm often performs this task by creating and optimizing a **decision boundary** in the feature space that separates classes. (The previous chapter introduced an example of a predictive classification model.)\n",
    "\n",
    "We speak of **regression** if the target variable is a _continuous_ value. This is the task of [ðŸ““fitting](../stats/stats-fitting-short.ipynb) a function to the data points so that it enables prediction.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/13/Main-qimg-48d5bd214e53d440fa32fc9e5300c894.png)\n",
    "**classification**\n",
    "_Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Main-qimg-48d5bd214e53d440fa32fc9e5300c894.png)_\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/500px-Linear_regression.svg.png) **regression** _Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Linear_regression.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we are going to use a data set of house prices and (a vast number of) attributes. The dataset was provided by [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) for one of their machine learning challenges, in which teams compete for the first place on the global leaderboard - the best prediction wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../.assets/data/house/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation of the data set contains explanation for the numerous attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {data_dir}/data_description.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look into the data file reveals a typical CSV file - we are going to parse it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {data_dir}/prices.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After creating a `SparkSession`, we read the contents of the .csv file into a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HousePricePredictor\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(f\"{data_dir}/prices.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a schema for this large dataframe beforehand is a daunting task, so we leave the types a the default (string) and cast later as needed. We know however that the prices should be floating point numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"SalePrice\", data[\"SalePrice\"].cast(\"DOUBLE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has a large number of columns - let's select some to take a brief look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"OverallQual\", \"OverallCond\", \"YearBuilt\", \"SalePrice\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to build a predictive model for house prices, using `prices.csv` as training data.\n",
    "\n",
    "- Build your pipeline using the building blocks provided by `pyspark.ml` (Estimator, Transformer, Pipeline...). Go back to our [ðŸ““previous classification pipeline](../spark/spark-ml-pipeline.ipynb) for inspiration.\n",
    "- `pyspark.ml` provides [**a few algorithms for regression**](https://spark.apache.org/docs/latest/ml-classification-regression.html#regression) - use both reasoning and experimentation to select a viable one.\n",
    "- Don't overcomplicate things at first - start by building a **minimal viable model** that uses a few strong features, and evaluate it - then add more features to improve performance.\n",
    "- The performance of your predictive model is going to be evaluated in the section below. Take a look at the evaluation code and the error metrics used. Make sure to use the following naming conventions so the code below gets the right inputs:\n",
    "    - `pipeline`: `pyspark.ml.Pipeline` object representing the entire ML pipeline that produces your model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your ML pipeline code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we evaluate the performance of the regression model. A better model produces smaller errors in the predicted price. The two error metrics we use are **Root-Mean-Squared-Error (RMSE)** and **Mean Average Error (MAE)** between the predicted value and the observed sales price. In order to get robust scores with less random fluctuation, we apply **cross-validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready = False   # set this to True once you are ready to evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready:\n",
    "    rmse = CrossValidator(estimator=pipeline,\n",
    "                        evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                        estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                        numFolds=4) \\\n",
    "                        .fit(data.withColumnRenamed(\"SalePrice\", \"label\")) \\\n",
    "                        .avgMetrics[0]\n",
    "\n",
    "    mae = CrossValidator(estimator=pipeline,\n",
    "                        evaluator=RegressionEvaluator(metricName=\"mae\", labelCol=\"label\", predictionCol=\"prediction\"),\n",
    "                        estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                        numFolds=4) \\\n",
    "                        .fit(data.withColumnRenamed(\"SalePrice\", \"label\")) \\\n",
    "                        .avgMetrics[0]\n",
    "    \n",
    "    team_name = \"Team 1\"  # change this to the name of your team\n",
    "    print(\"\\t\".join([\"time\", \"team\", \"RMSE\", \"MAE\"]))\n",
    "    print(\"\\t\".join([datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), team_name, \"{0:.4f}\".format(rmse), \"{0:.4f}\".format(mae)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better understanding of the error made by the model, plot the distribution of prices, predicted prices, and errors. This can provide useful feedback for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready:\n",
    "    predicted = pipeline.fit(data.withColumnRenamed(\"SalePrice\", \"label\")).transform(data)\n",
    "    predicted[[\"SalePrice\", \"prediction\"]].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ready:\n",
    "    predicted_pd = predicted[[\"SalePrice\", \"prediction\"]].toPandas()\n",
    "    seaborn.distplot(predicted_pd[\"SalePrice\"])\n",
    "    seaborn.distplot(predicted_pd[\"prediction\"])\n",
    "    seaborn.distplot(predicted_pd[\"SalePrice\"] - predicted_pd[\"prediction\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2018-2022 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
