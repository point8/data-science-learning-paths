{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Exercise Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds101_spark_tools import use_spark_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Pi Approximation as MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_point(x):\n",
    "    return (random.random(), random.random())\n",
    "\n",
    "def inside(p):\n",
    "    x, y = p\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "count = sc.range(num_samples).map(random_point).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-Reduce reimplementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside_counter(p):\n",
    "    x, y = p\n",
    "    if (x*x + y*y < 1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"PiMapReduced\")\n",
    "\n",
    "count = sc.range(num_samples).map(random_point).map(inside_counter).reduce(lambda a, b: a + b)\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Pi Approximation as Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/pi_approximation_job_solution.py\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import random\n",
    "\n",
    "SPARK_APP_NAME='pi'\n",
    "\n",
    "def random_point(x):\n",
    "    return (random.random(), random.random())\n",
    "\n",
    "def inside(p):\n",
    "    x, y = p\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "def pi_approximation(spark_context, num_samples):\n",
    "    \"\"\" Approximate pi via Monte Carlo method\"\"\"\n",
    "    count = spark_context.range(num_samples).map(random_point).filter(inside).count()\n",
    "    pi = 4 * count / num_samples\n",
    "    return pi\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(sparkAppName):\n",
    "    conf = SparkConf().setAppName(sparkAppName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "\n",
    "with use_spark_context(SPARK_APP_NAME) as spark_context:\n",
    "    num_samples = 100000000\n",
    "    pi = pi_approximation(spark_context, num_samples)\n",
    "    print()\n",
    "    print(\"RESULT: pi is approximately \", pi)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Wrangling the Zipcode DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_session(appName):\n",
    "    spark_session = pyspark.sql.SparkSession.builder.appName(appName).getOrCreate()\n",
    "    try:\n",
    "        print(\"starting \", appName)\n",
    "        yield spark_session\n",
    "    finally:\n",
    "        spark_session.stop()\n",
    "        print(\"stopping \", appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Output a table of the total population of each state in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "with use_spark_session(\"State population size\") as spark:\n",
    "    df = spark.read.json(\"../.assets/data/zipcodes/zips.json\")\n",
    "    df.groupby(\"state\") \\\n",
    "        .sum() \\\n",
    "        .sort(col(\"sum(pop)\").desc()) \\\n",
    "        .show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Show the zip code areas north of the 49th parallel with more than 1000 inhabitans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "with use_spark_session(\"49th parallel\") as spark:\n",
    "    df = spark.read.json(\"../.assets/data/zipcodes/zips.json\")\n",
    "    df.withColumn(\"lat\", df[\"loc\"][0]) \\\n",
    "        .withColumn(\"lon\", df[\"loc\"][1]) \\\n",
    "        .where(col(\"lon\") >= 49) \\\n",
    "        .where(col(\"pop\") >= 1000) \\\n",
    "        .show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Assembling the Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../.assets/data/titanic/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "             StructField('PassengerId', StringType()),\n",
    "             StructField('Survived', IntegerType()),\n",
    "             StructField('Pclass', IntegerType()),\n",
    "             StructField('Name', StringType()),\n",
    "             StructField('Sex', StringType()),\n",
    "             StructField('Age', IntegerType()),\n",
    "             StructField('SibSp', IntegerType()),\n",
    "             StructField('Parch', IntegerType()),\n",
    "             StructField('Ticket', StringType()),\n",
    "             StructField('Fare', DoubleType()),\n",
    "             StructField('Cabin', StringType()),\n",
    "             StructField('Embarked', StringType())\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Estimator, Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, SQLTransformer\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaDropper(Transformer):\n",
    "    \"\"\"\n",
    "    Drops rows with at least one not-a-number element\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataAfterDrop = data.dropna(subset=self.cols) \n",
    "        return dataAfterDrop\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rename_label(Transformer):\n",
    "    \"\"\"\n",
    "    Renames a target column to `label`\n",
    "    \"\"\"\n",
    "    \n",
    "    # lazy workaround - a transformer needs to have these attributes\n",
    "    # TODO: replace if needed\n",
    "    _defaultParamMap = dict()\n",
    "    _paramMap = dict()\n",
    "    _params = dict()\n",
    "    uid = 0\n",
    "\n",
    "    def __init__(self, col=None):\n",
    "        self.col = col\n",
    "\n",
    "\n",
    "    def _transform(self, data):\n",
    "        dataAfterRename = data.withColumnRenamed(self.col, \"label\") \n",
    "        return dataAfterRename\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\" Show a proper string representation when printing the pipeline stage\"\"\"\n",
    "        return str(type(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with use_spark_session(\"ML Pipeline\") as spark:\n",
    "    data_raw = spark.read.csv(data_path, header=True, schema=schema)\n",
    "    stages = []\n",
    "\n",
    "    nadrop = NaDropper(cols=data_raw.columns)\n",
    "    encode1 = StringIndexer(inputCol=\"Embarked\", outputCol=\"Embarked_encoded\")\n",
    "    encode2 = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_encoded\")\n",
    "    # rename = Rename_label(col=\"Survived\")\n",
    "    rename= SQLTransformer(statement = \"SELECT *, Survived as label FROM __THIS__\")\n",
    "    assemble_features = VectorAssembler(inputCols=[\"Age\", \"Fare\", \"Sex_encoded\", \"Embarked_encoded\"], \n",
    "                                        outputCol=\"features\")\n",
    "    classifier = DecisionTreeClassifier()\n",
    "\n",
    "    training_stages = [nadrop, encode1, encode2, rename, assemble_features, classifier]\n",
    "\n",
    "    splitRatio = 0.8\n",
    "    data_training, data_test = data_raw.randomSplit([splitRatio, 1-splitRatio])\n",
    "\n",
    "    model = Pipeline(stages=training_stages).fit(data_training)\n",
    "\n",
    "    predictions = model.transform(data_test)\n",
    "\n",
    "    predictions[[\"label\",\"rawPrediction\",\"probability\", \"prediction\"]].show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "with use_spark_session(\"Using SQL Transformer\") as spark:\n",
    "\n",
    "    schema = StructType([\n",
    "                 StructField('PassengerId', StringType()),\n",
    "                 StructField('Survived', IntegerType()),\n",
    "                 StructField('Pclass', IntegerType()),\n",
    "                 StructField('Name', StringType()),\n",
    "                 StructField('Sex', StringType()),\n",
    "                 StructField('Age', IntegerType()),\n",
    "                 StructField('SibSp', IntegerType()),\n",
    "                 StructField('Parch', IntegerType()),\n",
    "                 StructField('Ticket', StringType()),\n",
    "                 StructField('Fare', DoubleType()),\n",
    "                 StructField('Cabin', StringType()),\n",
    "                 StructField('Embarked', StringType())\n",
    "            ])\n",
    "    data_path = \"../.assets/data/titanic/titanic.csv\"\n",
    "    data = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(data_path) \n",
    "\n",
    "    \n",
    "    na_dropper = SQLTransformer(\n",
    "                    statement=\"SELECT * FROM __THIS__ WHERE {}\".format(\n",
    "                        \" AND \".join([\"{} IS NOT NULL\".format(x) for x in data.columns])\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "    print(\"BEFORE:\")\n",
    "    for col in data.columns:\n",
    "        print(col, \" : \", data.filter(f\"{col} is NULL\").count())\n",
    "    print()\n",
    "    \n",
    "    data_clean = na_dropper.transform(data)\n",
    "    \n",
    "    print(\"AFTER\")\n",
    "    for col in data_clean.columns:\n",
    "        print(col, \" : \", data_clean.filter(f\"{col} is NULL\").count())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"../.assets/data/iliad/iliad.txt\"\n",
    "\n",
    "with use_spark_context(\"WordCount\") as spark:\n",
    "    text_file = spark.textFile(text_path)\n",
    "    counts = text_file  \\\n",
    "                 .flatMap(lambda line: line.split(\".\")) \\\n",
    "                 .flatMap(lambda sentence: sentence.split(\" \")) \\\n",
    "                 .filter(lambda word: len(word) > 0) \\\n",
    "                 .map(lambda word: (word.lower(), 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b) \\\n",
    "                 .sortBy(lambda pair: pair[1], ascending=False)\n",
    "    for kv in counts.take(10):\n",
    "        print(kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent, but as script for `spark_submit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/spark_wordcount.py\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import argparse\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(appName):\n",
    "    conf = SparkConf().setAppName(appName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        print(\"starting \", appName)\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "        print(\"stopping \", appName)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('-t','--text', help='path to text data', required=True)\n",
    "    args = vars(parser.parse_args())\n",
    "    \n",
    "    text_path = args[\"text\"]\n",
    "\n",
    "    with use_spark_context(\"WordCount\") as spark:\n",
    "        text_file = spark.textFile(text_path)\n",
    "        counts = text_file  \\\n",
    "                     .flatMap(lambda line: line.split(\".\")) \\\n",
    "                     .flatMap(lambda sentence: sentence.split(\" \")) \\\n",
    "                     .filter(lambda word: len(word) > 0) \\\n",
    "                     .map(lambda word: (word.lower(), 1)) \\\n",
    "                     .reduceByKey(lambda a, b: a + b) \\\n",
    "                     .sortBy(lambda pair: pair[1], ascending=False)\n",
    "        for kv in counts.take(10):\n",
    "            print(kv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bigram Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"../.assets/data/iliad/iliad.txt\"\n",
    "\n",
    "with use_spark_context(\"BigramCount\") as spark:\n",
    "    text_file = spark.textFile(text_path)\n",
    "    bigrams = text_file.flatMap(lambda line: line.split(\".\")) \\\n",
    "                       .map(lambda line: line.strip().split(\" \")) \\\n",
    "                       .flatMap(lambda xs: (tuple(x) for x in zip(xs, xs[1:])))\n",
    "    counts = bigrams.map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .sortBy(lambda kv: kv[1], ascending=False)\n",
    "    for kv in counts.take(10):\n",
    "        print(kv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: The Greatest Name in British Surreal Comedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/python_names.txt\n",
    "Eric Idle\n",
    "Graham Chapman\n",
    "John Cleese\n",
    "Terry Gilliam\n",
    "Terry Jones\n",
    "Michael Palin\n",
    "Monty Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scripts/python_names.txt\", \"r\") as f:\n",
    "    names = [line.strip() for line in f]\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file scripts/spark_greatest_name.py\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import argparse\n",
    "\n",
    "@contextmanager\n",
    "def use_spark_context(appName):\n",
    "    conf = SparkConf().setAppName(appName) \n",
    "    spark_context = SparkContext(conf=conf)\n",
    "\n",
    "    try:\n",
    "        print(\"starting \", appName)\n",
    "        yield spark_context\n",
    "    finally:\n",
    "        spark_context.stop()\n",
    "        print(\"stopping \", appName)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('-t','--text', help='path to text data', required=True)\n",
    "    parser.add_argument('-n','--names', help='path to names file, one name per line', required=True)\n",
    "    args = vars(parser.parse_args())\n",
    "    \n",
    "    text_path = args[\"text\"]\n",
    "    names_path = args[\"names\"]\n",
    "    with open(names_path, \"r\") as names_file:\n",
    "        names = [line.strip() for line in names_file]\n",
    "        name_bigrams = [tuple(name.split(\" \")) for name in names]\n",
    "\n",
    "    print(\"input names: \", name_bigrams)\n",
    "    with use_spark_context(\"The Greatest Name\") as spark:\n",
    "        text_file = spark.textFile(text_path)\n",
    "        bigrams = text_file.flatMap(lambda line: line.split(\".\")) \\\n",
    "                           .map(lambda line: line.strip().split(\" \")) \\\n",
    "                           .flatMap(lambda xs: (tuple(x) for x in zip(xs, xs[1:])))\n",
    "        counts = bigrams.map(lambda bigram: (bigram, 1)) \\\n",
    "                .reduceByKey(lambda x, y: x + y) \\\n",
    "                .filter(lambda kv: kv[0] in name_bigrams) \\\n",
    "                .collect()\n",
    "        print(\"Number of mentions for each name:\")\n",
    "        print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2025 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
