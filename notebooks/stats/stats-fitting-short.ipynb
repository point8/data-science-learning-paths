{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting\n",
    "\n",
    "There are several ways (e.g. using [least square fit](https://en.wikipedia.org/wiki/Least_squares) or the [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)) to fit a model (a simple line, a polynomial of degree n or machine learning model) to your data. In general, we call this step **fitting** or **regression** and it is always a kind of minimization task. In python there are several modules for that purpose (see also [SciPy optimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html) or [Numpy polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html)).\n",
    "\n",
    "One key feature is to select the correct stack of parameters. On the one hand the correct degree of freedom of a polynomial on the other hand the model parameter of a machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize as opt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16.0, 8.0)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Example: Least square fit\n",
    "\n",
    "A [least square fit](https://en.wikipedia.org/wiki/Least_squares) is a standard approach to derive a reasonable fit especially in technical use cases when an obvious functional relation between the input and output parameter is known. The most trivial case is a fit of a line to linearly dependent sample.\n",
    "\n",
    "The idea is simple: Find the set of parameters of a given function so the sum of the squared **residuals** is minimized.\n",
    "\n",
    "We use some generated data point. In general they follow a linear relationship of $y = 1 + 3x$ but we will add some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model or function we want to fit\n",
    "def linear(x, intercept, slope):\n",
    "    return intercept + slope * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 50\n",
    "err = 1\n",
    "\n",
    "# Generate example data\n",
    "x = np.linspace(-2, 2, n_sample)\n",
    "y = 1 + 3 * x\n",
    "yerr = np.random.normal(0, err, size=n_sample)\n",
    "y = y + yerr\n",
    "\n",
    "# Linear fit\n",
    "params, cov = opt.curve_fit(linear, x, y, absolute_sigma=err)\n",
    "\n",
    "# Show results\n",
    "print(f'intercept={params[0]:.2f}, slope={params[1]:.2f}')\n",
    "plt.errorbar(x, y, err, marker='+', linewidth=0, elinewidth=0, color='dodgerblue', label='Data points');\n",
    "plt.plot(x, linear(x, *params), label='Linear fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Try out different values for `err`. What happens? What happens when you do not provide the uncertainty of each data point to the fit (remove `absolute_sigma=err`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Underfitting (bias–variance dilemma)\n",
    "\n",
    "In principal, regardless of the fitting method, the choice of the model will have the highest impact. There are two extremes when chosing a model or function:\n",
    "\n",
    "- **Underfitting** (bias)\n",
    "- **Overfitting** (variance)\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "Underfitting is the situation, when having not enough parameter to describe the behavior of our data points. Our model has not enough degrees of freedom to represent the data. In the example we have data coming from a quadratic function while we use a linear function as fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 50\n",
    "err = 1\n",
    "\n",
    "# Generate example data (quadratic)\n",
    "x = np.linspace(-2, 2, n_sample)\n",
    "y = 2 * x**2\n",
    "yerr = np.random.normal(0, err, size=n_sample)\n",
    "y = y + yerr\n",
    "\n",
    "# Linear fit\n",
    "params, cov = opt.curve_fit(linear, x, y, absolute_sigma=err)\n",
    "print(f'intercept={params[0]:.2f}, slope={params[1]:.2f}')\n",
    "\n",
    "# Show results\n",
    "plt.errorbar(x, y, err, marker='+', linewidth=0, elinewidth=0, color='dodgerblue', label='Data points');\n",
    "plt.plot(x, linear(x, *params), label='Linear fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct fitting\n",
    "\n",
    "Changing to a quadratic model will get us a valid fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x, a, b):\n",
    "    return a + b * x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic fit to the data\n",
    "params, cov = opt.curve_fit(quadratic, x, y, absolute_sigma=err)\n",
    "\n",
    "# Show results\n",
    "print(f'a={params[0]:.2f}, b={params[1]:.2f}')\n",
    "plt.errorbar(x, y, err, marker='+', linewidth=0, elinewidth=0, color='dodgerblue', label='Data points');\n",
    "plt.plot(x, quadratic(x, *params), label='Quadratic fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "The opposite problem is called overfitting: The model describes the data points \"too well\", e.g. it describes random noise instead of simplifying and describing the general relationships in the data. Let's have fewer data points following a linear relationship. We use a linear function and a polynomial function (degree of six) for a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly6(x, a, b, c, d, e, f, g):\n",
    "    return a + b * x + c * x**2 + d * x**3 + e * x**4 + f * x**5 + g * x**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n_sample = 7\n",
    "err = 3\n",
    "\n",
    "# Generation of data\n",
    "x = np.linspace(-2, 2, n_sample)\n",
    "y = 1 + 3 * x  # Linear function!\n",
    "yerr = np.random.normal(0, err, size=n_sample)\n",
    "y = y + yerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.errorbar(x, y, err, marker='+', linewidth=0, elinewidth=0, color='dodgerblue', label='Data points');\n",
    "\n",
    "# Linear fit\n",
    "if False:\n",
    "    params1, cov1 = opt.curve_fit(linear, x, y, absolute_sigma=err)\n",
    "    plt.plot(x, linear(x, *params1), label='Linear fit')\n",
    "\n",
    "# Polynomial fit\n",
    "if True:\n",
    "    params2, cov2 = opt.curve_fit(poly6, x, y, absolute_sigma=err)\n",
    "    x_res = np.linspace(-2.2, 2.15, n_sample * 100)\n",
    "    plt.plot(x_res, poly6(x_res, *params2), label='Polynomial fit')\n",
    "\n",
    "# Add new data points\n",
    "if False:\n",
    "    x_new = np.array([-2.5, -1,-0.5,0,1])\n",
    "    y_new = 1 + 3 * x_new\n",
    "    yerr_new = np.random.normal(0, err, size=5)\n",
    "    y_new = y_new + yerr_new\n",
    "    plt.scatter(x_new, y_new, marker='*', s=150, color='orange', label='New data');\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial can describe each data point exactly but does not get the overall linear dependency of our original linear relationship. Especially at the edges of the spectrum it even diverges. If we generate new data points belonging to the linear relationship they will not be represented very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2018-2022 [Point 8 GmbH](https://point-8.de)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
